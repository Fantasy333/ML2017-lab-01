{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1 :    Loss_Train:  [[ 258.32534371]]    Loss_Validation:  [[ 283.41108382]]\n",
      "Loop  2 :    Loss_Train:  [[ 208.20988367]]    Loss_Validation:  [[ 229.36432801]]\n",
      "Loop  3 :    Loss_Train:  [[ 169.59810239]]    Loss_Validation:  [[ 187.52815966]]\n",
      "Loop  4 :    Loss_Train:  [[ 139.82776893]]    Loss_Validation:  [[ 155.10304517]]\n",
      "Loop  5 :    Loss_Train:  [[ 116.85359415]]    Loss_Validation:  [[ 129.93476529]]\n",
      "Loop  6 :    Loss_Train:  [[ 99.10419294]]    Loss_Validation:  [[ 110.36524597]]\n",
      "Loop  7 :    Loss_Train:  [[ 85.37221646]]    Loss_Validation:  [[ 95.11792369]]\n",
      "Loop  8 :    Loss_Train:  [[ 74.72996184]]    Loss_Validation:  [[ 83.20964327]]\n",
      "Loop  9 :    Loss_Train:  [[ 66.46455055]]    Loss_Validation:  [[ 73.88294153]]\n",
      "Loop  10 :    Loss_Train:  [[ 60.02813728]]    Loss_Validation:  [[ 66.5539944]]\n",
      "Loop  11 :    Loss_Train:  [[ 54.99966363]]    Loss_Validation:  [[ 60.77260011]]\n",
      "Loop  12 :    Loss_Train:  [[ 51.05547936]]    Loss_Validation:  [[ 56.19141133]]\n",
      "Loop  13 :    Loss_Train:  [[ 47.94677474]]    Loss_Validation:  [[ 52.54227531]]\n",
      "Loop  14 :    Loss_Train:  [[ 45.48224461]]    Loss_Validation:  [[ 49.6180369]]\n",
      "Loop  15 :    Loss_Train:  [[ 43.51477101]]    Loss_Validation:  [[ 47.25854052]]\n",
      "Loop  16 :    Loss_Train:  [[ 41.93119248]]    Loss_Validation:  [[ 45.33985995]]\n",
      "Loop  17 :    Loss_Train:  [[ 40.64444446]]    Loss_Validation:  [[ 43.76600966]]\n",
      "Loop  18 :    Loss_Train:  [[ 39.58752092]]    Loss_Validation:  [[ 42.46256416]]\n",
      "Loop  19 :    Loss_Train:  [[ 38.70883504]]    Loss_Validation:  [[ 41.37174471]]\n",
      "Loop  20 :    Loss_Train:  [[ 37.96865459]]    Loss_Validation:  [[ 40.44863462]]\n",
      "Loop  21 :    Loss_Train:  [[ 37.33636288]]    Loss_Validation:  [[ 39.65826269]]\n",
      "Loop  22 :    Loss_Train:  [[ 36.78835397]]    Loss_Validation:  [[ 38.97335483]]\n",
      "Loop  23 :    Loss_Train:  [[ 36.30641502]]    Loss_Validation:  [[ 38.37259972]]\n",
      "Loop  24 :    Loss_Train:  [[ 35.87648304]]    Loss_Validation:  [[ 37.83931042]]\n",
      "Loop  25 :    Loss_Train:  [[ 35.48768921]]    Loss_Validation:  [[ 37.3603907]]\n",
      "Loop  26 :    Loss_Train:  [[ 35.13162415]]    Loss_Validation:  [[ 36.92553635]]\n",
      "Loop  27 :    Loss_Train:  [[ 34.801773]]    Loss_Validation:  [[ 36.52661735]]\n",
      "Loop  28 :    Loss_Train:  [[ 34.49308096]]    Loss_Validation:  [[ 36.15719968]]\n",
      "Loop  29 :    Loss_Train:  [[ 34.20161909]]    Loss_Validation:  [[ 35.81217473]]\n",
      "Loop  30 :    Loss_Train:  [[ 33.92432721]]    Loss_Validation:  [[ 35.48747187]]\n",
      "Loop  31 :    Loss_Train:  [[ 33.65881597]]    Loss_Validation:  [[ 35.17983512]]\n",
      "Loop  32 :    Loss_Train:  [[ 33.40321453]]    Loss_Validation:  [[ 34.88664948]]\n",
      "Loop  33 :    Loss_Train:  [[ 33.15605323]]    Loss_Validation:  [[ 34.60580566]]\n",
      "Loop  34 :    Loss_Train:  [[ 32.91617312]]    Loss_Validation:  [[ 34.33559447]]\n",
      "Loop  35 :    Loss_Train:  [[ 32.68265633]]    Loss_Validation:  [[ 34.07462431]]\n",
      "Loop  36 :    Loss_Train:  [[ 32.45477221]]    Loss_Validation:  [[ 33.82175645]]\n",
      "Loop  37 :    Loss_Train:  [[ 32.23193592]]    Loss_Validation:  [[ 33.57605427]]\n",
      "Loop  38 :    Loss_Train:  [[ 32.01367629]]    Loss_Validation:  [[ 33.3367433]]\n",
      "Loop  39 :    Loss_Train:  [[ 31.79961103]]    Loss_Validation:  [[ 33.10317963]]\n",
      "Loop  40 :    Loss_Train:  [[ 31.58942754]]    Loss_Validation:  [[ 32.87482495]]\n",
      "Loop  41 :    Loss_Train:  [[ 31.38286805]]    Loss_Validation:  [[ 32.65122671]]\n",
      "Loop  42 :    Loss_Train:  [[ 31.17971802]]    Loss_Validation:  [[ 32.43200232]]\n",
      "Loop  43 :    Loss_Train:  [[ 30.97979717]]    Loss_Validation:  [[ 32.21682649]]\n",
      "Loop  44 :    Loss_Train:  [[ 30.78295251]]    Loss_Validation:  [[ 32.00542113]]\n",
      "Loop  45 :    Loss_Train:  [[ 30.58905284]]    Loss_Validation:  [[ 31.79754713]]\n",
      "Loop  46 :    Loss_Train:  [[ 30.39798449]]    Loss_Validation:  [[ 31.59299782]]\n",
      "Loop  47 :    Loss_Train:  [[ 30.20964797]]    Loss_Validation:  [[ 31.39159351]]\n",
      "Loop  48 :    Loss_Train:  [[ 30.0239553]]    Loss_Validation:  [[ 31.19317718]]\n",
      "Loop  49 :    Loss_Train:  [[ 29.84082795]]    Loss_Validation:  [[ 30.99761089]]\n",
      "Loop  50 :    Loss_Train:  [[ 29.66019519]]    Loss_Validation:  [[ 30.80477278]]\n",
      "Loop  51 :    Loss_Train:  [[ 29.48199274]]    Loss_Validation:  [[ 30.61455468]]\n",
      "Loop  52 :    Loss_Train:  [[ 29.30616174]]    Loss_Validation:  [[ 30.42686006]]\n",
      "Loop  53 :    Loss_Train:  [[ 29.13264789]]    Loss_Validation:  [[ 30.24160235]]\n",
      "Loop  54 :    Loss_Train:  [[ 28.96140078]]    Loss_Validation:  [[ 30.05870355]]\n",
      "Loop  55 :    Loss_Train:  [[ 28.79237332]]    Loss_Validation:  [[ 29.87809299]]\n",
      "Loop  56 :    Loss_Train:  [[ 28.62552128]]    Loss_Validation:  [[ 29.69970638]]\n",
      "Loop  57 :    Loss_Train:  [[ 28.46080294]]    Loss_Validation:  [[ 29.52348496]]\n",
      "Loop  58 :    Loss_Train:  [[ 28.29817875]]    Loss_Validation:  [[ 29.34937475]]\n",
      "Loop  59 :    Loss_Train:  [[ 28.13761109]]    Loss_Validation:  [[ 29.17732597]]\n",
      "Loop  60 :    Loss_Train:  [[ 27.97906404]]    Loss_Validation:  [[ 29.00729248]]\n",
      "Loop  61 :    Loss_Train:  [[ 27.82250321]]    Loss_Validation:  [[ 28.83923138]]\n",
      "Loop  62 :    Loss_Train:  [[ 27.66789559]]    Loss_Validation:  [[ 28.6731026]]\n",
      "Loop  63 :    Loss_Train:  [[ 27.51520938]]    Loss_Validation:  [[ 28.50886853]]\n",
      "Loop  64 :    Loss_Train:  [[ 27.36441392]]    Loss_Validation:  [[ 28.34649382]]\n",
      "Loop  65 :    Loss_Train:  [[ 27.21547958]]    Loss_Validation:  [[ 28.18594504]]\n",
      "Loop  66 :    Loss_Train:  [[ 27.06837765]]    Loss_Validation:  [[ 28.02719053]]\n",
      "Loop  67 :    Loss_Train:  [[ 26.92308029]]    Loss_Validation:  [[ 27.87020017]]\n",
      "Loop  68 :    Loss_Train:  [[ 26.77956046]]    Loss_Validation:  [[ 27.71494525]]\n",
      "Loop  69 :    Loss_Train:  [[ 26.63779185]]    Loss_Validation:  [[ 27.5613983]]\n",
      "Loop  70 :    Loss_Train:  [[ 26.49774883]]    Loss_Validation:  [[ 27.40953299]]\n",
      "Loop  71 :    Loss_Train:  [[ 26.35940645]]    Loss_Validation:  [[ 27.25932399]]\n",
      "Loop  72 :    Loss_Train:  [[ 26.22274031]]    Loss_Validation:  [[ 27.11074692]]\n",
      "Loop  73 :    Loss_Train:  [[ 26.0877266]]    Loss_Validation:  [[ 26.96377822]]\n",
      "Loop  74 :    Loss_Train:  [[ 25.95434205]]    Loss_Validation:  [[ 26.8183951]]\n",
      "Loop  75 :    Loss_Train:  [[ 25.82256387]]    Loss_Validation:  [[ 26.67457548]]\n",
      "Loop  76 :    Loss_Train:  [[ 25.69236977]]    Loss_Validation:  [[ 26.53229791]]\n",
      "Loop  77 :    Loss_Train:  [[ 25.5637379]]    Loss_Validation:  [[ 26.39154154]]\n",
      "Loop  78 :    Loss_Train:  [[ 25.43664682]]    Loss_Validation:  [[ 26.25228607]]\n",
      "Loop  79 :    Loss_Train:  [[ 25.31107554]]    Loss_Validation:  [[ 26.11451172]]\n",
      "Loop  80 :    Loss_Train:  [[ 25.18700344]]    Loss_Validation:  [[ 25.97819915]]\n",
      "Loop  81 :    Loss_Train:  [[ 25.06441027]]    Loss_Validation:  [[ 25.8433295]]\n",
      "Loop  82 :    Loss_Train:  [[ 24.94327616]]    Loss_Validation:  [[ 25.70988431]]\n",
      "Loop  83 :    Loss_Train:  [[ 24.82358157]]    Loss_Validation:  [[ 25.57784551]]\n",
      "Loop  84 :    Loss_Train:  [[ 24.7053073]]    Loss_Validation:  [[ 25.44719538]]\n",
      "Loop  85 :    Loss_Train:  [[ 24.58843449]]    Loss_Validation:  [[ 25.31791659]]\n",
      "Loop  86 :    Loss_Train:  [[ 24.47294456]]    Loss_Validation:  [[ 25.1899921]]\n",
      "Loop  87 :    Loss_Train:  [[ 24.35881925]]    Loss_Validation:  [[ 25.06340519]]\n",
      "Loop  88 :    Loss_Train:  [[ 24.2460406]]    Loss_Validation:  [[ 24.93813947]]\n",
      "Loop  89 :    Loss_Train:  [[ 24.13459092]]    Loss_Validation:  [[ 24.81417879]]\n",
      "Loop  90 :    Loss_Train:  [[ 24.0244528]]    Loss_Validation:  [[ 24.69150732]]\n",
      "Loop  91 :    Loss_Train:  [[ 23.91560911]]    Loss_Validation:  [[ 24.57010945]]\n",
      "Loop  92 :    Loss_Train:  [[ 23.80804296]]    Loss_Validation:  [[ 24.44996986]]\n",
      "Loop  93 :    Loss_Train:  [[ 23.70173774]]    Loss_Validation:  [[ 24.33107345]]\n",
      "Loop  94 :    Loss_Train:  [[ 23.59667707]]    Loss_Validation:  [[ 24.21340537]]\n",
      "Loop  95 :    Loss_Train:  [[ 23.49284481]]    Loss_Validation:  [[ 24.09695099]]\n",
      "Loop  96 :    Loss_Train:  [[ 23.39022508]]    Loss_Validation:  [[ 23.9816959]]\n",
      "Loop  97 :    Loss_Train:  [[ 23.28880222]]    Loss_Validation:  [[ 23.86762593]]\n",
      "Loop  98 :    Loss_Train:  [[ 23.18856078]]    Loss_Validation:  [[ 23.75472707]]\n",
      "Loop  99 :    Loss_Train:  [[ 23.08948557]]    Loss_Validation:  [[ 23.64298557]]\n",
      "Loop  100 :    Loss_Train:  [[ 22.99156157]]    Loss_Validation:  [[ 23.53238783]]\n",
      "Loop  101 :    Loss_Train:  [[ 22.89477403]]    Loss_Validation:  [[ 23.42292048]]\n",
      "Loop  102 :    Loss_Train:  [[ 22.79910836]]    Loss_Validation:  [[ 23.31457031]]\n",
      "Loop  103 :    Loss_Train:  [[ 22.7045502]]    Loss_Validation:  [[ 23.20732431]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  104 :    Loss_Train:  [[ 22.61108539]]    Loss_Validation:  [[ 23.10116966]]\n",
      "Loop  105 :    Loss_Train:  [[ 22.51869996]]    Loss_Validation:  [[ 22.99609369]]\n",
      "Loop  106 :    Loss_Train:  [[ 22.42738015]]    Loss_Validation:  [[ 22.89208392]]\n",
      "Loop  107 :    Loss_Train:  [[ 22.33711238]]    Loss_Validation:  [[ 22.78912804]]\n",
      "Loop  108 :    Loss_Train:  [[ 22.24788326]]    Loss_Validation:  [[ 22.6872139]]\n",
      "Loop  109 :    Loss_Train:  [[ 22.15967958]]    Loss_Validation:  [[ 22.58632951]]\n",
      "Loop  110 :    Loss_Train:  [[ 22.07248832]]    Loss_Validation:  [[ 22.48646305]]\n",
      "Loop  111 :    Loss_Train:  [[ 21.98629663]]    Loss_Validation:  [[ 22.38760286]]\n",
      "Loop  112 :    Loss_Train:  [[ 21.90109186]]    Loss_Validation:  [[ 22.28973741]]\n",
      "Loop  113 :    Loss_Train:  [[ 21.8168615]]    Loss_Validation:  [[ 22.19285534]]\n",
      "Loop  114 :    Loss_Train:  [[ 21.73359323]]    Loss_Validation:  [[ 22.09694544]]\n",
      "Loop  115 :    Loss_Train:  [[ 21.6512749]]    Loss_Validation:  [[ 22.00199665]]\n",
      "Loop  116 :    Loss_Train:  [[ 21.56989451]]    Loss_Validation:  [[ 21.90799803]]\n",
      "Loop  117 :    Loss_Train:  [[ 21.48944023]]    Loss_Validation:  [[ 21.8149388]]\n",
      "Loop  118 :    Loss_Train:  [[ 21.4099004]]    Loss_Validation:  [[ 21.72280832]]\n",
      "Loop  119 :    Loss_Train:  [[ 21.3312635]]    Loss_Validation:  [[ 21.63159608]]\n",
      "Loop  120 :    Loss_Train:  [[ 21.25351818]]    Loss_Validation:  [[ 21.54129172]]\n",
      "Loop  121 :    Loss_Train:  [[ 21.17665324]]    Loss_Validation:  [[ 21.45188498]]\n",
      "Loop  122 :    Loss_Train:  [[ 21.10065763]]    Loss_Validation:  [[ 21.36336576]]\n",
      "Loop  123 :    Loss_Train:  [[ 21.02552044]]    Loss_Validation:  [[ 21.27572409]]\n",
      "Loop  124 :    Loss_Train:  [[ 20.95123092]]    Loss_Validation:  [[ 21.18895009]]\n",
      "Loop  125 :    Loss_Train:  [[ 20.87777846]]    Loss_Validation:  [[ 21.10303405]]\n",
      "Loop  126 :    Loss_Train:  [[ 20.80515258]]    Loss_Validation:  [[ 21.01796636]]\n",
      "Loop  127 :    Loss_Train:  [[ 20.73334296]]    Loss_Validation:  [[ 20.93373753]]\n",
      "Loop  128 :    Loss_Train:  [[ 20.6623394]]    Loss_Validation:  [[ 20.85033819]]\n",
      "Loop  129 :    Loss_Train:  [[ 20.59213186]]    Loss_Validation:  [[ 20.7677591]]\n",
      "Loop  130 :    Loss_Train:  [[ 20.5227104]]    Loss_Validation:  [[ 20.68599111]]\n",
      "Loop  131 :    Loss_Train:  [[ 20.45406524]]    Loss_Validation:  [[ 20.60502522]]\n",
      "Loop  132 :    Loss_Train:  [[ 20.38618672]]    Loss_Validation:  [[ 20.5248525]]\n",
      "Loop  133 :    Loss_Train:  [[ 20.31906531]]    Loss_Validation:  [[ 20.44546418]]\n",
      "Loop  134 :    Loss_Train:  [[ 20.2526916]]    Loss_Validation:  [[ 20.36685154]]\n",
      "Loop  135 :    Loss_Train:  [[ 20.18705632]]    Loss_Validation:  [[ 20.28900602]]\n",
      "Loop  136 :    Loss_Train:  [[ 20.12215031]]    Loss_Validation:  [[ 20.21191915]]\n",
      "Loop  137 :    Loss_Train:  [[ 20.05796454]]    Loss_Validation:  [[ 20.13558254]]\n",
      "Loop  138 :    Loss_Train:  [[ 19.99449009]]    Loss_Validation:  [[ 20.05998794]]\n",
      "Loop  139 :    Loss_Train:  [[ 19.93171816]]    Loss_Validation:  [[ 19.98512717]]\n",
      "Loop  140 :    Loss_Train:  [[ 19.86964008]]    Loss_Validation:  [[ 19.91099218]]\n",
      "Loop  141 :    Loss_Train:  [[ 19.80824728]]    Loss_Validation:  [[ 19.83757499]]\n",
      "Loop  142 :    Loss_Train:  [[ 19.7475313]]    Loss_Validation:  [[ 19.76486774]]\n",
      "Loop  143 :    Loss_Train:  [[ 19.6874838]]    Loss_Validation:  [[ 19.69286266]]\n",
      "Loop  144 :    Loss_Train:  [[ 19.62809656]]    Loss_Validation:  [[ 19.62155206]]\n",
      "Loop  145 :    Loss_Train:  [[ 19.56936144]]    Loss_Validation:  [[ 19.55092836]]\n",
      "Loop  146 :    Loss_Train:  [[ 19.51127044]]    Loss_Validation:  [[ 19.48098407]]\n",
      "Loop  147 :    Loss_Train:  [[ 19.45381564]]    Loss_Validation:  [[ 19.41171179]]\n",
      "Loop  148 :    Loss_Train:  [[ 19.39698924]]    Loss_Validation:  [[ 19.3431042]]\n",
      "Loop  149 :    Loss_Train:  [[ 19.34078352]]    Loss_Validation:  [[ 19.27515408]]\n",
      "Loop  150 :    Loss_Train:  [[ 19.2851909]]    Loss_Validation:  [[ 19.2078543]]\n",
      "Loop  151 :    Loss_Train:  [[ 19.23020386]]    Loss_Validation:  [[ 19.14119781]]\n",
      "Loop  152 :    Loss_Train:  [[ 19.17581499]]    Loss_Validation:  [[ 19.07517763]]\n",
      "Loop  153 :    Loss_Train:  [[ 19.122017]]    Loss_Validation:  [[ 19.00978689]]\n",
      "Loop  154 :    Loss_Train:  [[ 19.06880266]]    Loss_Validation:  [[ 18.94501879]]\n",
      "Loop  155 :    Loss_Train:  [[ 19.01616487]]    Loss_Validation:  [[ 18.88086661]]\n",
      "Loop  156 :    Loss_Train:  [[ 18.96409659]]    Loss_Validation:  [[ 18.81732372]]\n",
      "Loop  157 :    Loss_Train:  [[ 18.91259088]]    Loss_Validation:  [[ 18.75438355]]\n",
      "Loop  158 :    Loss_Train:  [[ 18.86164092]]    Loss_Validation:  [[ 18.69203963]]\n",
      "Loop  159 :    Loss_Train:  [[ 18.81123993]]    Loss_Validation:  [[ 18.63028556]]\n",
      "Loop  160 :    Loss_Train:  [[ 18.76138126]]    Loss_Validation:  [[ 18.569115]]\n",
      "Loop  161 :    Loss_Train:  [[ 18.71205833]]    Loss_Validation:  [[ 18.50852172]]\n",
      "Loop  162 :    Loss_Train:  [[ 18.66326464]]    Loss_Validation:  [[ 18.44849953]]\n",
      "Loop  163 :    Loss_Train:  [[ 18.61499379]]    Loss_Validation:  [[ 18.38904233]]\n",
      "Loop  164 :    Loss_Train:  [[ 18.56723944]]    Loss_Validation:  [[ 18.33014409]]\n",
      "Loop  165 :    Loss_Train:  [[ 18.51999535]]    Loss_Validation:  [[ 18.27179885]]\n",
      "Loop  166 :    Loss_Train:  [[ 18.47325536]]    Loss_Validation:  [[ 18.21400073]]\n",
      "Loop  167 :    Loss_Train:  [[ 18.42701339]]    Loss_Validation:  [[ 18.1567439]]\n",
      "Loop  168 :    Loss_Train:  [[ 18.38126343]]    Loss_Validation:  [[ 18.10002261]]\n",
      "Loop  169 :    Loss_Train:  [[ 18.33599956]]    Loss_Validation:  [[ 18.04383118]]\n",
      "Loop  170 :    Loss_Train:  [[ 18.29121592]]    Loss_Validation:  [[ 17.98816399]]\n",
      "Loop  171 :    Loss_Train:  [[ 18.24690674]]    Loss_Validation:  [[ 17.9330155]]\n",
      "Loop  172 :    Loss_Train:  [[ 18.20306632]]    Loss_Validation:  [[ 17.87838022]]\n",
      "Loop  173 :    Loss_Train:  [[ 18.15968904]]    Loss_Validation:  [[ 17.82425272]]\n",
      "Loop  174 :    Loss_Train:  [[ 18.11676933]]    Loss_Validation:  [[ 17.77062766]]\n",
      "Loop  175 :    Loss_Train:  [[ 18.07430173]]    Loss_Validation:  [[ 17.71749973]]\n",
      "Loop  176 :    Loss_Train:  [[ 18.0322808]]    Loss_Validation:  [[ 17.66486371]]\n",
      "Loop  177 :    Loss_Train:  [[ 17.99070123]]    Loss_Validation:  [[ 17.61271443]]\n",
      "Loop  178 :    Loss_Train:  [[ 17.94955772]]    Loss_Validation:  [[ 17.56104676]]\n",
      "Loop  179 :    Loss_Train:  [[ 17.90884508]]    Loss_Validation:  [[ 17.50985566]]\n",
      "Loop  180 :    Loss_Train:  [[ 17.86855816]]    Loss_Validation:  [[ 17.45913614]]\n",
      "Loop  181 :    Loss_Train:  [[ 17.8286919]]    Loss_Validation:  [[ 17.40888326]]\n",
      "Loop  182 :    Loss_Train:  [[ 17.78924128]]    Loss_Validation:  [[ 17.35909214]]\n",
      "Loop  183 :    Loss_Train:  [[ 17.75020136]]    Loss_Validation:  [[ 17.30975797]]\n",
      "Loop  184 :    Loss_Train:  [[ 17.71156727]]    Loss_Validation:  [[ 17.26087597]]\n",
      "Loop  185 :    Loss_Train:  [[ 17.67333418]]    Loss_Validation:  [[ 17.21244144]]\n",
      "Loop  186 :    Loss_Train:  [[ 17.63549733]]    Loss_Validation:  [[ 17.16444972]]\n",
      "Loop  187 :    Loss_Train:  [[ 17.59805204]]    Loss_Validation:  [[ 17.11689621]]\n",
      "Loop  188 :    Loss_Train:  [[ 17.56099367]]    Loss_Validation:  [[ 17.06977635]]\n",
      "Loop  189 :    Loss_Train:  [[ 17.52431763]]    Loss_Validation:  [[ 17.02308565]]\n",
      "Loop  190 :    Loss_Train:  [[ 17.48801941]]    Loss_Validation:  [[ 16.97681968]]\n",
      "Loop  191 :    Loss_Train:  [[ 17.45209456]]    Loss_Validation:  [[ 16.93097402]]\n",
      "Loop  192 :    Loss_Train:  [[ 17.41653866]]    Loss_Validation:  [[ 16.88554434]]\n",
      "Loop  193 :    Loss_Train:  [[ 17.38134738]]    Loss_Validation:  [[ 16.84052634]]\n",
      "Loop  194 :    Loss_Train:  [[ 17.34651641]]    Loss_Validation:  [[ 16.79591579]]\n",
      "Loop  195 :    Loss_Train:  [[ 17.31204152]]    Loss_Validation:  [[ 16.75170847]]\n",
      "Loop  196 :    Loss_Train:  [[ 17.27791852]]    Loss_Validation:  [[ 16.70790025]]\n",
      "Loop  197 :    Loss_Train:  [[ 17.24414329]]    Loss_Validation:  [[ 16.66448702]]\n",
      "Loop  198 :    Loss_Train:  [[ 17.21071175]]    Loss_Validation:  [[ 16.62146473]]\n",
      "Loop  199 :    Loss_Train:  [[ 17.17761986]]    Loss_Validation:  [[ 16.57882937]]\n",
      "Loop  200 :    Loss_Train:  [[ 17.14486365]]    Loss_Validation:  [[ 16.53657698]]\n",
      "Loop  201 :    Loss_Train:  [[ 17.11243921]]    Loss_Validation:  [[ 16.49470364]]\n",
      "Loop  202 :    Loss_Train:  [[ 17.08034264]]    Loss_Validation:  [[ 16.45320547]]\n",
      "Loop  203 :    Loss_Train:  [[ 17.04857013]]    Loss_Validation:  [[ 16.41207865]]\n",
      "Loop  204 :    Loss_Train:  [[ 17.01711789]]    Loss_Validation:  [[ 16.37131939]]\n",
      "Loop  205 :    Loss_Train:  [[ 16.98598219]]    Loss_Validation:  [[ 16.33092395]]\n",
      "Loop  206 :    Loss_Train:  [[ 16.95515935]]    Loss_Validation:  [[ 16.29088863]]\n",
      "Loop  207 :    Loss_Train:  [[ 16.92464574]]    Loss_Validation:  [[ 16.25120977]]\n",
      "Loop  208 :    Loss_Train:  [[ 16.89443775]]    Loss_Validation:  [[ 16.21188375]]\n",
      "Loop  209 :    Loss_Train:  [[ 16.86453185]]    Loss_Validation:  [[ 16.17290699]]\n",
      "Loop  210 :    Loss_Train:  [[ 16.83492452]]    Loss_Validation:  [[ 16.13427596]]\n",
      "Loop  211 :    Loss_Train:  [[ 16.80561232]]    Loss_Validation:  [[ 16.09598716]]\n",
      "Loop  212 :    Loss_Train:  [[ 16.77659183]]    Loss_Validation:  [[ 16.05803713]]\n",
      "Loop  213 :    Loss_Train:  [[ 16.74785967]]    Loss_Validation:  [[ 16.02042246]]\n",
      "Loop  214 :    Loss_Train:  [[ 16.71941252]]    Loss_Validation:  [[ 15.98313976]]\n",
      "Loop  215 :    Loss_Train:  [[ 16.69124708]]    Loss_Validation:  [[ 15.94618569]]\n",
      "Loop  216 :    Loss_Train:  [[ 16.66336011]]    Loss_Validation:  [[ 15.90955695]]\n",
      "Loop  217 :    Loss_Train:  [[ 16.63574841]]    Loss_Validation:  [[ 15.87325026]]\n",
      "Loop  218 :    Loss_Train:  [[ 16.60840881]]    Loss_Validation:  [[ 15.8372624]]\n",
      "Loop  219 :    Loss_Train:  [[ 16.58133817]]    Loss_Validation:  [[ 15.80159016]]\n",
      "Loop  220 :    Loss_Train:  [[ 16.55453342]]    Loss_Validation:  [[ 15.7662304]]\n",
      "Loop  221 :    Loss_Train:  [[ 16.5279915]]    Loss_Validation:  [[ 15.73117998]]\n",
      "Loop  222 :    Loss_Train:  [[ 16.5017094]]    Loss_Validation:  [[ 15.69643581]]\n",
      "Loop  223 :    Loss_Train:  [[ 16.47568416]]    Loss_Validation:  [[ 15.66199484]]\n",
      "Loop  224 :    Loss_Train:  [[ 16.44991282]]    Loss_Validation:  [[ 15.62785405]]\n",
      "Loop  225 :    Loss_Train:  [[ 16.42439249]]    Loss_Validation:  [[ 15.59401044]]\n",
      "Loop  226 :    Loss_Train:  [[ 16.39912032]]    Loss_Validation:  [[ 15.56046107]]\n",
      "Loop  227 :    Loss_Train:  [[ 16.37409346]]    Loss_Validation:  [[ 15.527203]]\n",
      "Loop  228 :    Loss_Train:  [[ 16.34930912]]    Loss_Validation:  [[ 15.49423335]]\n",
      "Loop  229 :    Loss_Train:  [[ 16.32476455]]    Loss_Validation:  [[ 15.46154926]]\n",
      "Loop  230 :    Loss_Train:  [[ 16.30045702]]    Loss_Validation:  [[ 15.4291479]]\n",
      "Loop  231 :    Loss_Train:  [[ 16.27638383]]    Loss_Validation:  [[ 15.39702648]]\n",
      "Loop  232 :    Loss_Train:  [[ 16.25254234]]    Loss_Validation:  [[ 15.36518223]]\n",
      "Loop  233 :    Loss_Train:  [[ 16.22892992]]    Loss_Validation:  [[ 15.33361241]]\n",
      "Loop  234 :    Loss_Train:  [[ 16.20554396]]    Loss_Validation:  [[ 15.30231433]]\n",
      "Loop  235 :    Loss_Train:  [[ 16.18238192]]    Loss_Validation:  [[ 15.2712853]]\n",
      "Loop  236 :    Loss_Train:  [[ 16.15944127]]    Loss_Validation:  [[ 15.24052268]]\n",
      "Loop  237 :    Loss_Train:  [[ 16.1367195]]    Loss_Validation:  [[ 15.21002386]]\n",
      "Loop  238 :    Loss_Train:  [[ 16.11421414]]    Loss_Validation:  [[ 15.17978624]]\n",
      "Loop  239 :    Loss_Train:  [[ 16.09192277]]    Loss_Validation:  [[ 15.14980727]]\n",
      "Loop  240 :    Loss_Train:  [[ 16.06984297]]    Loss_Validation:  [[ 15.12008441]]\n",
      "Loop  241 :    Loss_Train:  [[ 16.04797237]]    Loss_Validation:  [[ 15.09061517]]\n",
      "Loop  242 :    Loss_Train:  [[ 16.02630862]]    Loss_Validation:  [[ 15.06139705]]\n",
      "Loop  243 :    Loss_Train:  [[ 16.00484939]]    Loss_Validation:  [[ 15.03242762]]\n",
      "Loop  244 :    Loss_Train:  [[ 15.9835924]]    Loss_Validation:  [[ 15.00370444]]\n",
      "Loop  245 :    Loss_Train:  [[ 15.96253539]]    Loss_Validation:  [[ 14.97522513]]\n",
      "Loop  246 :    Loss_Train:  [[ 15.94167612]]    Loss_Validation:  [[ 14.94698731]]\n",
      "Loop  247 :    Loss_Train:  [[ 15.92101238]]    Loss_Validation:  [[ 14.91898864]]\n",
      "Loop  248 :    Loss_Train:  [[ 15.90054199]]    Loss_Validation:  [[ 14.89122679]]\n",
      "Loop  249 :    Loss_Train:  [[ 15.88026279]]    Loss_Validation:  [[ 14.86369947]]\n",
      "Loop  250 :    Loss_Train:  [[ 15.86017267]]    Loss_Validation:  [[ 14.83640441]]\n",
      "Loop  251 :    Loss_Train:  [[ 15.84026951]]    Loss_Validation:  [[ 14.80933936]]\n",
      "Loop  252 :    Loss_Train:  [[ 15.82055123]]    Loss_Validation:  [[ 14.78250211]]\n",
      "Loop  253 :    Loss_Train:  [[ 15.8010158]]    Loss_Validation:  [[ 14.75589046]]\n",
      "Loop  254 :    Loss_Train:  [[ 15.78166118]]    Loss_Validation:  [[ 14.72950223]]\n",
      "Loop  255 :    Loss_Train:  [[ 15.76248537]]    Loss_Validation:  [[ 14.70333527]]\n",
      "Loop  256 :    Loss_Train:  [[ 15.74348639]]    Loss_Validation:  [[ 14.67738746]]\n",
      "Loop  257 :    Loss_Train:  [[ 15.72466228]]    Loss_Validation:  [[ 14.65165669]]\n",
      "Loop  258 :    Loss_Train:  [[ 15.70601113]]    Loss_Validation:  [[ 14.62614087]]\n",
      "Loop  259 :    Loss_Train:  [[ 15.68753102]]    Loss_Validation:  [[ 14.60083796]]\n",
      "Loop  260 :    Loss_Train:  [[ 15.66922007]]    Loss_Validation:  [[ 14.5757459]]\n",
      "Loop  261 :    Loss_Train:  [[ 15.65107642]]    Loss_Validation:  [[ 14.5508627]]\n",
      "Loop  262 :    Loss_Train:  [[ 15.63309823]]    Loss_Validation:  [[ 14.52618634]]\n",
      "Loop  263 :    Loss_Train:  [[ 15.61528368]]    Loss_Validation:  [[ 14.50171486]]\n",
      "Loop  264 :    Loss_Train:  [[ 15.59763098]]    Loss_Validation:  [[ 14.47744631]]\n",
      "Loop  265 :    Loss_Train:  [[ 15.58013836]]    Loss_Validation:  [[ 14.45337876]]\n",
      "Loop  266 :    Loss_Train:  [[ 15.56280407]]    Loss_Validation:  [[ 14.42951029]]\n",
      "Loop  267 :    Loss_Train:  [[ 15.54562637]]    Loss_Validation:  [[ 14.40583902]]\n",
      "Loop  268 :    Loss_Train:  [[ 15.52860357]]    Loss_Validation:  [[ 14.38236307]]\n",
      "Loop  269 :    Loss_Train:  [[ 15.51173396]]    Loss_Validation:  [[ 14.3590806]]\n",
      "Loop  270 :    Loss_Train:  [[ 15.49501588]]    Loss_Validation:  [[ 14.33598977]]\n",
      "Loop  271 :    Loss_Train:  [[ 15.47844769]]    Loss_Validation:  [[ 14.31308877]]\n",
      "Loop  272 :    Loss_Train:  [[ 15.46202775]]    Loss_Validation:  [[ 14.29037581]]\n",
      "Loop  273 :    Loss_Train:  [[ 15.44575445]]    Loss_Validation:  [[ 14.26784912]]\n",
      "Loop  274 :    Loss_Train:  [[ 15.42962621]]    Loss_Validation:  [[ 14.24550694]]\n",
      "Loop  275 :    Loss_Train:  [[ 15.41364145]]    Loss_Validation:  [[ 14.22334754]]\n",
      "Loop  276 :    Loss_Train:  [[ 15.39779863]]    Loss_Validation:  [[ 14.2013692]]\n",
      "Loop  277 :    Loss_Train:  [[ 15.3820962]]    Loss_Validation:  [[ 14.17957021]]\n",
      "Loop  278 :    Loss_Train:  [[ 15.36653266]]    Loss_Validation:  [[ 14.15794891]]\n",
      "Loop  279 :    Loss_Train:  [[ 15.35110651]]    Loss_Validation:  [[ 14.13650361]]\n",
      "Loop  280 :    Loss_Train:  [[ 15.33581626]]    Loss_Validation:  [[ 14.11523269]]\n",
      "Loop  281 :    Loss_Train:  [[ 15.32066046]]    Loss_Validation:  [[ 14.09413449]]\n",
      "Loop  282 :    Loss_Train:  [[ 15.30563766]]    Loss_Validation:  [[ 14.07320743]]\n",
      "Loop  283 :    Loss_Train:  [[ 15.29074643]]    Loss_Validation:  [[ 14.05244989]]\n",
      "Loop  284 :    Loss_Train:  [[ 15.27598536]]    Loss_Validation:  [[ 14.0318603]]\n",
      "Loop  285 :    Loss_Train:  [[ 15.26135307]]    Loss_Validation:  [[ 14.0114371]]\n",
      "Loop  286 :    Loss_Train:  [[ 15.24684817]]    Loss_Validation:  [[ 13.99117874]]\n",
      "Loop  287 :    Loss_Train:  [[ 15.2324693]]    Loss_Validation:  [[ 13.9710837]]\n",
      "Loop  288 :    Loss_Train:  [[ 15.21821512]]    Loss_Validation:  [[ 13.95115045]]\n",
      "Loop  289 :    Loss_Train:  [[ 15.2040843]]    Loss_Validation:  [[ 13.93137749]]\n",
      "Loop  290 :    Loss_Train:  [[ 15.19007553]]    Loss_Validation:  [[ 13.91176336]]\n",
      "Loop  291 :    Loss_Train:  [[ 15.1761875]]    Loss_Validation:  [[ 13.89230658]]\n",
      "Loop  292 :    Loss_Train:  [[ 15.16241895]]    Loss_Validation:  [[ 13.87300569]]\n",
      "Loop  293 :    Loss_Train:  [[ 15.14876859]]    Loss_Validation:  [[ 13.85385926]]\n",
      "Loop  294 :    Loss_Train:  [[ 15.13523518]]    Loss_Validation:  [[ 13.83486587]]\n",
      "Loop  295 :    Loss_Train:  [[ 15.12181748]]    Loss_Validation:  [[ 13.81602411]]\n",
      "Loop  296 :    Loss_Train:  [[ 15.10851427]]    Loss_Validation:  [[ 13.79733258]]\n",
      "Loop  297 :    Loss_Train:  [[ 15.09532433]]    Loss_Validation:  [[ 13.77878992]]\n",
      "Loop  298 :    Loss_Train:  [[ 15.08224649]]    Loss_Validation:  [[ 13.76039474]]\n",
      "Loop  299 :    Loss_Train:  [[ 15.06927954]]    Loss_Validation:  [[ 13.74214571]]\n",
      "Loop  300 :    Loss_Train:  [[ 15.05642234]]    Loss_Validation:  [[ 13.72404149]]\n",
      "Loop  301 :    Loss_Train:  [[ 15.04367372]]    Loss_Validation:  [[ 13.70608076]]\n",
      "Loop  302 :    Loss_Train:  [[ 15.03103254]]    Loss_Validation:  [[ 13.68826219]]\n",
      "Loop  303 :    Loss_Train:  [[ 15.01849769]]    Loss_Validation:  [[ 13.67058451]]\n",
      "Loop  304 :    Loss_Train:  [[ 15.00606804]]    Loss_Validation:  [[ 13.65304642]]\n",
      "Loop  305 :    Loss_Train:  [[ 14.9937425]]    Loss_Validation:  [[ 13.63564666]]\n",
      "Loop  306 :    Loss_Train:  [[ 14.98151998]]    Loss_Validation:  [[ 13.61838397]]\n",
      "Loop  307 :    Loss_Train:  [[ 14.96939941]]    Loss_Validation:  [[ 13.60125711]]\n",
      "Loop  308 :    Loss_Train:  [[ 14.95737972]]    Loss_Validation:  [[ 13.58426485]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  309 :    Loss_Train:  [[ 14.94545986]]    Loss_Validation:  [[ 13.56740596]]\n",
      "Loop  310 :    Loss_Train:  [[ 14.93363881]]    Loss_Validation:  [[ 13.55067925]]\n",
      "Loop  311 :    Loss_Train:  [[ 14.92191552]]    Loss_Validation:  [[ 13.53408351]]\n",
      "Loop  312 :    Loss_Train:  [[ 14.91028899]]    Loss_Validation:  [[ 13.51761757]]\n",
      "Loop  313 :    Loss_Train:  [[ 14.89875823]]    Loss_Validation:  [[ 13.50128026]]\n",
      "Loop  314 :    Loss_Train:  [[ 14.88732222]]    Loss_Validation:  [[ 13.48507041]]\n",
      "Loop  315 :    Loss_Train:  [[ 14.87598001]]    Loss_Validation:  [[ 13.46898689]]\n",
      "Loop  316 :    Loss_Train:  [[ 14.86473062]]    Loss_Validation:  [[ 13.45302856]]\n",
      "Loop  317 :    Loss_Train:  [[ 14.85357309]]    Loss_Validation:  [[ 13.4371943]]\n",
      "Loop  318 :    Loss_Train:  [[ 14.84250648]]    Loss_Validation:  [[ 13.42148299]]\n",
      "Loop  319 :    Loss_Train:  [[ 14.83152986]]    Loss_Validation:  [[ 13.40589353]]\n",
      "Loop  320 :    Loss_Train:  [[ 14.82064231]]    Loss_Validation:  [[ 13.39042484]]\n",
      "Loop  321 :    Loss_Train:  [[ 14.8098429]]    Loss_Validation:  [[ 13.37507584]]\n",
      "Loop  322 :    Loss_Train:  [[ 14.79913074]]    Loss_Validation:  [[ 13.35984545]]\n",
      "Loop  323 :    Loss_Train:  [[ 14.78850494]]    Loss_Validation:  [[ 13.34473263]]\n",
      "Loop  324 :    Loss_Train:  [[ 14.77796461]]    Loss_Validation:  [[ 13.32973633]]\n",
      "Loop  325 :    Loss_Train:  [[ 14.76750888]]    Loss_Validation:  [[ 13.3148555]]\n",
      "Loop  326 :    Loss_Train:  [[ 14.75713689]]    Loss_Validation:  [[ 13.30008913]]\n",
      "Loop  327 :    Loss_Train:  [[ 14.7468478]]    Loss_Validation:  [[ 13.2854362]]\n",
      "Loop  328 :    Loss_Train:  [[ 14.73664075]]    Loss_Validation:  [[ 13.27089571]]\n",
      "Loop  329 :    Loss_Train:  [[ 14.72651492]]    Loss_Validation:  [[ 13.25646666]]\n",
      "Loop  330 :    Loss_Train:  [[ 14.71646948]]    Loss_Validation:  [[ 13.24214806]]\n",
      "Loop  331 :    Loss_Train:  [[ 14.70650362]]    Loss_Validation:  [[ 13.22793894]]\n",
      "Loop  332 :    Loss_Train:  [[ 14.69661653]]    Loss_Validation:  [[ 13.21383834]]\n",
      "Loop  333 :    Loss_Train:  [[ 14.68680742]]    Loss_Validation:  [[ 13.1998453]]\n",
      "Loop  334 :    Loss_Train:  [[ 14.67707551]]    Loss_Validation:  [[ 13.18595887]]\n",
      "Loop  335 :    Loss_Train:  [[ 14.66742002]]    Loss_Validation:  [[ 13.17217812]]\n",
      "Loop  336 :    Loss_Train:  [[ 14.65784018]]    Loss_Validation:  [[ 13.15850212]]\n",
      "Loop  337 :    Loss_Train:  [[ 14.64833522]]    Loss_Validation:  [[ 13.14492995]]\n",
      "Loop  338 :    Loss_Train:  [[ 14.63890441]]    Loss_Validation:  [[ 13.1314607]]\n",
      "Loop  339 :    Loss_Train:  [[ 14.62954699]]    Loss_Validation:  [[ 13.11809347]]\n",
      "Loop  340 :    Loss_Train:  [[ 14.62026223]]    Loss_Validation:  [[ 13.10482737]]\n",
      "Loop  341 :    Loss_Train:  [[ 14.61104942]]    Loss_Validation:  [[ 13.09166152]]\n",
      "Loop  342 :    Loss_Train:  [[ 14.60190782]]    Loss_Validation:  [[ 13.07859504]]\n",
      "Loop  343 :    Loss_Train:  [[ 14.59283673]]    Loss_Validation:  [[ 13.06562707]]\n",
      "Loop  344 :    Loss_Train:  [[ 14.58383545]]    Loss_Validation:  [[ 13.05275674]]\n",
      "Loop  345 :    Loss_Train:  [[ 14.57490329]]    Loss_Validation:  [[ 13.03998323]]\n",
      "Loop  346 :    Loss_Train:  [[ 14.56603956]]    Loss_Validation:  [[ 13.02730567]]\n",
      "Loop  347 :    Loss_Train:  [[ 14.55724358]]    Loss_Validation:  [[ 13.01472324]]\n",
      "Loop  348 :    Loss_Train:  [[ 14.54851469]]    Loss_Validation:  [[ 13.00223512]]\n",
      "Loop  349 :    Loss_Train:  [[ 14.53985221]]    Loss_Validation:  [[ 12.9898405]]\n",
      "Loop  350 :    Loss_Train:  [[ 14.5312555]]    Loss_Validation:  [[ 12.97753855]]\n",
      "Loop  351 :    Loss_Train:  [[ 14.52272391]]    Loss_Validation:  [[ 12.96532849]]\n",
      "Loop  352 :    Loss_Train:  [[ 14.51425679]]    Loss_Validation:  [[ 12.95320952]]\n",
      "Loop  353 :    Loss_Train:  [[ 14.50585352]]    Loss_Validation:  [[ 12.94118086]]\n",
      "Loop  354 :    Loss_Train:  [[ 14.49751346]]    Loss_Validation:  [[ 12.92924174]]\n",
      "Loop  355 :    Loss_Train:  [[ 14.489236]]    Loss_Validation:  [[ 12.91739137]]\n",
      "Loop  356 :    Loss_Train:  [[ 14.48102052]]    Loss_Validation:  [[ 12.905629]]\n",
      "Loop  357 :    Loss_Train:  [[ 14.47286643]]    Loss_Validation:  [[ 12.89395388]]\n",
      "Loop  358 :    Loss_Train:  [[ 14.46477311]]    Loss_Validation:  [[ 12.88236526]]\n",
      "Loop  359 :    Loss_Train:  [[ 14.45673998]]    Loss_Validation:  [[ 12.8708624]]\n",
      "Loop  360 :    Loss_Train:  [[ 14.44876645]]    Loss_Validation:  [[ 12.85944457]]\n",
      "Loop  361 :    Loss_Train:  [[ 14.44085195]]    Loss_Validation:  [[ 12.84811104]]\n",
      "Loop  362 :    Loss_Train:  [[ 14.43299589]]    Loss_Validation:  [[ 12.83686109]]\n",
      "Loop  363 :    Loss_Train:  [[ 14.42519772]]    Loss_Validation:  [[ 12.82569401]]\n",
      "Loop  364 :    Loss_Train:  [[ 14.41745688]]    Loss_Validation:  [[ 12.81460911]]\n",
      "Loop  365 :    Loss_Train:  [[ 14.4097728]]    Loss_Validation:  [[ 12.80360567]]\n",
      "Loop  366 :    Loss_Train:  [[ 14.40214495]]    Loss_Validation:  [[ 12.79268301]]\n",
      "Loop  367 :    Loss_Train:  [[ 14.39457278]]    Loss_Validation:  [[ 12.78184045]]\n",
      "Loop  368 :    Loss_Train:  [[ 14.38705575]]    Loss_Validation:  [[ 12.7710773]]\n",
      "Loop  369 :    Loss_Train:  [[ 14.37959334]]    Loss_Validation:  [[ 12.7603929]]\n",
      "Loop  370 :    Loss_Train:  [[ 14.37218502]]    Loss_Validation:  [[ 12.74978657]]\n",
      "Loop  371 :    Loss_Train:  [[ 14.36483027]]    Loss_Validation:  [[ 12.73925767]]\n",
      "Loop  372 :    Loss_Train:  [[ 14.35752858]]    Loss_Validation:  [[ 12.72880554]]\n",
      "Loop  373 :    Loss_Train:  [[ 14.35027945]]    Loss_Validation:  [[ 12.71842953]]\n",
      "Loop  374 :    Loss_Train:  [[ 14.34308237]]    Loss_Validation:  [[ 12.70812901]]\n",
      "Loop  375 :    Loss_Train:  [[ 14.33593684]]    Loss_Validation:  [[ 12.69790333]]\n",
      "Loop  376 :    Loss_Train:  [[ 14.32884239]]    Loss_Validation:  [[ 12.68775188]]\n",
      "Loop  377 :    Loss_Train:  [[ 14.32179851]]    Loss_Validation:  [[ 12.67767403]]\n",
      "Loop  378 :    Loss_Train:  [[ 14.31480473]]    Loss_Validation:  [[ 12.66766916]]\n",
      "Loop  379 :    Loss_Train:  [[ 14.30786059]]    Loss_Validation:  [[ 12.65773666]]\n",
      "Loop  380 :    Loss_Train:  [[ 14.3009656]]    Loss_Validation:  [[ 12.64787594]]\n",
      "Loop  381 :    Loss_Train:  [[ 14.2941193]]    Loss_Validation:  [[ 12.63808638]]\n",
      "Loop  382 :    Loss_Train:  [[ 14.28732124]]    Loss_Validation:  [[ 12.62836741]]\n",
      "Loop  383 :    Loss_Train:  [[ 14.28057096]]    Loss_Validation:  [[ 12.61871842]]\n",
      "Loop  384 :    Loss_Train:  [[ 14.27386801]]    Loss_Validation:  [[ 12.60913885]]\n",
      "Loop  385 :    Loss_Train:  [[ 14.26721195]]    Loss_Validation:  [[ 12.59962811]]\n",
      "Loop  386 :    Loss_Train:  [[ 14.26060234]]    Loss_Validation:  [[ 12.59018563]]\n",
      "Loop  387 :    Loss_Train:  [[ 14.25403874]]    Loss_Validation:  [[ 12.58081084]]\n",
      "Loop  388 :    Loss_Train:  [[ 14.24752072]]    Loss_Validation:  [[ 12.5715032]]\n",
      "Loop  389 :    Loss_Train:  [[ 14.24104786]]    Loss_Validation:  [[ 12.56226214]]\n",
      "Loop  390 :    Loss_Train:  [[ 14.23461974]]    Loss_Validation:  [[ 12.55308711]]\n",
      "Loop  391 :    Loss_Train:  [[ 14.22823594]]    Loss_Validation:  [[ 12.54397756]]\n",
      "Loop  392 :    Loss_Train:  [[ 14.22189604]]    Loss_Validation:  [[ 12.53493297]]\n",
      "Loop  393 :    Loss_Train:  [[ 14.21559965]]    Loss_Validation:  [[ 12.52595279]]\n",
      "Loop  394 :    Loss_Train:  [[ 14.20934636]]    Loss_Validation:  [[ 12.5170365]]\n",
      "Loop  395 :    Loss_Train:  [[ 14.20313577]]    Loss_Validation:  [[ 12.50818356]]\n",
      "Loop  396 :    Loss_Train:  [[ 14.19696749]]    Loss_Validation:  [[ 12.49939347]]\n",
      "Loop  397 :    Loss_Train:  [[ 14.19084112]]    Loss_Validation:  [[ 12.49066571]]\n",
      "Loop  398 :    Loss_Train:  [[ 14.18475629]]    Loss_Validation:  [[ 12.48199976]]\n",
      "Loop  399 :    Loss_Train:  [[ 14.1787126]]    Loss_Validation:  [[ 12.47339512]]\n",
      "Loop  400 :    Loss_Train:  [[ 14.17270968]]    Loss_Validation:  [[ 12.46485129]]\n",
      "Loop  401 :    Loss_Train:  [[ 14.16674716]]    Loss_Validation:  [[ 12.45636778]]\n",
      "Loop  402 :    Loss_Train:  [[ 14.16082467]]    Loss_Validation:  [[ 12.44794409]]\n",
      "Loop  403 :    Loss_Train:  [[ 14.15494184]]    Loss_Validation:  [[ 12.43957974]]\n",
      "Loop  404 :    Loss_Train:  [[ 14.14909832]]    Loss_Validation:  [[ 12.43127425]]\n",
      "Loop  405 :    Loss_Train:  [[ 14.14329373]]    Loss_Validation:  [[ 12.42302713]]\n",
      "Loop  406 :    Loss_Train:  [[ 14.13752774]]    Loss_Validation:  [[ 12.41483792]]\n",
      "Loop  407 :    Loss_Train:  [[ 14.13179998]]    Loss_Validation:  [[ 12.40670614]]\n",
      "Loop  408 :    Loss_Train:  [[ 14.12611012]]    Loss_Validation:  [[ 12.39863134]]\n",
      "Loop  409 :    Loss_Train:  [[ 14.1204578]]    Loss_Validation:  [[ 12.39061304]]\n",
      "Loop  410 :    Loss_Train:  [[ 14.1148427]]    Loss_Validation:  [[ 12.3826508]]\n",
      "Loop  411 :    Loss_Train:  [[ 14.10926447]]    Loss_Validation:  [[ 12.37474416]]\n",
      "Loop  412 :    Loss_Train:  [[ 14.10372278]]    Loss_Validation:  [[ 12.36689267]]\n",
      "Loop  413 :    Loss_Train:  [[ 14.0982173]]    Loss_Validation:  [[ 12.3590959]]\n",
      "Loop  414 :    Loss_Train:  [[ 14.09274772]]    Loss_Validation:  [[ 12.3513534]]\n",
      "Loop  415 :    Loss_Train:  [[ 14.0873137]]    Loss_Validation:  [[ 12.34366473]]\n",
      "Loop  416 :    Loss_Train:  [[ 14.08191493]]    Loss_Validation:  [[ 12.33602947]]\n",
      "Loop  417 :    Loss_Train:  [[ 14.07655109]]    Loss_Validation:  [[ 12.32844719]]\n",
      "Loop  418 :    Loss_Train:  [[ 14.07122187]]    Loss_Validation:  [[ 12.32091746]]\n",
      "Loop  419 :    Loss_Train:  [[ 14.06592697]]    Loss_Validation:  [[ 12.31343986]]\n",
      "Loop  420 :    Loss_Train:  [[ 14.06066608]]    Loss_Validation:  [[ 12.30601398]]\n",
      "Loop  421 :    Loss_Train:  [[ 14.05543889]]    Loss_Validation:  [[ 12.2986394]]\n",
      "Loop  422 :    Loss_Train:  [[ 14.05024511]]    Loss_Validation:  [[ 12.29131572]]\n",
      "Loop  423 :    Loss_Train:  [[ 14.04508444]]    Loss_Validation:  [[ 12.28404253]]\n",
      "Loop  424 :    Loss_Train:  [[ 14.03995659]]    Loss_Validation:  [[ 12.27681943]]\n",
      "Loop  425 :    Loss_Train:  [[ 14.03486127]]    Loss_Validation:  [[ 12.26964603]]\n",
      "Loop  426 :    Loss_Train:  [[ 14.0297982]]    Loss_Validation:  [[ 12.26252192]]\n",
      "Loop  427 :    Loss_Train:  [[ 14.02476708]]    Loss_Validation:  [[ 12.25544672]]\n",
      "Loop  428 :    Loss_Train:  [[ 14.01976765]]    Loss_Validation:  [[ 12.24842005]]\n",
      "Loop  429 :    Loss_Train:  [[ 14.01479961]]    Loss_Validation:  [[ 12.24144151]]\n",
      "Loop  430 :    Loss_Train:  [[ 14.00986271]]    Loss_Validation:  [[ 12.23451073]]\n",
      "Loop  431 :    Loss_Train:  [[ 14.00495665]]    Loss_Validation:  [[ 12.22762733]]\n",
      "Loop  432 :    Loss_Train:  [[ 14.00008119]]    Loss_Validation:  [[ 12.22079094]]\n",
      "Loop  433 :    Loss_Train:  [[ 13.99523604]]    Loss_Validation:  [[ 12.21400119]]\n",
      "Loop  434 :    Loss_Train:  [[ 13.99042095]]    Loss_Validation:  [[ 12.20725772]]\n",
      "Loop  435 :    Loss_Train:  [[ 13.98563566]]    Loss_Validation:  [[ 12.20056015]]\n",
      "Loop  436 :    Loss_Train:  [[ 13.9808799]]    Loss_Validation:  [[ 12.19390813]]\n",
      "Loop  437 :    Loss_Train:  [[ 13.97615343]]    Loss_Validation:  [[ 12.1873013]]\n",
      "Loop  438 :    Loss_Train:  [[ 13.97145598]]    Loss_Validation:  [[ 12.18073932]]\n",
      "Loop  439 :    Loss_Train:  [[ 13.96678732]]    Loss_Validation:  [[ 12.17422182]]\n",
      "Loop  440 :    Loss_Train:  [[ 13.96214718]]    Loss_Validation:  [[ 12.16774847]]\n",
      "Loop  441 :    Loss_Train:  [[ 13.95753533]]    Loss_Validation:  [[ 12.16131891]]\n",
      "Loop  442 :    Loss_Train:  [[ 13.95295152]]    Loss_Validation:  [[ 12.15493281]]\n",
      "Loop  443 :    Loss_Train:  [[ 13.94839552]]    Loss_Validation:  [[ 12.14858983]]\n",
      "Loop  444 :    Loss_Train:  [[ 13.94386708]]    Loss_Validation:  [[ 12.14228963]]\n",
      "Loop  445 :    Loss_Train:  [[ 13.93936598]]    Loss_Validation:  [[ 12.13603188]]\n",
      "Loop  446 :    Loss_Train:  [[ 13.93489197]]    Loss_Validation:  [[ 12.12981626]]\n",
      "Loop  447 :    Loss_Train:  [[ 13.93044483]]    Loss_Validation:  [[ 12.12364243]]\n",
      "Loop  448 :    Loss_Train:  [[ 13.92602433]]    Loss_Validation:  [[ 12.11751007]]\n",
      "Loop  449 :    Loss_Train:  [[ 13.92163024]]    Loss_Validation:  [[ 12.11141887]]\n",
      "Loop  450 :    Loss_Train:  [[ 13.91726234]]    Loss_Validation:  [[ 12.1053685]]\n",
      "Loop  451 :    Loss_Train:  [[ 13.91292041]]    Loss_Validation:  [[ 12.09935865]]\n",
      "Loop  452 :    Loss_Train:  [[ 13.90860423]]    Loss_Validation:  [[ 12.09338901]]\n",
      "Loop  453 :    Loss_Train:  [[ 13.90431358]]    Loss_Validation:  [[ 12.08745927]]\n",
      "Loop  454 :    Loss_Train:  [[ 13.90004825]]    Loss_Validation:  [[ 12.08156912]]\n",
      "Loop  455 :    Loss_Train:  [[ 13.89580803]]    Loss_Validation:  [[ 12.07571826]]\n",
      "Loop  456 :    Loss_Train:  [[ 13.8915927]]    Loss_Validation:  [[ 12.06990638]]\n",
      "Loop  457 :    Loss_Train:  [[ 13.88740206]]    Loss_Validation:  [[ 12.0641332]]\n",
      "Loop  458 :    Loss_Train:  [[ 13.8832359]]    Loss_Validation:  [[ 12.0583984]]\n",
      "Loop  459 :    Loss_Train:  [[ 13.87909402]]    Loss_Validation:  [[ 12.05270171]]\n",
      "Loop  460 :    Loss_Train:  [[ 13.87497622]]    Loss_Validation:  [[ 12.04704282]]\n",
      "Loop  461 :    Loss_Train:  [[ 13.87088229]]    Loss_Validation:  [[ 12.04142145]]\n",
      "Loop  462 :    Loss_Train:  [[ 13.86681204]]    Loss_Validation:  [[ 12.03583732]]\n",
      "Loop  463 :    Loss_Train:  [[ 13.86276527]]    Loss_Validation:  [[ 12.03029013]]\n",
      "Loop  464 :    Loss_Train:  [[ 13.85874179]]    Loss_Validation:  [[ 12.02477962]]\n",
      "Loop  465 :    Loss_Train:  [[ 13.85474141]]    Loss_Validation:  [[ 12.01930549]]\n",
      "Loop  466 :    Loss_Train:  [[ 13.85076394]]    Loss_Validation:  [[ 12.01386749]]\n",
      "Loop  467 :    Loss_Train:  [[ 13.84680918]]    Loss_Validation:  [[ 12.00846532]]\n",
      "Loop  468 :    Loss_Train:  [[ 13.84287695]]    Loss_Validation:  [[ 12.00309872]]\n",
      "Loop  469 :    Loss_Train:  [[ 13.83896707]]    Loss_Validation:  [[ 11.99776743]]\n",
      "Loop  470 :    Loss_Train:  [[ 13.83507936]]    Loss_Validation:  [[ 11.99247117]]\n",
      "Loop  471 :    Loss_Train:  [[ 13.83121363]]    Loss_Validation:  [[ 11.98720968]]\n",
      "Loop  472 :    Loss_Train:  [[ 13.82736971]]    Loss_Validation:  [[ 11.9819827]]\n",
      "Loop  473 :    Loss_Train:  [[ 13.82354741]]    Loss_Validation:  [[ 11.97678997]]\n",
      "Loop  474 :    Loss_Train:  [[ 13.81974657]]    Loss_Validation:  [[ 11.97163123]]\n",
      "Loop  475 :    Loss_Train:  [[ 13.815967]]    Loss_Validation:  [[ 11.96650623]]\n",
      "Loop  476 :    Loss_Train:  [[ 13.81220855]]    Loss_Validation:  [[ 11.96141471]]\n",
      "Loop  477 :    Loss_Train:  [[ 13.80847103]]    Loss_Validation:  [[ 11.95635642]]\n",
      "Loop  478 :    Loss_Train:  [[ 13.80475427]]    Loss_Validation:  [[ 11.95133112]]\n",
      "Loop  479 :    Loss_Train:  [[ 13.80105812]]    Loss_Validation:  [[ 11.94633855]]\n",
      "Loop  480 :    Loss_Train:  [[ 13.79738241]]    Loss_Validation:  [[ 11.94137848]]\n",
      "Loop  481 :    Loss_Train:  [[ 13.79372696]]    Loss_Validation:  [[ 11.93645066]]\n",
      "Loop  482 :    Loss_Train:  [[ 13.79009163]]    Loss_Validation:  [[ 11.93155484]]\n",
      "Loop  483 :    Loss_Train:  [[ 13.78647624]]    Loss_Validation:  [[ 11.9266908]]\n",
      "Loop  484 :    Loss_Train:  [[ 13.78288065]]    Loss_Validation:  [[ 11.92185829]]\n",
      "Loop  485 :    Loss_Train:  [[ 13.77930469]]    Loss_Validation:  [[ 11.91705708]]\n",
      "Loop  486 :    Loss_Train:  [[ 13.7757482]]    Loss_Validation:  [[ 11.91228695]]\n",
      "Loop  487 :    Loss_Train:  [[ 13.77221104]]    Loss_Validation:  [[ 11.90754764]]\n",
      "Loop  488 :    Loss_Train:  [[ 13.76869305]]    Loss_Validation:  [[ 11.90283895]]\n",
      "Loop  489 :    Loss_Train:  [[ 13.76519408]]    Loss_Validation:  [[ 11.89816065]]\n",
      "Loop  490 :    Loss_Train:  [[ 13.76171397]]    Loss_Validation:  [[ 11.8935125]]\n",
      "Loop  491 :    Loss_Train:  [[ 13.75825259]]    Loss_Validation:  [[ 11.88889429]]\n",
      "Loop  492 :    Loss_Train:  [[ 13.75480978]]    Loss_Validation:  [[ 11.88430579]]\n",
      "Loop  493 :    Loss_Train:  [[ 13.7513854]]    Loss_Validation:  [[ 11.87974679]]\n",
      "Loop  494 :    Loss_Train:  [[ 13.7479793]]    Loss_Validation:  [[ 11.87521707]]\n",
      "Loop  495 :    Loss_Train:  [[ 13.74459135]]    Loss_Validation:  [[ 11.87071642]]\n",
      "Loop  496 :    Loss_Train:  [[ 13.74122139]]    Loss_Validation:  [[ 11.86624461]]\n",
      "Loop  497 :    Loss_Train:  [[ 13.73786929]]    Loss_Validation:  [[ 11.86180145]]\n",
      "Loop  498 :    Loss_Train:  [[ 13.73453492]]    Loss_Validation:  [[ 11.85738672]]\n",
      "Loop  499 :    Loss_Train:  [[ 13.73121812]]    Loss_Validation:  [[ 11.8530002]]\n",
      "Loop  500 :    Loss_Train:  [[ 13.72791878]]    Loss_Validation:  [[ 11.84864171]]\n",
      "Loop  501 :    Loss_Train:  [[ 13.72463675]]    Loss_Validation:  [[ 11.84431102]]\n",
      "Loop  502 :    Loss_Train:  [[ 13.7213719]]    Loss_Validation:  [[ 11.84000794]]\n",
      "Loop  503 :    Loss_Train:  [[ 13.7181241]]    Loss_Validation:  [[ 11.83573227]]\n",
      "Loop  504 :    Loss_Train:  [[ 13.71489321]]    Loss_Validation:  [[ 11.83148381]]\n",
      "Loop  505 :    Loss_Train:  [[ 13.71167911]]    Loss_Validation:  [[ 11.82726236]]\n",
      "Loop  506 :    Loss_Train:  [[ 13.70848167]]    Loss_Validation:  [[ 11.82306772]]\n",
      "Loop  507 :    Loss_Train:  [[ 13.70530076]]    Loss_Validation:  [[ 11.8188997]]\n",
      "Loop  508 :    Loss_Train:  [[ 13.70213626]]    Loss_Validation:  [[ 11.8147581]]\n",
      "Loop  509 :    Loss_Train:  [[ 13.69898803]]    Loss_Validation:  [[ 11.81064274]]\n",
      "Loop  510 :    Loss_Train:  [[ 13.69585597]]    Loss_Validation:  [[ 11.80655343]]\n",
      "Loop  511 :    Loss_Train:  [[ 13.69273994]]    Loss_Validation:  [[ 11.80248997]]\n",
      "Loop  512 :    Loss_Train:  [[ 13.68963982]]    Loss_Validation:  [[ 11.79845218]]\n",
      "Loop  513 :    Loss_Train:  [[ 13.6865555]]    Loss_Validation:  [[ 11.79443988]]\n",
      "Loop  514 :    Loss_Train:  [[ 13.68348685]]    Loss_Validation:  [[ 11.79045287]]\n",
      "Loop  515 :    Loss_Train:  [[ 13.68043376]]    Loss_Validation:  [[ 11.78649099]]\n",
      "Loop  516 :    Loss_Train:  [[ 13.67739611]]    Loss_Validation:  [[ 11.78255405]]\n",
      "Loop  517 :    Loss_Train:  [[ 13.67437378]]    Loss_Validation:  [[ 11.77864186]]\n",
      "Loop  518 :    Loss_Train:  [[ 13.67136667]]    Loss_Validation:  [[ 11.77475426]]\n",
      "Loop  519 :    Loss_Train:  [[ 13.66837465]]    Loss_Validation:  [[ 11.77089107]]\n",
      "Loop  520 :    Loss_Train:  [[ 13.66539761]]    Loss_Validation:  [[ 11.76705211]]\n",
      "Loop  521 :    Loss_Train:  [[ 13.66243545]]    Loss_Validation:  [[ 11.7632372]]\n",
      "Loop  522 :    Loss_Train:  [[ 13.65948805]]    Loss_Validation:  [[ 11.75944618]]\n",
      "Loop  523 :    Loss_Train:  [[ 13.6565553]]    Loss_Validation:  [[ 11.75567888]]\n",
      "Loop  524 :    Loss_Train:  [[ 13.6536371]]    Loss_Validation:  [[ 11.75193513]]\n",
      "Loop  525 :    Loss_Train:  [[ 13.65073334]]    Loss_Validation:  [[ 11.74821475]]\n",
      "Loop  526 :    Loss_Train:  [[ 13.64784391]]    Loss_Validation:  [[ 11.74451759]]\n",
      "Loop  527 :    Loss_Train:  [[ 13.6449687]]    Loss_Validation:  [[ 11.74084348]]\n",
      "Loop  528 :    Loss_Train:  [[ 13.64210762]]    Loss_Validation:  [[ 11.73719225]]\n",
      "Loop  529 :    Loss_Train:  [[ 13.63926055]]    Loss_Validation:  [[ 11.73356374]]\n",
      "Loop  530 :    Loss_Train:  [[ 13.6364274]]    Loss_Validation:  [[ 11.7299578]]\n",
      "Loop  531 :    Loss_Train:  [[ 13.63360807]]    Loss_Validation:  [[ 11.72637426]]\n",
      "Loop  532 :    Loss_Train:  [[ 13.63080245]]    Loss_Validation:  [[ 11.72281296]]\n",
      "Loop  533 :    Loss_Train:  [[ 13.62801044]]    Loss_Validation:  [[ 11.71927375]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  534 :    Loss_Train:  [[ 13.62523195]]    Loss_Validation:  [[ 11.71575647]]\n",
      "Loop  535 :    Loss_Train:  [[ 13.62246688]]    Loss_Validation:  [[ 11.71226096]]\n",
      "Loop  536 :    Loss_Train:  [[ 13.61971513]]    Loss_Validation:  [[ 11.70878708]]\n",
      "Loop  537 :    Loss_Train:  [[ 13.61697661]]    Loss_Validation:  [[ 11.70533467]]\n",
      "Loop  538 :    Loss_Train:  [[ 13.61425122]]    Loss_Validation:  [[ 11.70190358]]\n",
      "Loop  539 :    Loss_Train:  [[ 13.61153886]]    Loss_Validation:  [[ 11.69849367]]\n",
      "Loop  540 :    Loss_Train:  [[ 13.60883945]]    Loss_Validation:  [[ 11.69510477]]\n",
      "Loop  541 :    Loss_Train:  [[ 13.60615289]]    Loss_Validation:  [[ 11.69173675]]\n",
      "Loop  542 :    Loss_Train:  [[ 13.60347909]]    Loss_Validation:  [[ 11.68838946]]\n",
      "Loop  543 :    Loss_Train:  [[ 13.60081795]]    Loss_Validation:  [[ 11.68506275]]\n",
      "Loop  544 :    Loss_Train:  [[ 13.59816939]]    Loss_Validation:  [[ 11.68175649]]\n",
      "Loop  545 :    Loss_Train:  [[ 13.59553333]]    Loss_Validation:  [[ 11.67847052]]\n",
      "Loop  546 :    Loss_Train:  [[ 13.59290966]]    Loss_Validation:  [[ 11.67520471]]\n",
      "Loop  547 :    Loss_Train:  [[ 13.59029831]]    Loss_Validation:  [[ 11.67195891]]\n",
      "Loop  548 :    Loss_Train:  [[ 13.58769918]]    Loss_Validation:  [[ 11.668733]]\n",
      "Loop  549 :    Loss_Train:  [[ 13.5851122]]    Loss_Validation:  [[ 11.66552682]]\n",
      "Loop  550 :    Loss_Train:  [[ 13.58253726]]    Loss_Validation:  [[ 11.66234024]]\n",
      "Loop  551 :    Loss_Train:  [[ 13.5799743]]    Loss_Validation:  [[ 11.65917314]]\n",
      "Loop  552 :    Loss_Train:  [[ 13.57742323]]    Loss_Validation:  [[ 11.65602536]]\n",
      "Loop  553 :    Loss_Train:  [[ 13.57488396]]    Loss_Validation:  [[ 11.65289678]]\n",
      "Loop  554 :    Loss_Train:  [[ 13.57235642]]    Loss_Validation:  [[ 11.64978727]]\n",
      "Loop  555 :    Loss_Train:  [[ 13.56984051]]    Loss_Validation:  [[ 11.64669669]]\n",
      "Loop  556 :    Loss_Train:  [[ 13.56733616]]    Loss_Validation:  [[ 11.64362492]]\n",
      "Loop  557 :    Loss_Train:  [[ 13.5648433]]    Loss_Validation:  [[ 11.64057181]]\n",
      "Loop  558 :    Loss_Train:  [[ 13.56236183]]    Loss_Validation:  [[ 11.63753726]]\n",
      "Loop  559 :    Loss_Train:  [[ 13.55989169]]    Loss_Validation:  [[ 11.63452112]]\n",
      "Loop  560 :    Loss_Train:  [[ 13.55743279]]    Loss_Validation:  [[ 11.63152328]]\n",
      "Loop  561 :    Loss_Train:  [[ 13.55498506]]    Loss_Validation:  [[ 11.6285436]]\n",
      "Loop  562 :    Loss_Train:  [[ 13.55254842]]    Loss_Validation:  [[ 11.62558196]]\n",
      "Loop  563 :    Loss_Train:  [[ 13.55012279]]    Loss_Validation:  [[ 11.62263824]]\n",
      "Loop  564 :    Loss_Train:  [[ 13.54770811]]    Loss_Validation:  [[ 11.61971231]]\n",
      "Loop  565 :    Loss_Train:  [[ 13.54530428]]    Loss_Validation:  [[ 11.61680406]]\n",
      "Loop  566 :    Loss_Train:  [[ 13.54291125]]    Loss_Validation:  [[ 11.61391337]]\n",
      "Loop  567 :    Loss_Train:  [[ 13.54052894]]    Loss_Validation:  [[ 11.6110401]]\n",
      "Loop  568 :    Loss_Train:  [[ 13.53815728]]    Loss_Validation:  [[ 11.60818416]]\n",
      "Loop  569 :    Loss_Train:  [[ 13.53579618]]    Loss_Validation:  [[ 11.6053454]]\n",
      "Loop  570 :    Loss_Train:  [[ 13.53344559]]    Loss_Validation:  [[ 11.60252373]]\n",
      "Loop  571 :    Loss_Train:  [[ 13.53110543]]    Loss_Validation:  [[ 11.59971903]]\n",
      "Loop  572 :    Loss_Train:  [[ 13.52877564]]    Loss_Validation:  [[ 11.59693117]]\n",
      "Loop  573 :    Loss_Train:  [[ 13.52645613]]    Loss_Validation:  [[ 11.59416005]]\n",
      "Loop  574 :    Loss_Train:  [[ 13.52414685]]    Loss_Validation:  [[ 11.59140555]]\n",
      "Loop  575 :    Loss_Train:  [[ 13.52184773]]    Loss_Validation:  [[ 11.58866755]]\n",
      "Loop  576 :    Loss_Train:  [[ 13.51955869]]    Loss_Validation:  [[ 11.58594596]]\n",
      "Loop  577 :    Loss_Train:  [[ 13.51727968]]    Loss_Validation:  [[ 11.58324065]]\n",
      "Loop  578 :    Loss_Train:  [[ 13.51501061]]    Loss_Validation:  [[ 11.58055152]]\n",
      "Loop  579 :    Loss_Train:  [[ 13.51275144]]    Loss_Validation:  [[ 11.57787846]]\n",
      "Loop  580 :    Loss_Train:  [[ 13.51050209]]    Loss_Validation:  [[ 11.57522136]]\n",
      "Loop  581 :    Loss_Train:  [[ 13.5082625]]    Loss_Validation:  [[ 11.57258011]]\n",
      "Loop  582 :    Loss_Train:  [[ 13.5060326]]    Loss_Validation:  [[ 11.56995461]]\n",
      "Loop  583 :    Loss_Train:  [[ 13.50381234]]    Loss_Validation:  [[ 11.56734475]]\n",
      "Loop  584 :    Loss_Train:  [[ 13.50160164]]    Loss_Validation:  [[ 11.56475042]]\n",
      "Loop  585 :    Loss_Train:  [[ 13.49940044]]    Loss_Validation:  [[ 11.56217153]]\n",
      "Loop  586 :    Loss_Train:  [[ 13.49720869]]    Loss_Validation:  [[ 11.55960796]]\n",
      "Loop  587 :    Loss_Train:  [[ 13.49502633]]    Loss_Validation:  [[ 11.55705962]]\n",
      "Loop  588 :    Loss_Train:  [[ 13.49285328]]    Loss_Validation:  [[ 11.5545264]]\n",
      "Loop  589 :    Loss_Train:  [[ 13.49068949]]    Loss_Validation:  [[ 11.55200821]]\n",
      "Loop  590 :    Loss_Train:  [[ 13.48853491]]    Loss_Validation:  [[ 11.54950494]]\n",
      "Loop  591 :    Loss_Train:  [[ 13.48638946]]    Loss_Validation:  [[ 11.5470165]]\n",
      "Loop  592 :    Loss_Train:  [[ 13.4842531]]    Loss_Validation:  [[ 11.54454278]]\n",
      "Loop  593 :    Loss_Train:  [[ 13.48212576]]    Loss_Validation:  [[ 11.54208369]]\n",
      "Loop  594 :    Loss_Train:  [[ 13.48000738]]    Loss_Validation:  [[ 11.53963913]]\n",
      "Loop  595 :    Loss_Train:  [[ 13.47789792]]    Loss_Validation:  [[ 11.537209]]\n",
      "Loop  596 :    Loss_Train:  [[ 13.47579731]]    Loss_Validation:  [[ 11.53479321]]\n",
      "Loop  597 :    Loss_Train:  [[ 13.47370549]]    Loss_Validation:  [[ 11.53239167]]\n",
      "Loop  598 :    Loss_Train:  [[ 13.47162241]]    Loss_Validation:  [[ 11.53000427]]\n",
      "Loop  599 :    Loss_Train:  [[ 13.46954802]]    Loss_Validation:  [[ 11.52763093]]\n",
      "Loop  600 :    Loss_Train:  [[ 13.46748225]]    Loss_Validation:  [[ 11.52527156]]\n",
      "Loop  601 :    Loss_Train:  [[ 13.46542506]]    Loss_Validation:  [[ 11.52292605]]\n",
      "Loop  602 :    Loss_Train:  [[ 13.46337639]]    Loss_Validation:  [[ 11.52059432]]\n",
      "Loop  603 :    Loss_Train:  [[ 13.46133619]]    Loss_Validation:  [[ 11.51827629]]\n",
      "Loop  604 :    Loss_Train:  [[ 13.4593044]]    Loss_Validation:  [[ 11.51597185]]\n",
      "Loop  605 :    Loss_Train:  [[ 13.45728097]]    Loss_Validation:  [[ 11.51368092]]\n",
      "Loop  606 :    Loss_Train:  [[ 13.45526585]]    Loss_Validation:  [[ 11.51140342]]\n",
      "Loop  607 :    Loss_Train:  [[ 13.45325899]]    Loss_Validation:  [[ 11.50913925]]\n",
      "Loop  608 :    Loss_Train:  [[ 13.45126033]]    Loss_Validation:  [[ 11.50688832]]\n",
      "Loop  609 :    Loss_Train:  [[ 13.44926982]]    Loss_Validation:  [[ 11.50465055]]\n",
      "Loop  610 :    Loss_Train:  [[ 13.44728742]]    Loss_Validation:  [[ 11.50242586]]\n",
      "Loop  611 :    Loss_Train:  [[ 13.44531307]]    Loss_Validation:  [[ 11.50021415]]\n",
      "Loop  612 :    Loss_Train:  [[ 13.44334673]]    Loss_Validation:  [[ 11.49801534]]\n",
      "Loop  613 :    Loss_Train:  [[ 13.44138834]]    Loss_Validation:  [[ 11.49582936]]\n",
      "Loop  614 :    Loss_Train:  [[ 13.43943786]]    Loss_Validation:  [[ 11.49365611]]\n",
      "Loop  615 :    Loss_Train:  [[ 13.43749523]]    Loss_Validation:  [[ 11.49149551]]\n",
      "Loop  616 :    Loss_Train:  [[ 13.43556041]]    Loss_Validation:  [[ 11.48934749]]\n",
      "Loop  617 :    Loss_Train:  [[ 13.43363335]]    Loss_Validation:  [[ 11.48721195]]\n",
      "Loop  618 :    Loss_Train:  [[ 13.43171401]]    Loss_Validation:  [[ 11.48508882]]\n",
      "Loop  619 :    Loss_Train:  [[ 13.42980233]]    Loss_Validation:  [[ 11.48297802]]\n",
      "Loop  620 :    Loss_Train:  [[ 13.42789828]]    Loss_Validation:  [[ 11.48087947]]\n",
      "Loop  621 :    Loss_Train:  [[ 13.42600179]]    Loss_Validation:  [[ 11.47879308]]\n",
      "Loop  622 :    Loss_Train:  [[ 13.42411284]]    Loss_Validation:  [[ 11.47671879]]\n",
      "Loop  623 :    Loss_Train:  [[ 13.42223137]]    Loss_Validation:  [[ 11.4746565]]\n",
      "Loop  624 :    Loss_Train:  [[ 13.42035733]]    Loss_Validation:  [[ 11.47260615]]\n",
      "Loop  625 :    Loss_Train:  [[ 13.41849069]]    Loss_Validation:  [[ 11.47056766]]\n",
      "Loop  626 :    Loss_Train:  [[ 13.4166314]]    Loss_Validation:  [[ 11.46854095]]\n",
      "Loop  627 :    Loss_Train:  [[ 13.41477941]]    Loss_Validation:  [[ 11.46652594]]\n",
      "Loop  628 :    Loss_Train:  [[ 13.41293469]]    Loss_Validation:  [[ 11.46452257]]\n",
      "Loop  629 :    Loss_Train:  [[ 13.41109718]]    Loss_Validation:  [[ 11.46253075]]\n",
      "Loop  630 :    Loss_Train:  [[ 13.40926684]]    Loss_Validation:  [[ 11.46055041]]\n",
      "Loop  631 :    Loss_Train:  [[ 13.40744364]]    Loss_Validation:  [[ 11.45858148]]\n",
      "Loop  632 :    Loss_Train:  [[ 13.40562753]]    Loss_Validation:  [[ 11.45662388]]\n",
      "Loop  633 :    Loss_Train:  [[ 13.40381846]]    Loss_Validation:  [[ 11.45467755]]\n",
      "Loop  634 :    Loss_Train:  [[ 13.40201641]]    Loss_Validation:  [[ 11.4527424]]\n",
      "Loop  635 :    Loss_Train:  [[ 13.40022131]]    Loss_Validation:  [[ 11.45081838]]\n",
      "Loop  636 :    Loss_Train:  [[ 13.39843315]]    Loss_Validation:  [[ 11.4489054]]\n",
      "Loop  637 :    Loss_Train:  [[ 13.39665186]]    Loss_Validation:  [[ 11.4470034]]\n",
      "Loop  638 :    Loss_Train:  [[ 13.39487742]]    Loss_Validation:  [[ 11.44511231]]\n",
      "Loop  639 :    Loss_Train:  [[ 13.39310979]]    Loss_Validation:  [[ 11.44323206]]\n",
      "Loop  640 :    Loss_Train:  [[ 13.39134891]]    Loss_Validation:  [[ 11.44136257]]\n",
      "Loop  641 :    Loss_Train:  [[ 13.38959476]]    Loss_Validation:  [[ 11.43950379]]\n",
      "Loop  642 :    Loss_Train:  [[ 13.3878473]]    Loss_Validation:  [[ 11.43765564]]\n",
      "Loop  643 :    Loss_Train:  [[ 13.38610649]]    Loss_Validation:  [[ 11.43581805]]\n",
      "Loop  644 :    Loss_Train:  [[ 13.38437228]]    Loss_Validation:  [[ 11.43399097]]\n",
      "Loop  645 :    Loss_Train:  [[ 13.38264464]]    Loss_Validation:  [[ 11.43217432]]\n",
      "Loop  646 :    Loss_Train:  [[ 13.38092354]]    Loss_Validation:  [[ 11.43036803]]\n",
      "Loop  647 :    Loss_Train:  [[ 13.37920893]]    Loss_Validation:  [[ 11.42857204]]\n",
      "Loop  648 :    Loss_Train:  [[ 13.37750078]]    Loss_Validation:  [[ 11.42678629]]\n",
      "Loop  649 :    Loss_Train:  [[ 13.37579905]]    Loss_Validation:  [[ 11.42501072]]\n",
      "Loop  650 :    Loss_Train:  [[ 13.3741037]]    Loss_Validation:  [[ 11.42324525]]\n",
      "Loop  651 :    Loss_Train:  [[ 13.3724147]]    Loss_Validation:  [[ 11.42148982]]\n",
      "Loop  652 :    Loss_Train:  [[ 13.37073202]]    Loss_Validation:  [[ 11.41974438]]\n",
      "Loop  653 :    Loss_Train:  [[ 13.36905561]]    Loss_Validation:  [[ 11.41800885]]\n",
      "Loop  654 :    Loss_Train:  [[ 13.36738544]]    Loss_Validation:  [[ 11.41628318]]\n",
      "Loop  655 :    Loss_Train:  [[ 13.36572148]]    Loss_Validation:  [[ 11.4145673]]\n",
      "Loop  656 :    Loss_Train:  [[ 13.36406369]]    Loss_Validation:  [[ 11.41286116]]\n",
      "Loop  657 :    Loss_Train:  [[ 13.36241203]]    Loss_Validation:  [[ 11.41116469]]\n",
      "Loop  658 :    Loss_Train:  [[ 13.36076648]]    Loss_Validation:  [[ 11.40947783]]\n",
      "Loop  659 :    Loss_Train:  [[ 13.35912699]]    Loss_Validation:  [[ 11.40780052]]\n",
      "Loop  660 :    Loss_Train:  [[ 13.35749354]]    Loss_Validation:  [[ 11.4061327]]\n",
      "Loop  661 :    Loss_Train:  [[ 13.35586608]]    Loss_Validation:  [[ 11.40447432]]\n",
      "Loop  662 :    Loss_Train:  [[ 13.3542446]]    Loss_Validation:  [[ 11.40282531]]\n",
      "Loop  663 :    Loss_Train:  [[ 13.35262904]]    Loss_Validation:  [[ 11.40118561]]\n",
      "Loop  664 :    Loss_Train:  [[ 13.35101939]]    Loss_Validation:  [[ 11.39955517]]\n",
      "Loop  665 :    Loss_Train:  [[ 13.34941561]]    Loss_Validation:  [[ 11.39793394]]\n",
      "Loop  666 :    Loss_Train:  [[ 13.34781766]]    Loss_Validation:  [[ 11.39632184]]\n",
      "Loop  667 :    Loss_Train:  [[ 13.34622551]]    Loss_Validation:  [[ 11.39471884]]\n",
      "Loop  668 :    Loss_Train:  [[ 13.34463914]]    Loss_Validation:  [[ 11.39312486]]\n",
      "Loop  669 :    Loss_Train:  [[ 13.3430585]]    Loss_Validation:  [[ 11.39153986]]\n",
      "Loop  670 :    Loss_Train:  [[ 13.34148358]]    Loss_Validation:  [[ 11.38996377]]\n",
      "Loop  671 :    Loss_Train:  [[ 13.33991433]]    Loss_Validation:  [[ 11.38839655]]\n",
      "Loop  672 :    Loss_Train:  [[ 13.33835073]]    Loss_Validation:  [[ 11.38683814]]\n",
      "Loop  673 :    Loss_Train:  [[ 13.33679274]]    Loss_Validation:  [[ 11.38528848]]\n",
      "Loop  674 :    Loss_Train:  [[ 13.33524035]]    Loss_Validation:  [[ 11.38374752]]\n",
      "Loop  675 :    Loss_Train:  [[ 13.3336935]]    Loss_Validation:  [[ 11.38221521]]\n",
      "Loop  676 :    Loss_Train:  [[ 13.33215218]]    Loss_Validation:  [[ 11.3806915]]\n",
      "Loop  677 :    Loss_Train:  [[ 13.33061636]]    Loss_Validation:  [[ 11.37917632]]\n",
      "Loop  678 :    Loss_Train:  [[ 13.329086]]    Loss_Validation:  [[ 11.37766964]]\n",
      "Loop  679 :    Loss_Train:  [[ 13.32756108]]    Loss_Validation:  [[ 11.37617139]]\n",
      "Loop  680 :    Loss_Train:  [[ 13.32604156]]    Loss_Validation:  [[ 11.37468152]]\n",
      "Loop  681 :    Loss_Train:  [[ 13.32452743]]    Loss_Validation:  [[ 11.37319999]]\n",
      "Loop  682 :    Loss_Train:  [[ 13.32301864]]    Loss_Validation:  [[ 11.37172674]]\n",
      "Loop  683 :    Loss_Train:  [[ 13.32151518]]    Loss_Validation:  [[ 11.37026172]]\n",
      "Loop  684 :    Loss_Train:  [[ 13.320017]]    Loss_Validation:  [[ 11.36880489]]\n",
      "Loop  685 :    Loss_Train:  [[ 13.3185241]]    Loss_Validation:  [[ 11.36735618]]\n",
      "Loop  686 :    Loss_Train:  [[ 13.31703643]]    Loss_Validation:  [[ 11.36591556]]\n",
      "Loop  687 :    Loss_Train:  [[ 13.31555397]]    Loss_Validation:  [[ 11.36448297]]\n",
      "Loop  688 :    Loss_Train:  [[ 13.31407669]]    Loss_Validation:  [[ 11.36305836]]\n",
      "Loop  689 :    Loss_Train:  [[ 13.31260457]]    Loss_Validation:  [[ 11.36164169]]\n",
      "Loop  690 :    Loss_Train:  [[ 13.31113757]]    Loss_Validation:  [[ 11.36023291]]\n",
      "Loop  691 :    Loss_Train:  [[ 13.30967568]]    Loss_Validation:  [[ 11.35883196]]\n",
      "Loop  692 :    Loss_Train:  [[ 13.30821886]]    Loss_Validation:  [[ 11.35743881]]\n",
      "Loop  693 :    Loss_Train:  [[ 13.30676709]]    Loss_Validation:  [[ 11.3560534]]\n",
      "Loop  694 :    Loss_Train:  [[ 13.30532034]]    Loss_Validation:  [[ 11.35467569]]\n",
      "Loop  695 :    Loss_Train:  [[ 13.30387858]]    Loss_Validation:  [[ 11.35330563]]\n",
      "Loop  696 :    Loss_Train:  [[ 13.3024418]]    Loss_Validation:  [[ 11.35194317]]\n",
      "Loop  697 :    Loss_Train:  [[ 13.30100996]]    Loss_Validation:  [[ 11.35058827]]\n",
      "Loop  698 :    Loss_Train:  [[ 13.29958303]]    Loss_Validation:  [[ 11.34924089]]\n",
      "Loop  699 :    Loss_Train:  [[ 13.298161]]    Loss_Validation:  [[ 11.34790097]]\n",
      "Loop  700 :    Loss_Train:  [[ 13.29674384]]    Loss_Validation:  [[ 11.34656848]]\n",
      "Loop  701 :    Loss_Train:  [[ 13.29533152]]    Loss_Validation:  [[ 11.34524336]]\n",
      "Loop  702 :    Loss_Train:  [[ 13.29392402]]    Loss_Validation:  [[ 11.34392558]]\n",
      "Loop  703 :    Loss_Train:  [[ 13.29252132]]    Loss_Validation:  [[ 11.34261508]]\n",
      "Loop  704 :    Loss_Train:  [[ 13.29112338]]    Loss_Validation:  [[ 11.34131183]]\n",
      "Loop  705 :    Loss_Train:  [[ 13.28973019]]    Loss_Validation:  [[ 11.34001579]]\n",
      "Loop  706 :    Loss_Train:  [[ 13.28834172]]    Loss_Validation:  [[ 11.3387269]]\n",
      "Loop  707 :    Loss_Train:  [[ 13.28695794]]    Loss_Validation:  [[ 11.33744513]]\n",
      "Loop  708 :    Loss_Train:  [[ 13.28557884]]    Loss_Validation:  [[ 11.33617043]]\n",
      "Loop  709 :    Loss_Train:  [[ 13.28420439]]    Loss_Validation:  [[ 11.33490277]]\n",
      "Loop  710 :    Loss_Train:  [[ 13.28283457]]    Loss_Validation:  [[ 11.33364209]]\n",
      "Loop  711 :    Loss_Train:  [[ 13.28146934]]    Loss_Validation:  [[ 11.33238836]]\n",
      "Loop  712 :    Loss_Train:  [[ 13.2801087]]    Loss_Validation:  [[ 11.33114154]]\n",
      "Loop  713 :    Loss_Train:  [[ 13.27875262]]    Loss_Validation:  [[ 11.32990158]]\n",
      "Loop  714 :    Loss_Train:  [[ 13.27740106]]    Loss_Validation:  [[ 11.32866844]]\n",
      "Loop  715 :    Loss_Train:  [[ 13.27605402]]    Loss_Validation:  [[ 11.32744209]]\n",
      "Loop  716 :    Loss_Train:  [[ 13.27471147]]    Loss_Validation:  [[ 11.32622248]]\n",
      "Loop  717 :    Loss_Train:  [[ 13.27337339]]    Loss_Validation:  [[ 11.32500958]]\n",
      "Loop  718 :    Loss_Train:  [[ 13.27203975]]    Loss_Validation:  [[ 11.32380334]]\n",
      "Loop  719 :    Loss_Train:  [[ 13.27071054]]    Loss_Validation:  [[ 11.32260372]]\n",
      "Loop  720 :    Loss_Train:  [[ 13.26938572]]    Loss_Validation:  [[ 11.32141068]]\n",
      "Loop  721 :    Loss_Train:  [[ 13.26806529]]    Loss_Validation:  [[ 11.3202242]]\n",
      "Loop  722 :    Loss_Train:  [[ 13.26674921]]    Loss_Validation:  [[ 11.31904421]]\n",
      "Loop  723 :    Loss_Train:  [[ 13.26543747]]    Loss_Validation:  [[ 11.3178707]]\n",
      "Loop  724 :    Loss_Train:  [[ 13.26413004]]    Loss_Validation:  [[ 11.31670362]]\n",
      "Loop  725 :    Loss_Train:  [[ 13.26282691]]    Loss_Validation:  [[ 11.31554292]]\n",
      "Loop  726 :    Loss_Train:  [[ 13.26152805]]    Loss_Validation:  [[ 11.31438859]]\n",
      "Loop  727 :    Loss_Train:  [[ 13.26023345]]    Loss_Validation:  [[ 11.31324057]]\n",
      "Loop  728 :    Loss_Train:  [[ 13.25894307]]    Loss_Validation:  [[ 11.31209882]]\n",
      "Loop  729 :    Loss_Train:  [[ 13.25765691]]    Loss_Validation:  [[ 11.31096333]]\n",
      "Loop  730 :    Loss_Train:  [[ 13.25637494]]    Loss_Validation:  [[ 11.30983403]]\n",
      "Loop  731 :    Loss_Train:  [[ 13.25509714]]    Loss_Validation:  [[ 11.30871091]]\n",
      "Loop  732 :    Loss_Train:  [[ 13.25382349]]    Loss_Validation:  [[ 11.30759392]]\n",
      "Loop  733 :    Loss_Train:  [[ 13.25255397]]    Loss_Validation:  [[ 11.30648303]]\n",
      "Loop  734 :    Loss_Train:  [[ 13.25128856]]    Loss_Validation:  [[ 11.3053782]]\n",
      "Loop  735 :    Loss_Train:  [[ 13.25002724]]    Loss_Validation:  [[ 11.30427939]]\n",
      "Loop  736 :    Loss_Train:  [[ 13.24877]]    Loss_Validation:  [[ 11.30318658]]\n",
      "Loop  737 :    Loss_Train:  [[ 13.2475168]]    Loss_Validation:  [[ 11.30209972]]\n",
      "Loop  738 :    Loss_Train:  [[ 13.24626764]]    Loss_Validation:  [[ 11.30101879]]\n",
      "Loop  739 :    Loss_Train:  [[ 13.2450225]]    Loss_Validation:  [[ 11.29994374]]\n",
      "Loop  740 :    Loss_Train:  [[ 13.24378135]]    Loss_Validation:  [[ 11.29887454]]\n",
      "Loop  741 :    Loss_Train:  [[ 13.24254417]]    Loss_Validation:  [[ 11.29781116]]\n",
      "Loop  742 :    Loss_Train:  [[ 13.24131095]]    Loss_Validation:  [[ 11.29675356]]\n",
      "Loop  743 :    Loss_Train:  [[ 13.24008167]]    Loss_Validation:  [[ 11.29570172]]\n",
      "Loop  744 :    Loss_Train:  [[ 13.23885631]]    Loss_Validation:  [[ 11.29465559]]\n",
      "Loop  745 :    Loss_Train:  [[ 13.23763485]]    Loss_Validation:  [[ 11.29361514]]\n",
      "Loop  746 :    Loss_Train:  [[ 13.23641728]]    Loss_Validation:  [[ 11.29258035]]\n",
      "Loop  747 :    Loss_Train:  [[ 13.23520357]]    Loss_Validation:  [[ 11.29155117]]\n",
      "Loop  748 :    Loss_Train:  [[ 13.2339937]]    Loss_Validation:  [[ 11.29052758]]\n",
      "Loop  749 :    Loss_Train:  [[ 13.23278767]]    Loss_Validation:  [[ 11.28950955]]\n",
      "Loop  750 :    Loss_Train:  [[ 13.23158545]]    Loss_Validation:  [[ 11.28849703]]\n",
      "Loop  751 :    Loss_Train:  [[ 13.23038702]]    Loss_Validation:  [[ 11.28749001]]\n",
      "Loop  752 :    Loss_Train:  [[ 13.22919237]]    Loss_Validation:  [[ 11.28648844]]\n",
      "Loop  753 :    Loss_Train:  [[ 13.22800147]]    Loss_Validation:  [[ 11.28549229]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  754 :    Loss_Train:  [[ 13.22681432]]    Loss_Validation:  [[ 11.28450154]]\n",
      "Loop  755 :    Loss_Train:  [[ 13.22563089]]    Loss_Validation:  [[ 11.28351616]]\n",
      "Loop  756 :    Loss_Train:  [[ 13.22445117]]    Loss_Validation:  [[ 11.28253611]]\n",
      "Loop  757 :    Loss_Train:  [[ 13.22327514]]    Loss_Validation:  [[ 11.28156135]]\n",
      "Loop  758 :    Loss_Train:  [[ 13.22210278]]    Loss_Validation:  [[ 11.28059187]]\n",
      "Loop  759 :    Loss_Train:  [[ 13.22093408]]    Loss_Validation:  [[ 11.27962764]]\n",
      "Loop  760 :    Loss_Train:  [[ 13.21976902]]    Loss_Validation:  [[ 11.27866861]]\n",
      "Loop  761 :    Loss_Train:  [[ 13.21860758]]    Loss_Validation:  [[ 11.27771476]]\n",
      "Loop  762 :    Loss_Train:  [[ 13.21744975]]    Loss_Validation:  [[ 11.27676607]]\n",
      "Loop  763 :    Loss_Train:  [[ 13.21629551]]    Loss_Validation:  [[ 11.27582249]]\n",
      "Loop  764 :    Loss_Train:  [[ 13.21514484]]    Loss_Validation:  [[ 11.27488401]]\n",
      "Loop  765 :    Loss_Train:  [[ 13.21399773]]    Loss_Validation:  [[ 11.2739506]]\n",
      "Loop  766 :    Loss_Train:  [[ 13.21285417]]    Loss_Validation:  [[ 11.27302221]]\n",
      "Loop  767 :    Loss_Train:  [[ 13.21171413]]    Loss_Validation:  [[ 11.27209884]]\n",
      "Loop  768 :    Loss_Train:  [[ 13.2105776]]    Loss_Validation:  [[ 11.27118044]]\n",
      "Loop  769 :    Loss_Train:  [[ 13.20944456]]    Loss_Validation:  [[ 11.27026699]]\n",
      "Loop  770 :    Loss_Train:  [[ 13.208315]]    Loss_Validation:  [[ 11.26935845]]\n",
      "Loop  771 :    Loss_Train:  [[ 13.20718891]]    Loss_Validation:  [[ 11.26845482]]\n",
      "Loop  772 :    Loss_Train:  [[ 13.20606626]]    Loss_Validation:  [[ 11.26755604]]\n",
      "Loop  773 :    Loss_Train:  [[ 13.20494705]]    Loss_Validation:  [[ 11.26666211]]\n",
      "Loop  774 :    Loss_Train:  [[ 13.20383126]]    Loss_Validation:  [[ 11.26577298]]\n",
      "Loop  775 :    Loss_Train:  [[ 13.20271886]]    Loss_Validation:  [[ 11.26488863]]\n",
      "Loop  776 :    Loss_Train:  [[ 13.20160986]]    Loss_Validation:  [[ 11.26400904]]\n",
      "Loop  777 :    Loss_Train:  [[ 13.20050422]]    Loss_Validation:  [[ 11.26313418]]\n",
      "Loop  778 :    Loss_Train:  [[ 13.19940195]]    Loss_Validation:  [[ 11.26226402]]\n",
      "Loop  779 :    Loss_Train:  [[ 13.19830301]]    Loss_Validation:  [[ 11.26139854]]\n",
      "Loop  780 :    Loss_Train:  [[ 13.19720741]]    Loss_Validation:  [[ 11.2605377]]\n",
      "Loop  781 :    Loss_Train:  [[ 13.19611512]]    Loss_Validation:  [[ 11.25968148]]\n",
      "Loop  782 :    Loss_Train:  [[ 13.19502612]]    Loss_Validation:  [[ 11.25882986]]\n",
      "Loop  783 :    Loss_Train:  [[ 13.19394041]]    Loss_Validation:  [[ 11.25798282]]\n",
      "Loop  784 :    Loss_Train:  [[ 13.19285797]]    Loss_Validation:  [[ 11.25714031]]\n",
      "Loop  785 :    Loss_Train:  [[ 13.19177879]]    Loss_Validation:  [[ 11.25630232]]\n",
      "Loop  786 :    Loss_Train:  [[ 13.19070285]]    Loss_Validation:  [[ 11.25546883]]\n",
      "Loop  787 :    Loss_Train:  [[ 13.18963014]]    Loss_Validation:  [[ 11.2546398]]\n",
      "Loop  788 :    Loss_Train:  [[ 13.18856064]]    Loss_Validation:  [[ 11.25381522]]\n",
      "Loop  789 :    Loss_Train:  [[ 13.18749434]]    Loss_Validation:  [[ 11.25299505]]\n",
      "Loop  790 :    Loss_Train:  [[ 13.18643123]]    Loss_Validation:  [[ 11.25217928]]\n",
      "Loop  791 :    Loss_Train:  [[ 13.18537129]]    Loss_Validation:  [[ 11.25136788]]\n",
      "Loop  792 :    Loss_Train:  [[ 13.18431451]]    Loss_Validation:  [[ 11.25056082]]\n",
      "Loop  793 :    Loss_Train:  [[ 13.18326087]]    Loss_Validation:  [[ 11.24975807]]\n",
      "Loop  794 :    Loss_Train:  [[ 13.18221037]]    Loss_Validation:  [[ 11.24895963]]\n",
      "Loop  795 :    Loss_Train:  [[ 13.18116299]]    Loss_Validation:  [[ 11.24816545]]\n",
      "Loop  796 :    Loss_Train:  [[ 13.18011871]]    Loss_Validation:  [[ 11.24737552]]\n",
      "Loop  797 :    Loss_Train:  [[ 13.17907752]]    Loss_Validation:  [[ 11.24658982]]\n",
      "Loop  798 :    Loss_Train:  [[ 13.17803941]]    Loss_Validation:  [[ 11.24580832]]\n",
      "Loop  799 :    Loss_Train:  [[ 13.17700437]]    Loss_Validation:  [[ 11.24503099]]\n",
      "Loop  800 :    Loss_Train:  [[ 13.17597238]]    Loss_Validation:  [[ 11.24425781]]\n",
      "Loop  801 :    Loss_Train:  [[ 13.17494343]]    Loss_Validation:  [[ 11.24348877]]\n",
      "Loop  802 :    Loss_Train:  [[ 13.17391751]]    Loss_Validation:  [[ 11.24272383]]\n",
      "Loop  803 :    Loss_Train:  [[ 13.1728946]]    Loss_Validation:  [[ 11.24196298]]\n",
      "Loop  804 :    Loss_Train:  [[ 13.17187469]]    Loss_Validation:  [[ 11.24120619]]\n",
      "Loop  805 :    Loss_Train:  [[ 13.17085778]]    Loss_Validation:  [[ 11.24045343]]\n",
      "Loop  806 :    Loss_Train:  [[ 13.16984384]]    Loss_Validation:  [[ 11.2397047]]\n",
      "Loop  807 :    Loss_Train:  [[ 13.16883287]]    Loss_Validation:  [[ 11.23895995]]\n",
      "Loop  808 :    Loss_Train:  [[ 13.16782484]]    Loss_Validation:  [[ 11.23821918]]\n",
      "Loop  809 :    Loss_Train:  [[ 13.16681976]]    Loss_Validation:  [[ 11.23748236]]\n",
      "Loop  810 :    Loss_Train:  [[ 13.1658176]]    Loss_Validation:  [[ 11.23674947]]\n",
      "Loop  811 :    Loss_Train:  [[ 13.16481836]]    Loss_Validation:  [[ 11.23602049]]\n",
      "Loop  812 :    Loss_Train:  [[ 13.16382203]]    Loss_Validation:  [[ 11.23529539]]\n",
      "Loop  813 :    Loss_Train:  [[ 13.16282858]]    Loss_Validation:  [[ 11.23457415]]\n",
      "Loop  814 :    Loss_Train:  [[ 13.16183802]]    Loss_Validation:  [[ 11.23385675]]\n",
      "Loop  815 :    Loss_Train:  [[ 13.16085032]]    Loss_Validation:  [[ 11.23314318]]\n",
      "Loop  816 :    Loss_Train:  [[ 13.15986548]]    Loss_Validation:  [[ 11.23243341]]\n",
      "Loop  817 :    Loss_Train:  [[ 13.15888348]]    Loss_Validation:  [[ 11.23172741]]\n",
      "Loop  818 :    Loss_Train:  [[ 13.15790431]]    Loss_Validation:  [[ 11.23102517]]\n",
      "Loop  819 :    Loss_Train:  [[ 13.15692797]]    Loss_Validation:  [[ 11.23032667]]\n",
      "Loop  820 :    Loss_Train:  [[ 13.15595443]]    Loss_Validation:  [[ 11.22963189]]\n",
      "Loop  821 :    Loss_Train:  [[ 13.1549837]]    Loss_Validation:  [[ 11.2289408]]\n",
      "Loop  822 :    Loss_Train:  [[ 13.15401574]]    Loss_Validation:  [[ 11.22825339]]\n",
      "Loop  823 :    Loss_Train:  [[ 13.15305057]]    Loss_Validation:  [[ 11.22756964]]\n",
      "Loop  824 :    Loss_Train:  [[ 13.15208816]]    Loss_Validation:  [[ 11.22688952]]\n",
      "Loop  825 :    Loss_Train:  [[ 13.1511285]]    Loss_Validation:  [[ 11.22621301]]\n",
      "Loop  826 :    Loss_Train:  [[ 13.15017158]]    Loss_Validation:  [[ 11.22554011]]\n",
      "Loop  827 :    Loss_Train:  [[ 13.14921739]]    Loss_Validation:  [[ 11.22487078]]\n",
      "Loop  828 :    Loss_Train:  [[ 13.14826593]]    Loss_Validation:  [[ 11.224205]]\n",
      "Loop  829 :    Loss_Train:  [[ 13.14731717]]    Loss_Validation:  [[ 11.22354277]]\n",
      "Loop  830 :    Loss_Train:  [[ 13.14637111]]    Loss_Validation:  [[ 11.22288405]]\n",
      "Loop  831 :    Loss_Train:  [[ 13.14542774]]    Loss_Validation:  [[ 11.22222883]]\n",
      "Loop  832 :    Loss_Train:  [[ 13.14448704]]    Loss_Validation:  [[ 11.22157709]]\n",
      "Loop  833 :    Loss_Train:  [[ 13.14354901]]    Loss_Validation:  [[ 11.22092881]]\n",
      "Loop  834 :    Loss_Train:  [[ 13.14261363]]    Loss_Validation:  [[ 11.22028397]]\n",
      "Loop  835 :    Loss_Train:  [[ 13.1416809]]    Loss_Validation:  [[ 11.21964256]]\n",
      "Loop  836 :    Loss_Train:  [[ 13.1407508]]    Loss_Validation:  [[ 11.21900455]]\n",
      "Loop  837 :    Loss_Train:  [[ 13.13982333]]    Loss_Validation:  [[ 11.21836993]]\n",
      "Loop  838 :    Loss_Train:  [[ 13.13889847]]    Loss_Validation:  [[ 11.21773867]]\n",
      "Loop  839 :    Loss_Train:  [[ 13.13797621]]    Loss_Validation:  [[ 11.21711077]]\n",
      "Loop  840 :    Loss_Train:  [[ 13.13705655]]    Loss_Validation:  [[ 11.21648619]]\n",
      "Loop  841 :    Loss_Train:  [[ 13.13613946]]    Loss_Validation:  [[ 11.21586493]]\n",
      "Loop  842 :    Loss_Train:  [[ 13.13522495]]    Loss_Validation:  [[ 11.21524696]]\n",
      "Loop  843 :    Loss_Train:  [[ 13.134313]]    Loss_Validation:  [[ 11.21463227]]\n",
      "Loop  844 :    Loss_Train:  [[ 13.13340361]]    Loss_Validation:  [[ 11.21402085]]\n",
      "Loop  845 :    Loss_Train:  [[ 13.13249675]]    Loss_Validation:  [[ 11.21341266]]\n",
      "Loop  846 :    Loss_Train:  [[ 13.13159243]]    Loss_Validation:  [[ 11.2128077]]\n",
      "Loop  847 :    Loss_Train:  [[ 13.13069063]]    Loss_Validation:  [[ 11.21220594]]\n",
      "Loop  848 :    Loss_Train:  [[ 13.12979134]]    Loss_Validation:  [[ 11.21160737]]\n",
      "Loop  849 :    Loss_Train:  [[ 13.12889456]]    Loss_Validation:  [[ 11.21101198]]\n",
      "Loop  850 :    Loss_Train:  [[ 13.12800027]]    Loss_Validation:  [[ 11.21041974]]\n",
      "Loop  851 :    Loss_Train:  [[ 13.12710846]]    Loss_Validation:  [[ 11.20983064]]\n",
      "Loop  852 :    Loss_Train:  [[ 13.12621913]]    Loss_Validation:  [[ 11.20924466]]\n",
      "Loop  853 :    Loss_Train:  [[ 13.12533226]]    Loss_Validation:  [[ 11.20866179]]\n",
      "Loop  854 :    Loss_Train:  [[ 13.12444784]]    Loss_Validation:  [[ 11.20808201]]\n",
      "Loop  855 :    Loss_Train:  [[ 13.12356587]]    Loss_Validation:  [[ 11.20750529]]\n",
      "Loop  856 :    Loss_Train:  [[ 13.12268634]]    Loss_Validation:  [[ 11.20693163]]\n",
      "Loop  857 :    Loss_Train:  [[ 13.12180923]]    Loss_Validation:  [[ 11.20636101]]\n",
      "Loop  858 :    Loss_Train:  [[ 13.12093454]]    Loss_Validation:  [[ 11.20579342]]\n",
      "Loop  859 :    Loss_Train:  [[ 13.12006226]]    Loss_Validation:  [[ 11.20522882]]\n",
      "Loop  860 :    Loss_Train:  [[ 13.11919238]]    Loss_Validation:  [[ 11.20466722]]\n",
      "Loop  861 :    Loss_Train:  [[ 13.11832489]]    Loss_Validation:  [[ 11.20410859]]\n",
      "Loop  862 :    Loss_Train:  [[ 13.11745977]]    Loss_Validation:  [[ 11.20355292]]\n",
      "Loop  863 :    Loss_Train:  [[ 13.11659703]]    Loss_Validation:  [[ 11.2030002]]\n",
      "Loop  864 :    Loss_Train:  [[ 13.11573666]]    Loss_Validation:  [[ 11.20245039]]\n",
      "Loop  865 :    Loss_Train:  [[ 13.11487863]]    Loss_Validation:  [[ 11.2019035]]\n",
      "Loop  866 :    Loss_Train:  [[ 13.11402295]]    Loss_Validation:  [[ 11.20135951]]\n",
      "Loop  867 :    Loss_Train:  [[ 13.11316961]]    Loss_Validation:  [[ 11.20081839]]\n",
      "Loop  868 :    Loss_Train:  [[ 13.1123186]]    Loss_Validation:  [[ 11.20028014]]\n",
      "Loop  869 :    Loss_Train:  [[ 13.1114699]]    Loss_Validation:  [[ 11.19974474]]\n",
      "Loop  870 :    Loss_Train:  [[ 13.11062351]]    Loss_Validation:  [[ 11.19921217]]\n",
      "Loop  871 :    Loss_Train:  [[ 13.10977943]]    Loss_Validation:  [[ 11.19868242]]\n",
      "Loop  872 :    Loss_Train:  [[ 13.10893763]]    Loss_Validation:  [[ 11.19815548]]\n",
      "Loop  873 :    Loss_Train:  [[ 13.10809812]]    Loss_Validation:  [[ 11.19763132]]\n",
      "Loop  874 :    Loss_Train:  [[ 13.10726089]]    Loss_Validation:  [[ 11.19710994]]\n",
      "Loop  875 :    Loss_Train:  [[ 13.10642592]]    Loss_Validation:  [[ 11.19659132]]\n",
      "Loop  876 :    Loss_Train:  [[ 13.10559321]]    Loss_Validation:  [[ 11.19607544]]\n",
      "Loop  877 :    Loss_Train:  [[ 13.10476275]]    Loss_Validation:  [[ 11.1955623]]\n",
      "Loop  878 :    Loss_Train:  [[ 13.10393454]]    Loss_Validation:  [[ 11.19505187]]\n",
      "Loop  879 :    Loss_Train:  [[ 13.10310855]]    Loss_Validation:  [[ 11.19454414]]\n",
      "Loop  880 :    Loss_Train:  [[ 13.10228479]]    Loss_Validation:  [[ 11.1940391]]\n",
      "Loop  881 :    Loss_Train:  [[ 13.10146325]]    Loss_Validation:  [[ 11.19353673]]\n",
      "Loop  882 :    Loss_Train:  [[ 13.10064392]]    Loss_Validation:  [[ 11.19303702]]\n",
      "Loop  883 :    Loss_Train:  [[ 13.09982679]]    Loss_Validation:  [[ 11.19253995]]\n",
      "Loop  884 :    Loss_Train:  [[ 13.09901185]]    Loss_Validation:  [[ 11.19204552]]\n",
      "Loop  885 :    Loss_Train:  [[ 13.0981991]]    Loss_Validation:  [[ 11.1915537]]\n",
      "Loop  886 :    Loss_Train:  [[ 13.09738852]]    Loss_Validation:  [[ 11.19106449]]\n",
      "Loop  887 :    Loss_Train:  [[ 13.09658011]]    Loss_Validation:  [[ 11.19057786]]\n",
      "Loop  888 :    Loss_Train:  [[ 13.09577386]]    Loss_Validation:  [[ 11.19009381]]\n",
      "Loop  889 :    Loss_Train:  [[ 13.09496977]]    Loss_Validation:  [[ 11.18961233]]\n",
      "Loop  890 :    Loss_Train:  [[ 13.09416782]]    Loss_Validation:  [[ 11.18913339]]\n",
      "Loop  891 :    Loss_Train:  [[ 13.093368]]    Loss_Validation:  [[ 11.18865699]]\n",
      "Loop  892 :    Loss_Train:  [[ 13.09257032]]    Loss_Validation:  [[ 11.18818311]]\n",
      "Loop  893 :    Loss_Train:  [[ 13.09177476]]    Loss_Validation:  [[ 11.18771173]]\n",
      "Loop  894 :    Loss_Train:  [[ 13.09098131]]    Loss_Validation:  [[ 11.18724286]]\n",
      "Loop  895 :    Loss_Train:  [[ 13.09018997]]    Loss_Validation:  [[ 11.18677647]]\n",
      "Loop  896 :    Loss_Train:  [[ 13.08940073]]    Loss_Validation:  [[ 11.18631254]]\n",
      "Loop  897 :    Loss_Train:  [[ 13.08861357]]    Loss_Validation:  [[ 11.18585108]]\n",
      "Loop  898 :    Loss_Train:  [[ 13.08782851]]    Loss_Validation:  [[ 11.18539205]]\n",
      "Loop  899 :    Loss_Train:  [[ 13.08704552]]    Loss_Validation:  [[ 11.18493546]]\n",
      "Loop  900 :    Loss_Train:  [[ 13.08626459]]    Loss_Validation:  [[ 11.18448129]]\n",
      "Loop  901 :    Loss_Train:  [[ 13.08548573]]    Loss_Validation:  [[ 11.18402952]]\n",
      "Loop  902 :    Loss_Train:  [[ 13.08470893]]    Loss_Validation:  [[ 11.18358014]]\n",
      "Loop  903 :    Loss_Train:  [[ 13.08393417]]    Loss_Validation:  [[ 11.18313315]]\n",
      "Loop  904 :    Loss_Train:  [[ 13.08316145]]    Loss_Validation:  [[ 11.18268852]]\n",
      "Loop  905 :    Loss_Train:  [[ 13.08239076]]    Loss_Validation:  [[ 11.18224625]]\n",
      "Loop  906 :    Loss_Train:  [[ 13.0816221]]    Loss_Validation:  [[ 11.18180632]]\n",
      "Loop  907 :    Loss_Train:  [[ 13.08085545]]    Loss_Validation:  [[ 11.18136872]]\n",
      "Loop  908 :    Loss_Train:  [[ 13.08009082]]    Loss_Validation:  [[ 11.18093345]]\n",
      "Loop  909 :    Loss_Train:  [[ 13.07932819]]    Loss_Validation:  [[ 11.18050048]]\n",
      "Loop  910 :    Loss_Train:  [[ 13.07856755]]    Loss_Validation:  [[ 11.1800698]]\n",
      "Loop  911 :    Loss_Train:  [[ 13.07780891]]    Loss_Validation:  [[ 11.17964141]]\n",
      "Loop  912 :    Loss_Train:  [[ 13.07705225]]    Loss_Validation:  [[ 11.17921529]]\n",
      "Loop  913 :    Loss_Train:  [[ 13.07629756]]    Loss_Validation:  [[ 11.17879143]]\n",
      "Loop  914 :    Loss_Train:  [[ 13.07554484]]    Loss_Validation:  [[ 11.17836982]]\n",
      "Loop  915 :    Loss_Train:  [[ 13.07479408]]    Loss_Validation:  [[ 11.17795044]]\n",
      "Loop  916 :    Loss_Train:  [[ 13.07404528]]    Loss_Validation:  [[ 11.17753328]]\n",
      "Loop  917 :    Loss_Train:  [[ 13.07329843]]    Loss_Validation:  [[ 11.17711835]]\n",
      "Loop  918 :    Loss_Train:  [[ 13.07255351]]    Loss_Validation:  [[ 11.17670561]]\n",
      "Loop  919 :    Loss_Train:  [[ 13.07181053]]    Loss_Validation:  [[ 11.17629506]]\n",
      "Loop  920 :    Loss_Train:  [[ 13.07106948]]    Loss_Validation:  [[ 11.17588669]]\n",
      "Loop  921 :    Loss_Train:  [[ 13.07033035]]    Loss_Validation:  [[ 11.17548049]]\n",
      "Loop  922 :    Loss_Train:  [[ 13.06959314]]    Loss_Validation:  [[ 11.17507644]]\n",
      "Loop  923 :    Loss_Train:  [[ 13.06885783]]    Loss_Validation:  [[ 11.17467454]]\n",
      "Loop  924 :    Loss_Train:  [[ 13.06812442]]    Loss_Validation:  [[ 11.17427478]]\n",
      "Loop  925 :    Loss_Train:  [[ 13.06739291]]    Loss_Validation:  [[ 11.17387713]]\n",
      "Loop  926 :    Loss_Train:  [[ 13.06666328]]    Loss_Validation:  [[ 11.1734816]]\n",
      "Loop  927 :    Loss_Train:  [[ 13.06593554]]    Loss_Validation:  [[ 11.17308817]]\n",
      "Loop  928 :    Loss_Train:  [[ 13.06520967]]    Loss_Validation:  [[ 11.17269683]]\n",
      "Loop  929 :    Loss_Train:  [[ 13.06448567]]    Loss_Validation:  [[ 11.17230757]]\n",
      "Loop  930 :    Loss_Train:  [[ 13.06376353]]    Loss_Validation:  [[ 11.17192038]]\n",
      "Loop  931 :    Loss_Train:  [[ 13.06304325]]    Loss_Validation:  [[ 11.17153525]]\n",
      "Loop  932 :    Loss_Train:  [[ 13.06232481]]    Loss_Validation:  [[ 11.17115217]]\n",
      "Loop  933 :    Loss_Train:  [[ 13.06160822]]    Loss_Validation:  [[ 11.17077112]]\n",
      "Loop  934 :    Loss_Train:  [[ 13.06089347]]    Loss_Validation:  [[ 11.1703921]]\n",
      "Loop  935 :    Loss_Train:  [[ 13.06018055]]    Loss_Validation:  [[ 11.1700151]]\n",
      "Loop  936 :    Loss_Train:  [[ 13.05946945]]    Loss_Validation:  [[ 11.1696401]]\n",
      "Loop  937 :    Loss_Train:  [[ 13.05876016]]    Loss_Validation:  [[ 11.1692671]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  938 :    Loss_Train:  [[ 13.05805269]]    Loss_Validation:  [[ 11.16889608]]\n",
      "Loop  939 :    Loss_Train:  [[ 13.05734703]]    Loss_Validation:  [[ 11.16852704]]\n",
      "Loop  940 :    Loss_Train:  [[ 13.05664317]]    Loss_Validation:  [[ 11.16815997]]\n",
      "Loop  941 :    Loss_Train:  [[ 13.0559411]]    Loss_Validation:  [[ 11.16779485]]\n",
      "Loop  942 :    Loss_Train:  [[ 13.05524081]]    Loss_Validation:  [[ 11.16743167]]\n",
      "Loop  943 :    Loss_Train:  [[ 13.05454231]]    Loss_Validation:  [[ 11.16707043]]\n",
      "Loop  944 :    Loss_Train:  [[ 13.05384558]]    Loss_Validation:  [[ 11.16671112]]\n",
      "Loop  945 :    Loss_Train:  [[ 13.05315062]]    Loss_Validation:  [[ 11.16635372]]\n",
      "Loop  946 :    Loss_Train:  [[ 13.05245743]]    Loss_Validation:  [[ 11.16599823]]\n",
      "Loop  947 :    Loss_Train:  [[ 13.05176599]]    Loss_Validation:  [[ 11.16564463]]\n",
      "Loop  948 :    Loss_Train:  [[ 13.05107631]]    Loss_Validation:  [[ 11.16529292]]\n",
      "Loop  949 :    Loss_Train:  [[ 13.05038837]]    Loss_Validation:  [[ 11.16494309]]\n",
      "Loop  950 :    Loss_Train:  [[ 13.04970217]]    Loss_Validation:  [[ 11.16459512]]\n",
      "Loop  951 :    Loss_Train:  [[ 13.0490177]]    Loss_Validation:  [[ 11.16424901]]\n",
      "Loop  952 :    Loss_Train:  [[ 13.04833497]]    Loss_Validation:  [[ 11.16390475]]\n",
      "Loop  953 :    Loss_Train:  [[ 13.04765395]]    Loss_Validation:  [[ 11.16356233]]\n",
      "Loop  954 :    Loss_Train:  [[ 13.04697465]]    Loss_Validation:  [[ 11.16322174]]\n",
      "Loop  955 :    Loss_Train:  [[ 13.04629707]]    Loss_Validation:  [[ 11.16288297]]\n",
      "Loop  956 :    Loss_Train:  [[ 13.04562119]]    Loss_Validation:  [[ 11.16254601]]\n",
      "Loop  957 :    Loss_Train:  [[ 13.04494701]]    Loss_Validation:  [[ 11.16221085]]\n",
      "Loop  958 :    Loss_Train:  [[ 13.04427452]]    Loss_Validation:  [[ 11.16187749]]\n",
      "Loop  959 :    Loss_Train:  [[ 13.04360372]]    Loss_Validation:  [[ 11.16154591]]\n",
      "Loop  960 :    Loss_Train:  [[ 13.04293461]]    Loss_Validation:  [[ 11.1612161]]\n",
      "Loop  961 :    Loss_Train:  [[ 13.04226717]]    Loss_Validation:  [[ 11.16088806]]\n",
      "Loop  962 :    Loss_Train:  [[ 13.0416014]]    Loss_Validation:  [[ 11.16056177]]\n",
      "Loop  963 :    Loss_Train:  [[ 13.0409373]]    Loss_Validation:  [[ 11.16023724]]\n",
      "Loop  964 :    Loss_Train:  [[ 13.04027486]]    Loss_Validation:  [[ 11.15991444]]\n",
      "Loop  965 :    Loss_Train:  [[ 13.03961408]]    Loss_Validation:  [[ 11.15959337]]\n",
      "Loop  966 :    Loss_Train:  [[ 13.03895495]]    Loss_Validation:  [[ 11.15927403]]\n",
      "Loop  967 :    Loss_Train:  [[ 13.03829746]]    Loss_Validation:  [[ 11.15895639]]\n",
      "Loop  968 :    Loss_Train:  [[ 13.03764161]]    Loss_Validation:  [[ 11.15864046]]\n",
      "Loop  969 :    Loss_Train:  [[ 13.0369874]]    Loss_Validation:  [[ 11.15832623]]\n",
      "Loop  970 :    Loss_Train:  [[ 13.03633481]]    Loss_Validation:  [[ 11.15801368]]\n",
      "Loop  971 :    Loss_Train:  [[ 13.03568385]]    Loss_Validation:  [[ 11.15770282]]\n",
      "Loop  972 :    Loss_Train:  [[ 13.03503451]]    Loss_Validation:  [[ 11.15739362]]\n",
      "Loop  973 :    Loss_Train:  [[ 13.03438677]]    Loss_Validation:  [[ 11.15708608]]\n",
      "Loop  974 :    Loss_Train:  [[ 13.03374065]]    Loss_Validation:  [[ 11.1567802]]\n",
      "Loop  975 :    Loss_Train:  [[ 13.03309613]]    Loss_Validation:  [[ 11.15647596]]\n",
      "Loop  976 :    Loss_Train:  [[ 13.0324532]]    Loss_Validation:  [[ 11.15617336]]\n",
      "Loop  977 :    Loss_Train:  [[ 13.03181187]]    Loss_Validation:  [[ 11.15587239]]\n",
      "Loop  978 :    Loss_Train:  [[ 13.03117213]]    Loss_Validation:  [[ 11.15557303]]\n",
      "Loop  979 :    Loss_Train:  [[ 13.03053396]]    Loss_Validation:  [[ 11.15527529]]\n",
      "Loop  980 :    Loss_Train:  [[ 13.02989738]]    Loss_Validation:  [[ 11.15497916]]\n",
      "Loop  981 :    Loss_Train:  [[ 13.02926236]]    Loss_Validation:  [[ 11.15468462]]\n",
      "Loop  982 :    Loss_Train:  [[ 13.02862892]]    Loss_Validation:  [[ 11.15439167]]\n",
      "Loop  983 :    Loss_Train:  [[ 13.02799703]]    Loss_Validation:  [[ 11.15410029]]\n",
      "Loop  984 :    Loss_Train:  [[ 13.0273667]]    Loss_Validation:  [[ 11.15381049]]\n",
      "Loop  985 :    Loss_Train:  [[ 13.02673792]]    Loss_Validation:  [[ 11.15352226]]\n",
      "Loop  986 :    Loss_Train:  [[ 13.02611069]]    Loss_Validation:  [[ 11.15323558]]\n",
      "Loop  987 :    Loss_Train:  [[ 13.025485]]    Loss_Validation:  [[ 11.15295045]]\n",
      "Loop  988 :    Loss_Train:  [[ 13.02486085]]    Loss_Validation:  [[ 11.15266686]]\n",
      "Loop  989 :    Loss_Train:  [[ 13.02423823]]    Loss_Validation:  [[ 11.1523848]]\n",
      "Loop  990 :    Loss_Train:  [[ 13.02361713]]    Loss_Validation:  [[ 11.15210427]]\n",
      "Loop  991 :    Loss_Train:  [[ 13.02299756]]    Loss_Validation:  [[ 11.15182526]]\n",
      "Loop  992 :    Loss_Train:  [[ 13.02237951]]    Loss_Validation:  [[ 11.15154776]]\n",
      "Loop  993 :    Loss_Train:  [[ 13.02176297]]    Loss_Validation:  [[ 11.15127176]]\n",
      "Loop  994 :    Loss_Train:  [[ 13.02114793]]    Loss_Validation:  [[ 11.15099725]]\n",
      "Loop  995 :    Loss_Train:  [[ 13.0205344]]    Loss_Validation:  [[ 11.15072424]]\n",
      "Loop  996 :    Loss_Train:  [[ 13.01992237]]    Loss_Validation:  [[ 11.1504527]]\n",
      "Loop  997 :    Loss_Train:  [[ 13.01931183]]    Loss_Validation:  [[ 11.15018264]]\n",
      "Loop  998 :    Loss_Train:  [[ 13.01870278]]    Loss_Validation:  [[ 11.14991404]]\n",
      "Loop  999 :    Loss_Train:  [[ 13.01809522]]    Loss_Validation:  [[ 11.14964691]]\n",
      "Loop  1000 :    Loss_Train:  [[ 13.01748913]]    Loss_Validation:  [[ 11.14938122]]\n",
      "Loop  1001 :    Loss_Train:  [[ 13.01688452]]    Loss_Validation:  [[ 11.14911698]]\n",
      "Loop  1002 :    Loss_Train:  [[ 13.01628138]]    Loss_Validation:  [[ 11.14885417]]\n",
      "Loop  1003 :    Loss_Train:  [[ 13.0156797]]    Loss_Validation:  [[ 11.1485928]]\n",
      "Loop  1004 :    Loss_Train:  [[ 13.01507949]]    Loss_Validation:  [[ 11.14833284]]\n",
      "Loop  1005 :    Loss_Train:  [[ 13.01448073]]    Loss_Validation:  [[ 11.1480743]]\n",
      "Loop  1006 :    Loss_Train:  [[ 13.01388343]]    Loss_Validation:  [[ 11.14781718]]\n",
      "Loop  1007 :    Loss_Train:  [[ 13.01328757]]    Loss_Validation:  [[ 11.14756145]]\n",
      "Loop  1008 :    Loss_Train:  [[ 13.01269316]]    Loss_Validation:  [[ 11.14730712]]\n",
      "Loop  1009 :    Loss_Train:  [[ 13.01210018]]    Loss_Validation:  [[ 11.14705417]]\n",
      "Loop  1010 :    Loss_Train:  [[ 13.01150864]]    Loss_Validation:  [[ 11.14680261]]\n",
      "Loop  1011 :    Loss_Train:  [[ 13.01091852]]    Loss_Validation:  [[ 11.14655242]]\n",
      "Loop  1012 :    Loss_Train:  [[ 13.01032984]]    Loss_Validation:  [[ 11.1463036]]\n",
      "Loop  1013 :    Loss_Train:  [[ 13.00974257]]    Loss_Validation:  [[ 11.14605613]]\n",
      "Loop  1014 :    Loss_Train:  [[ 13.00915672]]    Loss_Validation:  [[ 11.14581003]]\n",
      "Loop  1015 :    Loss_Train:  [[ 13.00857228]]    Loss_Validation:  [[ 11.14556526]]\n",
      "Loop  1016 :    Loss_Train:  [[ 13.00798925]]    Loss_Validation:  [[ 11.14532184]]\n",
      "Loop  1017 :    Loss_Train:  [[ 13.00740762]]    Loss_Validation:  [[ 11.14507976]]\n",
      "Loop  1018 :    Loss_Train:  [[ 13.00682739]]    Loss_Validation:  [[ 11.144839]]\n",
      "Loop  1019 :    Loss_Train:  [[ 13.00624856]]    Loss_Validation:  [[ 11.14459956]]\n",
      "Loop  1020 :    Loss_Train:  [[ 13.00567111]]    Loss_Validation:  [[ 11.14436143]]\n",
      "Loop  1021 :    Loss_Train:  [[ 13.00509506]]    Loss_Validation:  [[ 11.14412462]]\n",
      "Loop  1022 :    Loss_Train:  [[ 13.00452038]]    Loss_Validation:  [[ 11.1438891]]\n",
      "Loop  1023 :    Loss_Train:  [[ 13.00394708]]    Loss_Validation:  [[ 11.14365488]]\n",
      "Loop  1024 :    Loss_Train:  [[ 13.00337516]]    Loss_Validation:  [[ 11.14342195]]\n",
      "Loop  1025 :    Loss_Train:  [[ 13.0028046]]    Loss_Validation:  [[ 11.1431903]]\n",
      "Loop  1026 :    Loss_Train:  [[ 13.00223541]]    Loss_Validation:  [[ 11.14295993]]\n",
      "Loop  1027 :    Loss_Train:  [[ 13.00166758]]    Loss_Validation:  [[ 11.14273082]]\n",
      "Loop  1028 :    Loss_Train:  [[ 13.00110111]]    Loss_Validation:  [[ 11.14250298]]\n",
      "Loop  1029 :    Loss_Train:  [[ 13.00053599]]    Loss_Validation:  [[ 11.1422764]]\n",
      "Loop  1030 :    Loss_Train:  [[ 12.99997221]]    Loss_Validation:  [[ 11.14205106]]\n",
      "Loop  1031 :    Loss_Train:  [[ 12.99940979]]    Loss_Validation:  [[ 11.14182698]]\n",
      "Loop  1032 :    Loss_Train:  [[ 12.9988487]]    Loss_Validation:  [[ 11.14160413]]\n",
      "Loop  1033 :    Loss_Train:  [[ 12.99828895]]    Loss_Validation:  [[ 11.14138251]]\n",
      "Loop  1034 :    Loss_Train:  [[ 12.99773053]]    Loss_Validation:  [[ 11.14116212]]\n",
      "Loop  1035 :    Loss_Train:  [[ 12.99717344]]    Loss_Validation:  [[ 11.14094295]]\n",
      "Loop  1036 :    Loss_Train:  [[ 12.99661767]]    Loss_Validation:  [[ 11.140725]]\n",
      "Loop  1037 :    Loss_Train:  [[ 12.99606323]]    Loss_Validation:  [[ 11.14050825]]\n",
      "Loop  1038 :    Loss_Train:  [[ 12.9955101]]    Loss_Validation:  [[ 11.14029271]]\n",
      "Loop  1039 :    Loss_Train:  [[ 12.99495828]]    Loss_Validation:  [[ 11.14007837]]\n",
      "Loop  1040 :    Loss_Train:  [[ 12.99440777]]    Loss_Validation:  [[ 11.13986521]]\n",
      "Loop  1041 :    Loss_Train:  [[ 12.99385857]]    Loss_Validation:  [[ 11.13965324]]\n",
      "Loop  1042 :    Loss_Train:  [[ 12.99331066]]    Loss_Validation:  [[ 11.13944245]]\n",
      "Loop  1043 :    Loss_Train:  [[ 12.99276405]]    Loss_Validation:  [[ 11.13923284]]\n",
      "Loop  1044 :    Loss_Train:  [[ 12.99221874]]    Loss_Validation:  [[ 11.13902439]]\n",
      "Loop  1045 :    Loss_Train:  [[ 12.99167471]]    Loss_Validation:  [[ 11.1388171]]\n",
      "Loop  1046 :    Loss_Train:  [[ 12.99113197]]    Loss_Validation:  [[ 11.13861097]]\n",
      "Loop  1047 :    Loss_Train:  [[ 12.99059051]]    Loss_Validation:  [[ 11.13840599]]\n",
      "Loop  1048 :    Loss_Train:  [[ 12.99005033]]    Loss_Validation:  [[ 11.13820216]]\n",
      "Loop  1049 :    Loss_Train:  [[ 12.98951142]]    Loss_Validation:  [[ 11.13799947]]\n",
      "Loop  1050 :    Loss_Train:  [[ 12.98897378]]    Loss_Validation:  [[ 11.13779791]]\n",
      "Loop  1051 :    Loss_Train:  [[ 12.98843741]]    Loss_Validation:  [[ 11.13759748]]\n",
      "Loop  1052 :    Loss_Train:  [[ 12.9879023]]    Loss_Validation:  [[ 11.13739817]]\n",
      "Loop  1053 :    Loss_Train:  [[ 12.98736844]]    Loss_Validation:  [[ 11.13719998]]\n",
      "Loop  1054 :    Loss_Train:  [[ 12.98683585]]    Loss_Validation:  [[ 11.13700291]]\n",
      "Loop  1055 :    Loss_Train:  [[ 12.9863045]]    Loss_Validation:  [[ 11.13680694]]\n",
      "Loop  1056 :    Loss_Train:  [[ 12.9857744]]    Loss_Validation:  [[ 11.13661207]]\n",
      "Loop  1057 :    Loss_Train:  [[ 12.98524554]]    Loss_Validation:  [[ 11.1364183]]\n",
      "Loop  1058 :    Loss_Train:  [[ 12.98471793]]    Loss_Validation:  [[ 11.13622562]]\n",
      "Loop  1059 :    Loss_Train:  [[ 12.98419155]]    Loss_Validation:  [[ 11.13603402]]\n",
      "Loop  1060 :    Loss_Train:  [[ 12.9836664]]    Loss_Validation:  [[ 11.13584351]]\n",
      "Loop  1061 :    Loss_Train:  [[ 12.98314248]]    Loss_Validation:  [[ 11.13565407]]\n",
      "Loop  1062 :    Loss_Train:  [[ 12.98261979]]    Loss_Validation:  [[ 11.1354657]]\n",
      "Loop  1063 :    Loss_Train:  [[ 12.98209832]]    Loss_Validation:  [[ 11.13527839]]\n",
      "Loop  1064 :    Loss_Train:  [[ 12.98157807]]    Loss_Validation:  [[ 11.13509215]]\n",
      "Loop  1065 :    Loss_Train:  [[ 12.98105903]]    Loss_Validation:  [[ 11.13490696]]\n",
      "Loop  1066 :    Loss_Train:  [[ 12.98054121]]    Loss_Validation:  [[ 11.13472281]]\n",
      "Loop  1067 :    Loss_Train:  [[ 12.98002459]]    Loss_Validation:  [[ 11.13453972]]\n",
      "Loop  1068 :    Loss_Train:  [[ 12.97950918]]    Loss_Validation:  [[ 11.13435766]]\n",
      "Loop  1069 :    Loss_Train:  [[ 12.97899496]]    Loss_Validation:  [[ 11.13417663]]\n",
      "Loop  1070 :    Loss_Train:  [[ 12.97848195]]    Loss_Validation:  [[ 11.13399664]]\n",
      "Loop  1071 :    Loss_Train:  [[ 12.97797012]]    Loss_Validation:  [[ 11.13381767]]\n",
      "Loop  1072 :    Loss_Train:  [[ 12.97745949]]    Loss_Validation:  [[ 11.13363971]]\n",
      "Loop  1073 :    Loss_Train:  [[ 12.97695004]]    Loss_Validation:  [[ 11.13346278]]\n",
      "Loop  1074 :    Loss_Train:  [[ 12.97644178]]    Loss_Validation:  [[ 11.13328685]]\n",
      "Loop  1075 :    Loss_Train:  [[ 12.9759347]]    Loss_Validation:  [[ 11.13311193]]\n",
      "Loop  1076 :    Loss_Train:  [[ 12.97542879]]    Loss_Validation:  [[ 11.132938]]\n",
      "Loop  1077 :    Loss_Train:  [[ 12.97492406]]    Loss_Validation:  [[ 11.13276507]]\n",
      "Loop  1078 :    Loss_Train:  [[ 12.97442049]]    Loss_Validation:  [[ 11.13259313]]\n",
      "Loop  1079 :    Loss_Train:  [[ 12.9739181]]    Loss_Validation:  [[ 11.13242218]]\n",
      "Loop  1080 :    Loss_Train:  [[ 12.97341686]]    Loss_Validation:  [[ 11.13225221]]\n",
      "Loop  1081 :    Loss_Train:  [[ 12.97291679]]    Loss_Validation:  [[ 11.13208321]]\n",
      "Loop  1082 :    Loss_Train:  [[ 12.97241787]]    Loss_Validation:  [[ 11.13191519]]\n",
      "Loop  1083 :    Loss_Train:  [[ 12.9719201]]    Loss_Validation:  [[ 11.13174813]]\n",
      "Loop  1084 :    Loss_Train:  [[ 12.97142348]]    Loss_Validation:  [[ 11.13158203]]\n",
      "Loop  1085 :    Loss_Train:  [[ 12.97092801]]    Loss_Validation:  [[ 11.13141689]]\n",
      "Loop  1086 :    Loss_Train:  [[ 12.97043368]]    Loss_Validation:  [[ 11.1312527]]\n",
      "Loop  1087 :    Loss_Train:  [[ 12.9699405]]    Loss_Validation:  [[ 11.13108946]]\n",
      "Loop  1088 :    Loss_Train:  [[ 12.96944844]]    Loss_Validation:  [[ 11.13092717]]\n",
      "Loop  1089 :    Loss_Train:  [[ 12.96895753]]    Loss_Validation:  [[ 11.13076581]]\n",
      "Loop  1090 :    Loss_Train:  [[ 12.96846774]]    Loss_Validation:  [[ 11.13060539]]\n",
      "Loop  1091 :    Loss_Train:  [[ 12.96797907]]    Loss_Validation:  [[ 11.13044589]]\n",
      "Loop  1092 :    Loss_Train:  [[ 12.96749154]]    Loss_Validation:  [[ 11.13028733]]\n",
      "Loop  1093 :    Loss_Train:  [[ 12.96700512]]    Loss_Validation:  [[ 11.13012968]]\n",
      "Loop  1094 :    Loss_Train:  [[ 12.96651982]]    Loss_Validation:  [[ 11.12997295]]\n",
      "Loop  1095 :    Loss_Train:  [[ 12.96603563]]    Loss_Validation:  [[ 11.12981713]]\n",
      "Loop  1096 :    Loss_Train:  [[ 12.96555255]]    Loss_Validation:  [[ 11.12966223]]\n",
      "Loop  1097 :    Loss_Train:  [[ 12.96507058]]    Loss_Validation:  [[ 11.12950822]]\n",
      "Loop  1098 :    Loss_Train:  [[ 12.96458972]]    Loss_Validation:  [[ 11.12935511]]\n",
      "Loop  1099 :    Loss_Train:  [[ 12.96410995]]    Loss_Validation:  [[ 11.1292029]]\n",
      "Loop  1100 :    Loss_Train:  [[ 12.96363129]]    Loss_Validation:  [[ 11.12905158]]\n",
      "Loop  1101 :    Loss_Train:  [[ 12.96315372]]    Loss_Validation:  [[ 11.12890115]]\n",
      "Loop  1102 :    Loss_Train:  [[ 12.96267724]]    Loss_Validation:  [[ 11.1287516]]\n",
      "Loop  1103 :    Loss_Train:  [[ 12.96220185]]    Loss_Validation:  [[ 11.12860292]]\n",
      "Loop  1104 :    Loss_Train:  [[ 12.96172754]]    Loss_Validation:  [[ 11.12845512]]\n",
      "Loop  1105 :    Loss_Train:  [[ 12.96125432]]    Loss_Validation:  [[ 11.12830819]]\n",
      "Loop  1106 :    Loss_Train:  [[ 12.96078218]]    Loss_Validation:  [[ 11.12816212]]\n",
      "Loop  1107 :    Loss_Train:  [[ 12.96031112]]    Loss_Validation:  [[ 11.12801692]]\n",
      "Loop  1108 :    Loss_Train:  [[ 12.95984113]]    Loss_Validation:  [[ 11.12787257]]\n",
      "Loop  1109 :    Loss_Train:  [[ 12.95937221]]    Loss_Validation:  [[ 11.12772908]]\n",
      "Loop  1110 :    Loss_Train:  [[ 12.95890435]]    Loss_Validation:  [[ 11.12758643]]\n",
      "Loop  1111 :    Loss_Train:  [[ 12.95843756]]    Loss_Validation:  [[ 11.12744463]]\n",
      "Loop  1112 :    Loss_Train:  [[ 12.95797184]]    Loss_Validation:  [[ 11.12730367]]\n",
      "Loop  1113 :    Loss_Train:  [[ 12.95750717]]    Loss_Validation:  [[ 11.12716355]]\n",
      "Loop  1114 :    Loss_Train:  [[ 12.95704356]]    Loss_Validation:  [[ 11.12702426]]\n",
      "Loop  1115 :    Loss_Train:  [[ 12.956581]]    Loss_Validation:  [[ 11.1268858]]\n",
      "Loop  1116 :    Loss_Train:  [[ 12.95611949]]    Loss_Validation:  [[ 11.12674817]]\n",
      "Loop  1117 :    Loss_Train:  [[ 12.95565902]]    Loss_Validation:  [[ 11.12661135]]\n",
      "Loop  1118 :    Loss_Train:  [[ 12.9551996]]    Loss_Validation:  [[ 11.12647536]]\n",
      "Loop  1119 :    Loss_Train:  [[ 12.95474122]]    Loss_Validation:  [[ 11.12634017]]\n",
      "Loop  1120 :    Loss_Train:  [[ 12.95428388]]    Loss_Validation:  [[ 11.1262058]]\n",
      "Loop  1121 :    Loss_Train:  [[ 12.95382758]]    Loss_Validation:  [[ 11.12607223]]\n",
      "Loop  1122 :    Loss_Train:  [[ 12.95337231]]    Loss_Validation:  [[ 11.12593946]]\n",
      "Loop  1123 :    Loss_Train:  [[ 12.95291806]]    Loss_Validation:  [[ 11.1258075]]\n",
      "Loop  1124 :    Loss_Train:  [[ 12.95246485]]    Loss_Validation:  [[ 11.12567632]]\n",
      "Loop  1125 :    Loss_Train:  [[ 12.95201265]]    Loss_Validation:  [[ 11.12554594]]\n",
      "Loop  1126 :    Loss_Train:  [[ 12.95156148]]    Loss_Validation:  [[ 11.12541634]]\n",
      "Loop  1127 :    Loss_Train:  [[ 12.95111133]]    Loss_Validation:  [[ 11.12528753]]\n",
      "Loop  1128 :    Loss_Train:  [[ 12.95066219]]    Loss_Validation:  [[ 11.12515949]]\n",
      "Loop  1129 :    Loss_Train:  [[ 12.95021407]]    Loss_Validation:  [[ 11.12503223]]\n",
      "Loop  1130 :    Loss_Train:  [[ 12.94976695]]    Loss_Validation:  [[ 11.12490574]]\n",
      "Loop  1131 :    Loss_Train:  [[ 12.94932084]]    Loss_Validation:  [[ 11.12478002]]\n",
      "Loop  1132 :    Loss_Train:  [[ 12.94887574]]    Loss_Validation:  [[ 11.12465507]]\n",
      "Loop  1133 :    Loss_Train:  [[ 12.94843163]]    Loss_Validation:  [[ 11.12453087]]\n",
      "Loop  1134 :    Loss_Train:  [[ 12.94798853]]    Loss_Validation:  [[ 11.12440744]]\n",
      "Loop  1135 :    Loss_Train:  [[ 12.94754642]]    Loss_Validation:  [[ 11.12428475]]\n",
      "Loop  1136 :    Loss_Train:  [[ 12.9471053]]    Loss_Validation:  [[ 11.12416282]]\n",
      "Loop  1137 :    Loss_Train:  [[ 12.94666518]]    Loss_Validation:  [[ 11.12404163]]\n",
      "Loop  1138 :    Loss_Train:  [[ 12.94622604]]    Loss_Validation:  [[ 11.12392119]]\n",
      "Loop  1139 :    Loss_Train:  [[ 12.94578789]]    Loss_Validation:  [[ 11.12380149]]\n",
      "Loop  1140 :    Loss_Train:  [[ 12.94535072]]    Loss_Validation:  [[ 11.12368252]]\n",
      "Loop  1141 :    Loss_Train:  [[ 12.94491453]]    Loss_Validation:  [[ 11.12356428]]\n",
      "Loop  1142 :    Loss_Train:  [[ 12.94447931]]    Loss_Validation:  [[ 11.12344678]]\n",
      "Loop  1143 :    Loss_Train:  [[ 12.94404507]]    Loss_Validation:  [[ 11.12333]]\n",
      "Loop  1144 :    Loss_Train:  [[ 12.94361181]]    Loss_Validation:  [[ 11.12321394]]\n",
      "Loop  1145 :    Loss_Train:  [[ 12.94317951]]    Loss_Validation:  [[ 11.1230986]]\n",
      "Loop  1146 :    Loss_Train:  [[ 12.94274818]]    Loss_Validation:  [[ 11.12298398]]\n",
      "Loop  1147 :    Loss_Train:  [[ 12.94231781]]    Loss_Validation:  [[ 11.12287007]]\n",
      "Loop  1148 :    Loss_Train:  [[ 12.9418884]]    Loss_Validation:  [[ 11.12275686]]\n",
      "Loop  1149 :    Loss_Train:  [[ 12.94145995]]    Loss_Validation:  [[ 11.12264436]]\n",
      "Loop  1150 :    Loss_Train:  [[ 12.94103246]]    Loss_Validation:  [[ 11.12253257]]\n",
      "Loop  1151 :    Loss_Train:  [[ 12.94060592]]    Loss_Validation:  [[ 11.12242147]]\n",
      "Loop  1152 :    Loss_Train:  [[ 12.94018033]]    Loss_Validation:  [[ 11.12231107]]\n",
      "Loop  1153 :    Loss_Train:  [[ 12.93975569]]    Loss_Validation:  [[ 11.12220136]]\n",
      "Loop  1154 :    Loss_Train:  [[ 12.93933199]]    Loss_Validation:  [[ 11.12209234]]\n",
      "Loop  1155 :    Loss_Train:  [[ 12.93890924]]    Loss_Validation:  [[ 11.12198401]]\n",
      "Loop  1156 :    Loss_Train:  [[ 12.93848743]]    Loss_Validation:  [[ 11.12187636]]\n",
      "Loop  1157 :    Loss_Train:  [[ 12.93806656]]    Loss_Validation:  [[ 11.12176938]]\n",
      "Loop  1158 :    Loss_Train:  [[ 12.93764662]]    Loss_Validation:  [[ 11.12166309]]\n",
      "Loop  1159 :    Loss_Train:  [[ 12.93722761]]    Loss_Validation:  [[ 11.12155746]]\n",
      "Loop  1160 :    Loss_Train:  [[ 12.93680954]]    Loss_Validation:  [[ 11.12145251]]\n",
      "Loop  1161 :    Loss_Train:  [[ 12.93639239]]    Loss_Validation:  [[ 11.12134822]]\n",
      "Loop  1162 :    Loss_Train:  [[ 12.93597617]]    Loss_Validation:  [[ 11.1212446]]\n",
      "Loop  1163 :    Loss_Train:  [[ 12.93556087]]    Loss_Validation:  [[ 11.12114163]]\n",
      "Loop  1164 :    Loss_Train:  [[ 12.93514649]]    Loss_Validation:  [[ 11.12103933]]\n",
      "Loop  1165 :    Loss_Train:  [[ 12.93473303]]    Loss_Validation:  [[ 11.12093767]]\n",
      "Loop  1166 :    Loss_Train:  [[ 12.93432049]]    Loss_Validation:  [[ 11.12083667]]\n",
      "Loop  1167 :    Loss_Train:  [[ 12.93390886]]    Loss_Validation:  [[ 11.12073632]]\n",
      "Loop  1168 :    Loss_Train:  [[ 12.93349813]]    Loss_Validation:  [[ 11.12063661]]\n",
      "Loop  1169 :    Loss_Train:  [[ 12.93308832]]    Loss_Validation:  [[ 11.12053754]]\n",
      "Loop  1170 :    Loss_Train:  [[ 12.93267941]]    Loss_Validation:  [[ 11.12043912]]\n",
      "Loop  1171 :    Loss_Train:  [[ 12.93227141]]    Loss_Validation:  [[ 11.12034132]]\n",
      "Loop  1172 :    Loss_Train:  [[ 12.93186431]]    Loss_Validation:  [[ 11.12024417]]\n",
      "Loop  1173 :    Loss_Train:  [[ 12.9314581]]    Loss_Validation:  [[ 11.12014764]]\n",
      "Loop  1174 :    Loss_Train:  [[ 12.93105279]]    Loss_Validation:  [[ 11.12005173]]\n",
      "Loop  1175 :    Loss_Train:  [[ 12.93064838]]    Loss_Validation:  [[ 11.11995646]]\n",
      "Loop  1176 :    Loss_Train:  [[ 12.93024486]]    Loss_Validation:  [[ 11.1198618]]\n",
      "Loop  1177 :    Loss_Train:  [[ 12.92984222]]    Loss_Validation:  [[ 11.11976776]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1178 :    Loss_Train:  [[ 12.92944047]]    Loss_Validation:  [[ 11.11967434]]\n",
      "Loop  1179 :    Loss_Train:  [[ 12.92903961]]    Loss_Validation:  [[ 11.11958153]]\n",
      "Loop  1180 :    Loss_Train:  [[ 12.92863963]]    Loss_Validation:  [[ 11.11948933]]\n",
      "Loop  1181 :    Loss_Train:  [[ 12.92824053]]    Loss_Validation:  [[ 11.11939773]]\n",
      "Loop  1182 :    Loss_Train:  [[ 12.9278423]]    Loss_Validation:  [[ 11.11930674]]\n",
      "Loop  1183 :    Loss_Train:  [[ 12.92744495]]    Loss_Validation:  [[ 11.11921635]]\n",
      "Loop  1184 :    Loss_Train:  [[ 12.92704848]]    Loss_Validation:  [[ 11.11912656]]\n",
      "Loop  1185 :    Loss_Train:  [[ 12.92665287]]    Loss_Validation:  [[ 11.11903737]]\n",
      "Loop  1186 :    Loss_Train:  [[ 12.92625813]]    Loss_Validation:  [[ 11.11894876]]\n",
      "Loop  1187 :    Loss_Train:  [[ 12.92586426]]    Loss_Validation:  [[ 11.11886075]]\n",
      "Loop  1188 :    Loss_Train:  [[ 12.92547125]]    Loss_Validation:  [[ 11.11877332]]\n",
      "Loop  1189 :    Loss_Train:  [[ 12.9250791]]    Loss_Validation:  [[ 11.11868648]]\n",
      "Loop  1190 :    Loss_Train:  [[ 12.92468781]]    Loss_Validation:  [[ 11.11860022]]\n",
      "Loop  1191 :    Loss_Train:  [[ 12.92429738]]    Loss_Validation:  [[ 11.11851454]]\n",
      "Loop  1192 :    Loss_Train:  [[ 12.9239078]]    Loss_Validation:  [[ 11.11842944]]\n",
      "Loop  1193 :    Loss_Train:  [[ 12.92351908]]    Loss_Validation:  [[ 11.11834491]]\n",
      "Loop  1194 :    Loss_Train:  [[ 12.9231312]]    Loss_Validation:  [[ 11.11826094]]\n",
      "Loop  1195 :    Loss_Train:  [[ 12.92274417]]    Loss_Validation:  [[ 11.11817755]]\n",
      "Loop  1196 :    Loss_Train:  [[ 12.92235799]]    Loss_Validation:  [[ 11.11809473]]\n",
      "Loop  1197 :    Loss_Train:  [[ 12.92197265]]    Loss_Validation:  [[ 11.11801246]]\n",
      "Loop  1198 :    Loss_Train:  [[ 12.92158815]]    Loss_Validation:  [[ 11.11793076]]\n",
      "Loop  1199 :    Loss_Train:  [[ 12.92120449]]    Loss_Validation:  [[ 11.11784961]]\n",
      "Loop  1200 :    Loss_Train:  [[ 12.92082167]]    Loss_Validation:  [[ 11.11776902]]\n",
      "Loop  1201 :    Loss_Train:  [[ 12.92043968]]    Loss_Validation:  [[ 11.11768899]]\n",
      "Loop  1202 :    Loss_Train:  [[ 12.92005853]]    Loss_Validation:  [[ 11.1176095]]\n",
      "Loop  1203 :    Loss_Train:  [[ 12.9196782]]    Loss_Validation:  [[ 11.11753056]]\n",
      "Loop  1204 :    Loss_Train:  [[ 12.9192987]]    Loss_Validation:  [[ 11.11745216]]\n",
      "Loop  1205 :    Loss_Train:  [[ 12.91892003]]    Loss_Validation:  [[ 11.11737431]]\n",
      "Loop  1206 :    Loss_Train:  [[ 12.91854218]]    Loss_Validation:  [[ 11.117297]]\n",
      "Loop  1207 :    Loss_Train:  [[ 12.91816516]]    Loss_Validation:  [[ 11.11722022]]\n",
      "Loop  1208 :    Loss_Train:  [[ 12.91778895]]    Loss_Validation:  [[ 11.11714398]]\n",
      "Loop  1209 :    Loss_Train:  [[ 12.91741356]]    Loss_Validation:  [[ 11.11706827]]\n",
      "Loop  1210 :    Loss_Train:  [[ 12.91703898]]    Loss_Validation:  [[ 11.11699309]]\n",
      "Loop  1211 :    Loss_Train:  [[ 12.91666522]]    Loss_Validation:  [[ 11.11691844]]\n",
      "Loop  1212 :    Loss_Train:  [[ 12.91629227]]    Loss_Validation:  [[ 11.11684431]]\n",
      "Loop  1213 :    Loss_Train:  [[ 12.91592013]]    Loss_Validation:  [[ 11.1167707]]\n",
      "Loop  1214 :    Loss_Train:  [[ 12.91554879]]    Loss_Validation:  [[ 11.11669762]]\n",
      "Loop  1215 :    Loss_Train:  [[ 12.91517826]]    Loss_Validation:  [[ 11.11662505]]\n",
      "Loop  1216 :    Loss_Train:  [[ 12.91480854]]    Loss_Validation:  [[ 11.116553]]\n",
      "Loop  1217 :    Loss_Train:  [[ 12.91443961]]    Loss_Validation:  [[ 11.11648146]]\n",
      "Loop  1218 :    Loss_Train:  [[ 12.91407148]]    Loss_Validation:  [[ 11.11641043]]\n",
      "Loop  1219 :    Loss_Train:  [[ 12.91370415]]    Loss_Validation:  [[ 11.11633991]]\n",
      "Loop  1220 :    Loss_Train:  [[ 12.91333761]]    Loss_Validation:  [[ 11.1162699]]\n",
      "Loop  1221 :    Loss_Train:  [[ 12.91297186]]    Loss_Validation:  [[ 11.11620038]]\n",
      "Loop  1222 :    Loss_Train:  [[ 12.91260691]]    Loss_Validation:  [[ 11.11613137]]\n",
      "Loop  1223 :    Loss_Train:  [[ 12.91224274]]    Loss_Validation:  [[ 11.11606286]]\n",
      "Loop  1224 :    Loss_Train:  [[ 12.91187936]]    Loss_Validation:  [[ 11.11599485]]\n",
      "Loop  1225 :    Loss_Train:  [[ 12.91151676]]    Loss_Validation:  [[ 11.11592732]]\n",
      "Loop  1226 :    Loss_Train:  [[ 12.91115495]]    Loss_Validation:  [[ 11.11586029]]\n",
      "Loop  1227 :    Loss_Train:  [[ 12.91079392]]    Loss_Validation:  [[ 11.11579375]]\n",
      "Loop  1228 :    Loss_Train:  [[ 12.91043366]]    Loss_Validation:  [[ 11.1157277]]\n",
      "Loop  1229 :    Loss_Train:  [[ 12.91007418]]    Loss_Validation:  [[ 11.11566213]]\n",
      "Loop  1230 :    Loss_Train:  [[ 12.90971547]]    Loss_Validation:  [[ 11.11559705]]\n",
      "Loop  1231 :    Loss_Train:  [[ 12.90935754]]    Loss_Validation:  [[ 11.11553244]]\n",
      "Loop  1232 :    Loss_Train:  [[ 12.90900038]]    Loss_Validation:  [[ 11.11546831]]\n",
      "Loop  1233 :    Loss_Train:  [[ 12.90864398]]    Loss_Validation:  [[ 11.11540466]]\n",
      "Loop  1234 :    Loss_Train:  [[ 12.90828835]]    Loss_Validation:  [[ 11.11534148]]\n",
      "Loop  1235 :    Loss_Train:  [[ 12.90793349]]    Loss_Validation:  [[ 11.11527878]]\n",
      "Loop  1236 :    Loss_Train:  [[ 12.90757939]]    Loss_Validation:  [[ 11.11521654]]\n",
      "Loop  1237 :    Loss_Train:  [[ 12.90722605]]    Loss_Validation:  [[ 11.11515477]]\n",
      "Loop  1238 :    Loss_Train:  [[ 12.90687346]]    Loss_Validation:  [[ 11.11509347]]\n",
      "Loop  1239 :    Loss_Train:  [[ 12.90652164]]    Loss_Validation:  [[ 11.11503263]]\n",
      "Loop  1240 :    Loss_Train:  [[ 12.90617057]]    Loss_Validation:  [[ 11.11497225]]\n",
      "Loop  1241 :    Loss_Train:  [[ 12.90582025]]    Loss_Validation:  [[ 11.11491233]]\n",
      "Loop  1242 :    Loss_Train:  [[ 12.90547068]]    Loss_Validation:  [[ 11.11485286]]\n",
      "Loop  1243 :    Loss_Train:  [[ 12.90512186]]    Loss_Validation:  [[ 11.11479385]]\n",
      "Loop  1244 :    Loss_Train:  [[ 12.90477379]]    Loss_Validation:  [[ 11.11473529]]\n",
      "Loop  1245 :    Loss_Train:  [[ 12.90442646]]    Loss_Validation:  [[ 11.11467718]]\n",
      "Loop  1246 :    Loss_Train:  [[ 12.90407988]]    Loss_Validation:  [[ 11.11461952]]\n",
      "Loop  1247 :    Loss_Train:  [[ 12.90373404]]    Loss_Validation:  [[ 11.11456231]]\n",
      "Loop  1248 :    Loss_Train:  [[ 12.90338894]]    Loss_Validation:  [[ 11.11450554]]\n",
      "Loop  1249 :    Loss_Train:  [[ 12.90304457]]    Loss_Validation:  [[ 11.11444921]]\n",
      "Loop  1250 :    Loss_Train:  [[ 12.90270094]]    Loss_Validation:  [[ 11.11439333]]\n",
      "Loop  1251 :    Loss_Train:  [[ 12.90235805]]    Loss_Validation:  [[ 11.11433788]]\n",
      "Loop  1252 :    Loss_Train:  [[ 12.90201589]]    Loss_Validation:  [[ 11.11428286]]\n",
      "Loop  1253 :    Loss_Train:  [[ 12.90167445]]    Loss_Validation:  [[ 11.11422828]]\n",
      "Loop  1254 :    Loss_Train:  [[ 12.90133375]]    Loss_Validation:  [[ 11.11417413]]\n",
      "Loop  1255 :    Loss_Train:  [[ 12.90099377]]    Loss_Validation:  [[ 11.11412041]]\n",
      "Loop  1256 :    Loss_Train:  [[ 12.90065452]]    Loss_Validation:  [[ 11.11406712]]\n",
      "Loop  1257 :    Loss_Train:  [[ 12.90031599]]    Loss_Validation:  [[ 11.11401426]]\n",
      "Loop  1258 :    Loss_Train:  [[ 12.89997818]]    Loss_Validation:  [[ 11.11396182]]\n",
      "Loop  1259 :    Loss_Train:  [[ 12.89964109]]    Loss_Validation:  [[ 11.1139098]]\n",
      "Loop  1260 :    Loss_Train:  [[ 12.89930472]]    Loss_Validation:  [[ 11.1138582]]\n",
      "Loop  1261 :    Loss_Train:  [[ 12.89896907]]    Loss_Validation:  [[ 11.11380702]]\n",
      "Loop  1262 :    Loss_Train:  [[ 12.89863412]]    Loss_Validation:  [[ 11.11375625]]\n",
      "Loop  1263 :    Loss_Train:  [[ 12.89829989]]    Loss_Validation:  [[ 11.1137059]]\n",
      "Loop  1264 :    Loss_Train:  [[ 12.89796637]]    Loss_Validation:  [[ 11.11365596]]\n",
      "Loop  1265 :    Loss_Train:  [[ 12.89763356]]    Loss_Validation:  [[ 11.11360643]]\n",
      "Loop  1266 :    Loss_Train:  [[ 12.89730146]]    Loss_Validation:  [[ 11.11355731]]\n",
      "Loop  1267 :    Loss_Train:  [[ 12.89697006]]    Loss_Validation:  [[ 11.1135086]]\n",
      "Loop  1268 :    Loss_Train:  [[ 12.89663937]]    Loss_Validation:  [[ 11.11346029]]\n",
      "Loop  1269 :    Loss_Train:  [[ 12.89630937]]    Loss_Validation:  [[ 11.11341239]]\n",
      "Loop  1270 :    Loss_Train:  [[ 12.89598008]]    Loss_Validation:  [[ 11.11336488]]\n",
      "Loop  1271 :    Loss_Train:  [[ 12.89565148]]    Loss_Validation:  [[ 11.11331778]]\n",
      "Loop  1272 :    Loss_Train:  [[ 12.89532358]]    Loss_Validation:  [[ 11.11327107]]\n",
      "Loop  1273 :    Loss_Train:  [[ 12.89499638]]    Loss_Validation:  [[ 11.11322476]]\n",
      "Loop  1274 :    Loss_Train:  [[ 12.89466986]]    Loss_Validation:  [[ 11.11317884]]\n",
      "Loop  1275 :    Loss_Train:  [[ 12.89434404]]    Loss_Validation:  [[ 11.11313331]]\n",
      "Loop  1276 :    Loss_Train:  [[ 12.89401891]]    Loss_Validation:  [[ 11.11308817]]\n",
      "Loop  1277 :    Loss_Train:  [[ 12.89369447]]    Loss_Validation:  [[ 11.11304343]]\n",
      "Loop  1278 :    Loss_Train:  [[ 12.89337071]]    Loss_Validation:  [[ 11.11299906]]\n",
      "Loop  1279 :    Loss_Train:  [[ 12.89304764]]    Loss_Validation:  [[ 11.11295509]]\n",
      "Loop  1280 :    Loss_Train:  [[ 12.89272524]]    Loss_Validation:  [[ 11.11291149]]\n",
      "Loop  1281 :    Loss_Train:  [[ 12.89240353]]    Loss_Validation:  [[ 11.11286828]]\n",
      "Loop  1282 :    Loss_Train:  [[ 12.8920825]]    Loss_Validation:  [[ 11.11282544]]\n",
      "Loop  1283 :    Loss_Train:  [[ 12.89176215]]    Loss_Validation:  [[ 11.11278299]]\n",
      "Loop  1284 :    Loss_Train:  [[ 12.89144247]]    Loss_Validation:  [[ 11.11274091]]\n",
      "Loop  1285 :    Loss_Train:  [[ 12.89112347]]    Loss_Validation:  [[ 11.1126992]]\n",
      "Loop  1286 :    Loss_Train:  [[ 12.89080514]]    Loss_Validation:  [[ 11.11265787]]\n",
      "Loop  1287 :    Loss_Train:  [[ 12.89048748]]    Loss_Validation:  [[ 11.1126169]]\n",
      "Loop  1288 :    Loss_Train:  [[ 12.89017049]]    Loss_Validation:  [[ 11.11257631]]\n",
      "Loop  1289 :    Loss_Train:  [[ 12.88985416]]    Loss_Validation:  [[ 11.11253608]]\n",
      "Loop  1290 :    Loss_Train:  [[ 12.88953851]]    Loss_Validation:  [[ 11.11249622]]\n",
      "Loop  1291 :    Loss_Train:  [[ 12.88922351]]    Loss_Validation:  [[ 11.11245672]]\n",
      "Loop  1292 :    Loss_Train:  [[ 12.88890918]]    Loss_Validation:  [[ 11.11241759]]\n",
      "Loop  1293 :    Loss_Train:  [[ 12.88859551]]    Loss_Validation:  [[ 11.11237881]]\n",
      "Loop  1294 :    Loss_Train:  [[ 12.8882825]]    Loss_Validation:  [[ 11.1123404]]\n",
      "Loop  1295 :    Loss_Train:  [[ 12.88797015]]    Loss_Validation:  [[ 11.11230234]]\n",
      "Loop  1296 :    Loss_Train:  [[ 12.88765846]]    Loss_Validation:  [[ 11.11226463]]\n",
      "Loop  1297 :    Loss_Train:  [[ 12.88734741]]    Loss_Validation:  [[ 11.11222729]]\n",
      "Loop  1298 :    Loss_Train:  [[ 12.88703703]]    Loss_Validation:  [[ 11.11219029]]\n",
      "Loop  1299 :    Loss_Train:  [[ 12.88672729]]    Loss_Validation:  [[ 11.11215364]]\n",
      "Loop  1300 :    Loss_Train:  [[ 12.8864182]]    Loss_Validation:  [[ 11.11211735]]\n",
      "Loop  1301 :    Loss_Train:  [[ 12.88610976]]    Loss_Validation:  [[ 11.1120814]]\n",
      "Loop  1302 :    Loss_Train:  [[ 12.88580197]]    Loss_Validation:  [[ 11.11204579]]\n",
      "Loop  1303 :    Loss_Train:  [[ 12.88549482]]    Loss_Validation:  [[ 11.11201054]]\n",
      "Loop  1304 :    Loss_Train:  [[ 12.88518832]]    Loss_Validation:  [[ 11.11197562]]\n",
      "Loop  1305 :    Loss_Train:  [[ 12.88488246]]    Loss_Validation:  [[ 11.11194104]]\n",
      "Loop  1306 :    Loss_Train:  [[ 12.88457724]]    Loss_Validation:  [[ 11.11190681]]\n",
      "Loop  1307 :    Loss_Train:  [[ 12.88427265]]    Loss_Validation:  [[ 11.11187291]]\n",
      "Loop  1308 :    Loss_Train:  [[ 12.88396871]]    Loss_Validation:  [[ 11.11183935]]\n",
      "Loop  1309 :    Loss_Train:  [[ 12.8836654]]    Loss_Validation:  [[ 11.11180613]]\n",
      "Loop  1310 :    Loss_Train:  [[ 12.88336272]]    Loss_Validation:  [[ 11.11177323]]\n",
      "Loop  1311 :    Loss_Train:  [[ 12.88306068]]    Loss_Validation:  [[ 11.11174067]]\n",
      "Loop  1312 :    Loss_Train:  [[ 12.88275927]]    Loss_Validation:  [[ 11.11170844]]\n",
      "Loop  1313 :    Loss_Train:  [[ 12.88245848]]    Loss_Validation:  [[ 11.11167654]]\n",
      "Loop  1314 :    Loss_Train:  [[ 12.88215833]]    Loss_Validation:  [[ 11.11164497]]\n",
      "Loop  1315 :    Loss_Train:  [[ 12.8818588]]    Loss_Validation:  [[ 11.11161372]]\n",
      "Loop  1316 :    Loss_Train:  [[ 12.8815599]]    Loss_Validation:  [[ 11.11158279]]\n",
      "Loop  1317 :    Loss_Train:  [[ 12.88126162]]    Loss_Validation:  [[ 11.11155219]]\n",
      "Loop  1318 :    Loss_Train:  [[ 12.88096396]]    Loss_Validation:  [[ 11.11152191]]\n",
      "Loop  1319 :    Loss_Train:  [[ 12.88066692]]    Loss_Validation:  [[ 11.11149195]]\n",
      "Loop  1320 :    Loss_Train:  [[ 12.8803705]]    Loss_Validation:  [[ 11.11146231]]\n",
      "Loop  1321 :    Loss_Train:  [[ 12.8800747]]    Loss_Validation:  [[ 11.11143298]]\n",
      "Loop  1322 :    Loss_Train:  [[ 12.87977952]]    Loss_Validation:  [[ 11.11140398]]\n",
      "Loop  1323 :    Loss_Train:  [[ 12.87948494]]    Loss_Validation:  [[ 11.11137528]]\n",
      "Loop  1324 :    Loss_Train:  [[ 12.87919099]]    Loss_Validation:  [[ 11.1113469]]\n",
      "Loop  1325 :    Loss_Train:  [[ 12.87889764]]    Loss_Validation:  [[ 11.11131882]]\n",
      "Loop  1326 :    Loss_Train:  [[ 12.8786049]]    Loss_Validation:  [[ 11.11129106]]\n",
      "Loop  1327 :    Loss_Train:  [[ 12.87831277]]    Loss_Validation:  [[ 11.11126361]]\n",
      "Loop  1328 :    Loss_Train:  [[ 12.87802125]]    Loss_Validation:  [[ 11.11123646]]\n",
      "Loop  1329 :    Loss_Train:  [[ 12.87773034]]    Loss_Validation:  [[ 11.11120962]]\n",
      "Loop  1330 :    Loss_Train:  [[ 12.87744003]]    Loss_Validation:  [[ 11.11118308]]\n",
      "Loop  1331 :    Loss_Train:  [[ 12.87715032]]    Loss_Validation:  [[ 11.11115685]]\n",
      "Loop  1332 :    Loss_Train:  [[ 12.87686121]]    Loss_Validation:  [[ 11.11113091]]\n",
      "Loop  1333 :    Loss_Train:  [[ 12.8765727]]    Loss_Validation:  [[ 11.11110528]]\n",
      "Loop  1334 :    Loss_Train:  [[ 12.87628479]]    Loss_Validation:  [[ 11.11107994]]\n",
      "Loop  1335 :    Loss_Train:  [[ 12.87599748]]    Loss_Validation:  [[ 11.11105491]]\n",
      "Loop  1336 :    Loss_Train:  [[ 12.87571076]]    Loss_Validation:  [[ 11.11103016]]\n",
      "Loop  1337 :    Loss_Train:  [[ 12.87542464]]    Loss_Validation:  [[ 11.11100571]]\n",
      "Loop  1338 :    Loss_Train:  [[ 12.87513911]]    Loss_Validation:  [[ 11.11098156]]\n",
      "Loop  1339 :    Loss_Train:  [[ 12.87485417]]    Loss_Validation:  [[ 11.1109577]]\n",
      "Loop  1340 :    Loss_Train:  [[ 12.87456982]]    Loss_Validation:  [[ 11.11093412]]\n",
      "Loop  1341 :    Loss_Train:  [[ 12.87428606]]    Loss_Validation:  [[ 11.11091084]]\n",
      "Loop  1342 :    Loss_Train:  [[ 12.87400288]]    Loss_Validation:  [[ 11.11088784]]\n",
      "Loop  1343 :    Loss_Train:  [[ 12.8737203]]    Loss_Validation:  [[ 11.11086513]]\n",
      "Loop  1344 :    Loss_Train:  [[ 12.87343829]]    Loss_Validation:  [[ 11.11084271]]\n",
      "Loop  1345 :    Loss_Train:  [[ 12.87315687]]    Loss_Validation:  [[ 11.11082056]]\n",
      "Loop  1346 :    Loss_Train:  [[ 12.87287603]]    Loss_Validation:  [[ 11.11079871]]\n",
      "Loop  1347 :    Loss_Train:  [[ 12.87259577]]    Loss_Validation:  [[ 11.11077713]]\n",
      "Loop  1348 :    Loss_Train:  [[ 12.87231609]]    Loss_Validation:  [[ 11.11075583]]\n",
      "Loop  1349 :    Loss_Train:  [[ 12.87203698]]    Loss_Validation:  [[ 11.11073481]]\n",
      "Loop  1350 :    Loss_Train:  [[ 12.87175845]]    Loss_Validation:  [[ 11.11071407]]\n",
      "Loop  1351 :    Loss_Train:  [[ 12.8714805]]    Loss_Validation:  [[ 11.1106936]]\n",
      "Loop  1352 :    Loss_Train:  [[ 12.87120312]]    Loss_Validation:  [[ 11.11067341]]\n",
      "Loop  1353 :    Loss_Train:  [[ 12.87092631]]    Loss_Validation:  [[ 11.11065349]]\n",
      "Loop  1354 :    Loss_Train:  [[ 12.87065007]]    Loss_Validation:  [[ 11.11063384]]\n",
      "Loop  1355 :    Loss_Train:  [[ 12.8703744]]    Loss_Validation:  [[ 11.11061447]]\n",
      "Loop  1356 :    Loss_Train:  [[ 12.8700993]]    Loss_Validation:  [[ 11.11059536]]\n",
      "Loop  1357 :    Loss_Train:  [[ 12.86982476]]    Loss_Validation:  [[ 11.11057653]]\n",
      "Loop  1358 :    Loss_Train:  [[ 12.86955079]]    Loss_Validation:  [[ 11.11055796]]\n",
      "Loop  1359 :    Loss_Train:  [[ 12.86927739]]    Loss_Validation:  [[ 11.11053965]]\n",
      "Loop  1360 :    Loss_Train:  [[ 12.86900454]]    Loss_Validation:  [[ 11.11052161]]\n",
      "Loop  1361 :    Loss_Train:  [[ 12.86873226]]    Loss_Validation:  [[ 11.11050384]]\n",
      "Loop  1362 :    Loss_Train:  [[ 12.86846053]]    Loss_Validation:  [[ 11.11048632]]\n",
      "Loop  1363 :    Loss_Train:  [[ 12.86818937]]    Loss_Validation:  [[ 11.11046907]]\n",
      "Loop  1364 :    Loss_Train:  [[ 12.86791876]]    Loss_Validation:  [[ 11.11045208]]\n",
      "Loop  1365 :    Loss_Train:  [[ 12.86764871]]    Loss_Validation:  [[ 11.11043535]]\n",
      "Loop  1366 :    Loss_Train:  [[ 12.86737921]]    Loss_Validation:  [[ 11.11041887]]\n",
      "Loop  1367 :    Loss_Train:  [[ 12.86711026]]    Loss_Validation:  [[ 11.11040265]]\n",
      "Loop  1368 :    Loss_Train:  [[ 12.86684187]]    Loss_Validation:  [[ 11.11038669]]\n",
      "Loop  1369 :    Loss_Train:  [[ 12.86657403]]    Loss_Validation:  [[ 11.11037097]]\n",
      "Loop  1370 :    Loss_Train:  [[ 12.86630673]]    Loss_Validation:  [[ 11.11035552]]\n",
      "Loop  1371 :    Loss_Train:  [[ 12.86603999]]    Loss_Validation:  [[ 11.11034031]]\n",
      "Loop  1372 :    Loss_Train:  [[ 12.86577379]]    Loss_Validation:  [[ 11.11032536]]\n",
      "Loop  1373 :    Loss_Train:  [[ 12.86550813]]    Loss_Validation:  [[ 11.11031065]]\n",
      "Loop  1374 :    Loss_Train:  [[ 12.86524303]]    Loss_Validation:  [[ 11.11029619]]\n",
      "Loop  1375 :    Loss_Train:  [[ 12.86497846]]    Loss_Validation:  [[ 11.11028198]]\n",
      "Loop  1376 :    Loss_Train:  [[ 12.86471443]]    Loss_Validation:  [[ 11.11026801]]\n",
      "Loop  1377 :    Loss_Train:  [[ 12.86445095]]    Loss_Validation:  [[ 11.11025429]]\n",
      "Loop  1378 :    Loss_Train:  [[ 12.864188]]    Loss_Validation:  [[ 11.11024082]]\n",
      "Loop  1379 :    Loss_Train:  [[ 12.86392559]]    Loss_Validation:  [[ 11.11022758]]\n",
      "Loop  1380 :    Loss_Train:  [[ 12.86366372]]    Loss_Validation:  [[ 11.11021459]]\n",
      "Loop  1381 :    Loss_Train:  [[ 12.86340238]]    Loss_Validation:  [[ 11.11020184]]\n",
      "Loop  1382 :    Loss_Train:  [[ 12.86314158]]    Loss_Validation:  [[ 11.11018932]]\n",
      "Loop  1383 :    Loss_Train:  [[ 12.86288131]]    Loss_Validation:  [[ 11.11017705]]\n",
      "Loop  1384 :    Loss_Train:  [[ 12.86262157]]    Loss_Validation:  [[ 11.11016501]]\n",
      "Loop  1385 :    Loss_Train:  [[ 12.86236236]]    Loss_Validation:  [[ 11.11015321]]\n",
      "Loop  1386 :    Loss_Train:  [[ 12.86210368]]    Loss_Validation:  [[ 11.11014164]]\n",
      "Loop  1387 :    Loss_Train:  [[ 12.86184553]]    Loss_Validation:  [[ 11.11013031]]\n",
      "Loop  1388 :    Loss_Train:  [[ 12.86158791]]    Loss_Validation:  [[ 11.11011921]]\n",
      "Loop  1389 :    Loss_Train:  [[ 12.8613308]]    Loss_Validation:  [[ 11.11010834]]\n",
      "Loop  1390 :    Loss_Train:  [[ 12.86107423]]    Loss_Validation:  [[ 11.1100977]]\n",
      "Loop  1391 :    Loss_Train:  [[ 12.86081818]]    Loss_Validation:  [[ 11.11008729]]\n",
      "Loop  1392 :    Loss_Train:  [[ 12.86056264]]    Loss_Validation:  [[ 11.11007711]]\n",
      "Loop  1393 :    Loss_Train:  [[ 12.86030763]]    Loss_Validation:  [[ 11.11006715]]\n",
      "Loop  1394 :    Loss_Train:  [[ 12.86005314]]    Loss_Validation:  [[ 11.11005742]]\n",
      "Loop  1395 :    Loss_Train:  [[ 12.85979917]]    Loss_Validation:  [[ 11.11004792]]\n",
      "Loop  1396 :    Loss_Train:  [[ 12.85954571]]    Loss_Validation:  [[ 11.11003864]]\n",
      "Loop  1397 :    Loss_Train:  [[ 12.85929277]]    Loss_Validation:  [[ 11.11002959]]\n",
      "Loop  1398 :    Loss_Train:  [[ 12.85904034]]    Loss_Validation:  [[ 11.11002076]]\n",
      "Loop  1399 :    Loss_Train:  [[ 12.85878842]]    Loss_Validation:  [[ 11.11001214]]\n",
      "Loop  1400 :    Loss_Train:  [[ 12.85853702]]    Loss_Validation:  [[ 11.11000375]]\n",
      "Loop  1401 :    Loss_Train:  [[ 12.85828613]]    Loss_Validation:  [[ 11.10999558]]\n",
      "Loop  1402 :    Loss_Train:  [[ 12.85803575]]    Loss_Validation:  [[ 11.10998763]]\n",
      "Loop  1403 :    Loss_Train:  [[ 12.85778587]]    Loss_Validation:  [[ 11.10997989]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1404 :    Loss_Train:  [[ 12.85753651]]    Loss_Validation:  [[ 11.10997237]]\n",
      "Loop  1405 :    Loss_Train:  [[ 12.85728765]]    Loss_Validation:  [[ 11.10996506]]\n",
      "Loop  1406 :    Loss_Train:  [[ 12.85703929]]    Loss_Validation:  [[ 11.10995797]]\n",
      "Loop  1407 :    Loss_Train:  [[ 12.85679144]]    Loss_Validation:  [[ 11.10995109]]\n",
      "Loop  1408 :    Loss_Train:  [[ 12.85654409]]    Loss_Validation:  [[ 11.10994442]]\n",
      "Loop  1409 :    Loss_Train:  [[ 12.85629724]]    Loss_Validation:  [[ 11.10993796]]\n",
      "Loop  1410 :    Loss_Train:  [[ 12.8560509]]    Loss_Validation:  [[ 11.10993172]]\n",
      "Loop  1411 :    Loss_Train:  [[ 12.85580505]]    Loss_Validation:  [[ 11.10992568]]\n",
      "Loop  1412 :    Loss_Train:  [[ 12.8555597]]    Loss_Validation:  [[ 11.10991985]]\n",
      "Loop  1413 :    Loss_Train:  [[ 12.85531484]]    Loss_Validation:  [[ 11.10991423]]\n",
      "Loop  1414 :    Loss_Train:  [[ 12.85507049]]    Loss_Validation:  [[ 11.10990882]]\n",
      "Loop  1415 :    Loss_Train:  [[ 12.85482662]]    Loss_Validation:  [[ 11.10990361]]\n",
      "Loop  1416 :    Loss_Train:  [[ 12.85458325]]    Loss_Validation:  [[ 11.1098986]]\n",
      "Loop  1417 :    Loss_Train:  [[ 12.85434037]]    Loss_Validation:  [[ 11.1098938]]\n",
      "Loop  1418 :    Loss_Train:  [[ 12.85409799]]    Loss_Validation:  [[ 11.1098892]]\n",
      "Loop  1419 :    Loss_Train:  [[ 12.85385609]]    Loss_Validation:  [[ 11.1098848]]\n",
      "Loop  1420 :    Loss_Train:  [[ 12.85361468]]    Loss_Validation:  [[ 11.1098806]]\n",
      "Loop  1421 :    Loss_Train:  [[ 12.85337376]]    Loss_Validation:  [[ 11.10987661]]\n",
      "Loop  1422 :    Loss_Train:  [[ 12.85313333]]    Loss_Validation:  [[ 11.10987281]]\n",
      "Loop  1423 :    Loss_Train:  [[ 12.85289338]]    Loss_Validation:  [[ 11.10986921]]\n",
      "Loop  1424 :    Loss_Train:  [[ 12.85265392]]    Loss_Validation:  [[ 11.1098658]]\n",
      "Loop  1425 :    Loss_Train:  [[ 12.85241494]]    Loss_Validation:  [[ 11.10986259]]\n",
      "Loop  1426 :    Loss_Train:  [[ 12.85217644]]    Loss_Validation:  [[ 11.10985958]]\n",
      "Loop  1427 :    Loss_Train:  [[ 12.85193842]]    Loss_Validation:  [[ 11.10985676]]\n",
      "Loop  1428 :    Loss_Train:  [[ 12.85170088]]    Loss_Validation:  [[ 11.10985414]]\n",
      "Loop  1429 :    Loss_Train:  [[ 12.85146382]]    Loss_Validation:  [[ 11.1098517]]\n",
      "Loop  1430 :    Loss_Train:  [[ 12.85122724]]    Loss_Validation:  [[ 11.10984946]]\n",
      "Loop  1431 :    Loss_Train:  [[ 12.85099114]]    Loss_Validation:  [[ 11.10984741]]\n",
      "Loop  1432 :    Loss_Train:  [[ 12.85075551]]    Loss_Validation:  [[ 11.10984555]]\n",
      "Loop  1433 :    Loss_Train:  [[ 12.85052036]]    Loss_Validation:  [[ 11.10984388]]\n",
      "Loop  1434 :    Loss_Train:  [[ 12.85028568]]    Loss_Validation:  [[ 11.10984239]]\n",
      "Loop  1435 :    Loss_Train:  [[ 12.85005147]]    Loss_Validation:  [[ 11.10984109]]\n",
      "Loop  1436 :    Loss_Train:  [[ 12.84981773]]    Loss_Validation:  [[ 11.10983998]]\n",
      "Loop  1437 :    Loss_Train:  [[ 12.84958446]]    Loss_Validation:  [[ 11.10983906]]\n",
      "Loop  1438 :    Loss_Train:  [[ 12.84935167]]    Loss_Validation:  [[ 11.10983832]]\n",
      "Loop  1439 :    Loss_Train:  [[ 12.84911934]]    Loss_Validation:  [[ 11.10983776]]\n",
      "Loop  1440 :    Loss_Train:  [[ 12.84888747]]    Loss_Validation:  [[ 11.10983738]]\n",
      "Loop  1441 :    Loss_Train:  [[ 12.84865608]]    Loss_Validation:  [[ 11.10983719]]\n",
      "Loop  1442 :    Loss_Train:  [[ 12.84842514]]    Loss_Validation:  [[ 11.10983718]]\n",
      "Loop  1443 :    Loss_Train:  [[ 12.84819467]]    Loss_Validation:  [[ 11.10983735]]\n",
      "Loop  1444 :    Loss_Train:  [[ 12.84796467]]    Loss_Validation:  [[ 11.1098377]]\n",
      "Loop  1445 :    Loss_Train:  [[ 12.84773513]]    Loss_Validation:  [[ 11.10983822]]\n",
      "Loop  1446 :    Loss_Train:  [[ 12.84750604]]    Loss_Validation:  [[ 11.10983893]]\n",
      "Loop  1447 :    Loss_Train:  [[ 12.84727742]]    Loss_Validation:  [[ 11.10983981]]\n",
      "Loop  1448 :    Loss_Train:  [[ 12.84704925]]    Loss_Validation:  [[ 11.10984086]]\n",
      "Loop  1449 :    Loss_Train:  [[ 12.84682154]]    Loss_Validation:  [[ 11.1098421]]\n",
      "Loop  1450 :    Loss_Train:  [[ 12.84659429]]    Loss_Validation:  [[ 11.1098435]]\n",
      "Loop  1451 :    Loss_Train:  [[ 12.8463675]]    Loss_Validation:  [[ 11.10984508]]\n",
      "Loop  1452 :    Loss_Train:  [[ 12.84614116]]    Loss_Validation:  [[ 11.10984684]]\n",
      "Loop  1453 :    Loss_Train:  [[ 12.84591527]]    Loss_Validation:  [[ 11.10984876]]\n",
      "Loop  1454 :    Loss_Train:  [[ 12.84568983]]    Loss_Validation:  [[ 11.10985086]]\n",
      "Loop  1455 :    Loss_Train:  [[ 12.84546485]]    Loss_Validation:  [[ 11.10985313]]\n",
      "Loop  1456 :    Loss_Train:  [[ 12.84524031]]    Loss_Validation:  [[ 11.10985556]]\n",
      "Loop  1457 :    Loss_Train:  [[ 12.84501623]]    Loss_Validation:  [[ 11.10985817]]\n",
      "Loop  1458 :    Loss_Train:  [[ 12.84479259]]    Loss_Validation:  [[ 11.10986094]]\n",
      "Loop  1459 :    Loss_Train:  [[ 12.8445694]]    Loss_Validation:  [[ 11.10986388]]\n",
      "Loop  1460 :    Loss_Train:  [[ 12.84434666]]    Loss_Validation:  [[ 11.10986699]]\n",
      "Loop  1461 :    Loss_Train:  [[ 12.84412436]]    Loss_Validation:  [[ 11.10987027]]\n",
      "Loop  1462 :    Loss_Train:  [[ 12.84390251]]    Loss_Validation:  [[ 11.1098737]]\n",
      "Loop  1463 :    Loss_Train:  [[ 12.8436811]]    Loss_Validation:  [[ 11.10987731]]\n",
      "Loop  1464 :    Loss_Train:  [[ 12.84346013]]    Loss_Validation:  [[ 11.10988107]]\n",
      "Loop  1465 :    Loss_Train:  [[ 12.8432396]]    Loss_Validation:  [[ 11.109885]]\n",
      "Loop  1466 :    Loss_Train:  [[ 12.84301951]]    Loss_Validation:  [[ 11.10988909]]\n",
      "Loop  1467 :    Loss_Train:  [[ 12.84279987]]    Loss_Validation:  [[ 11.10989334]]\n",
      "Loop  1468 :    Loss_Train:  [[ 12.84258066]]    Loss_Validation:  [[ 11.10989775]]\n",
      "Loop  1469 :    Loss_Train:  [[ 12.84236189]]    Loss_Validation:  [[ 11.10990233]]\n",
      "Loop  1470 :    Loss_Train:  [[ 12.84214355]]    Loss_Validation:  [[ 11.10990706]]\n",
      "Loop  1471 :    Loss_Train:  [[ 12.84192565]]    Loss_Validation:  [[ 11.10991194]]\n",
      "Loop  1472 :    Loss_Train:  [[ 12.84170818]]    Loss_Validation:  [[ 11.10991699]]\n",
      "Loop  1473 :    Loss_Train:  [[ 12.84149115]]    Loss_Validation:  [[ 11.10992219]]\n",
      "Loop  1474 :    Loss_Train:  [[ 12.84127455]]    Loss_Validation:  [[ 11.10992755]]\n",
      "Loop  1475 :    Loss_Train:  [[ 12.84105838]]    Loss_Validation:  [[ 11.10993307]]\n",
      "Loop  1476 :    Loss_Train:  [[ 12.84084264]]    Loss_Validation:  [[ 11.10993873]]\n",
      "Loop  1477 :    Loss_Train:  [[ 12.84062733]]    Loss_Validation:  [[ 11.10994456]]\n",
      "Loop  1478 :    Loss_Train:  [[ 12.84041245]]    Loss_Validation:  [[ 11.10995053]]\n",
      "Loop  1479 :    Loss_Train:  [[ 12.840198]]    Loss_Validation:  [[ 11.10995666]]\n",
      "Loop  1480 :    Loss_Train:  [[ 12.83998397]]    Loss_Validation:  [[ 11.10996294]]\n",
      "Loop  1481 :    Loss_Train:  [[ 12.83977037]]    Loss_Validation:  [[ 11.10996937]]\n",
      "Loop  1482 :    Loss_Train:  [[ 12.8395572]]    Loss_Validation:  [[ 11.10997595]]\n",
      "Loop  1483 :    Loss_Train:  [[ 12.83934444]]    Loss_Validation:  [[ 11.10998268]]\n",
      "Loop  1484 :    Loss_Train:  [[ 12.83913211]]    Loss_Validation:  [[ 11.10998955]]\n",
      "Loop  1485 :    Loss_Train:  [[ 12.83892021]]    Loss_Validation:  [[ 11.10999658]]\n",
      "Loop  1486 :    Loss_Train:  [[ 12.83870872]]    Loss_Validation:  [[ 11.11000375]]\n",
      "Loop  1487 :    Loss_Train:  [[ 12.83849765]]    Loss_Validation:  [[ 11.11001107]]\n",
      "Loop  1488 :    Loss_Train:  [[ 12.838287]]    Loss_Validation:  [[ 11.11001854]]\n",
      "Loop  1489 :    Loss_Train:  [[ 12.83807677]]    Loss_Validation:  [[ 11.11002615]]\n",
      "Loop  1490 :    Loss_Train:  [[ 12.83786696]]    Loss_Validation:  [[ 11.11003391]]\n",
      "Loop  1491 :    Loss_Train:  [[ 12.83765757]]    Loss_Validation:  [[ 11.11004181]]\n",
      "Loop  1492 :    Loss_Train:  [[ 12.83744858]]    Loss_Validation:  [[ 11.11004985]]\n",
      "Loop  1493 :    Loss_Train:  [[ 12.83724002]]    Loss_Validation:  [[ 11.11005804]]\n",
      "Loop  1494 :    Loss_Train:  [[ 12.83703186]]    Loss_Validation:  [[ 11.11006636]]\n",
      "Loop  1495 :    Loss_Train:  [[ 12.83682412]]    Loss_Validation:  [[ 11.11007483]]\n",
      "Loop  1496 :    Loss_Train:  [[ 12.83661679]]    Loss_Validation:  [[ 11.11008344]]\n",
      "Loop  1497 :    Loss_Train:  [[ 12.83640988]]    Loss_Validation:  [[ 11.11009219]]\n",
      "Loop  1498 :    Loss_Train:  [[ 12.83620337]]    Loss_Validation:  [[ 11.11010108]]\n",
      "Loop  1499 :    Loss_Train:  [[ 12.83599727]]    Loss_Validation:  [[ 11.11011011]]\n",
      "Loop  1500 :    Loss_Train:  [[ 12.83579158]]    Loss_Validation:  [[ 11.11011927]]\n",
      "Loop  1501 :    Loss_Train:  [[ 12.83558629]]    Loss_Validation:  [[ 11.11012858]]\n",
      "Loop  1502 :    Loss_Train:  [[ 12.83538141]]    Loss_Validation:  [[ 11.11013802]]\n",
      "Loop  1503 :    Loss_Train:  [[ 12.83517694]]    Loss_Validation:  [[ 11.11014759]]\n",
      "Loop  1504 :    Loss_Train:  [[ 12.83497287]]    Loss_Validation:  [[ 11.1101573]]\n",
      "Loop  1505 :    Loss_Train:  [[ 12.83476921]]    Loss_Validation:  [[ 11.11016715]]\n",
      "Loop  1506 :    Loss_Train:  [[ 12.83456594]]    Loss_Validation:  [[ 11.11017713]]\n",
      "Loop  1507 :    Loss_Train:  [[ 12.83436308]]    Loss_Validation:  [[ 11.11018724]]\n",
      "Loop  1508 :    Loss_Train:  [[ 12.83416062]]    Loss_Validation:  [[ 11.11019748]]\n",
      "Loop  1509 :    Loss_Train:  [[ 12.83395856]]    Loss_Validation:  [[ 11.11020786]]\n",
      "Loop  1510 :    Loss_Train:  [[ 12.8337569]]    Loss_Validation:  [[ 11.11021837]]\n",
      "Loop  1511 :    Loss_Train:  [[ 12.83355564]]    Loss_Validation:  [[ 11.11022901]]\n",
      "Loop  1512 :    Loss_Train:  [[ 12.83335477]]    Loss_Validation:  [[ 11.11023978]]\n",
      "Loop  1513 :    Loss_Train:  [[ 12.8331543]]    Loss_Validation:  [[ 11.11025068]]\n",
      "Loop  1514 :    Loss_Train:  [[ 12.83295423]]    Loss_Validation:  [[ 11.11026171]]\n",
      "Loop  1515 :    Loss_Train:  [[ 12.83275455]]    Loss_Validation:  [[ 11.11027287]]\n",
      "Loop  1516 :    Loss_Train:  [[ 12.83255526]]    Loss_Validation:  [[ 11.11028415]]\n",
      "Loop  1517 :    Loss_Train:  [[ 12.83235637]]    Loss_Validation:  [[ 11.11029557]]\n",
      "Loop  1518 :    Loss_Train:  [[ 12.83215787]]    Loss_Validation:  [[ 11.1103071]]\n",
      "Loop  1519 :    Loss_Train:  [[ 12.83195976]]    Loss_Validation:  [[ 11.11031877]]\n",
      "Loop  1520 :    Loss_Train:  [[ 12.83176204]]    Loss_Validation:  [[ 11.11033056]]\n",
      "Loop  1521 :    Loss_Train:  [[ 12.83156471]]    Loss_Validation:  [[ 11.11034248]]\n",
      "Loop  1522 :    Loss_Train:  [[ 12.83136777]]    Loss_Validation:  [[ 11.11035452]]\n",
      "Loop  1523 :    Loss_Train:  [[ 12.83117122]]    Loss_Validation:  [[ 11.11036668]]\n",
      "Loop  1524 :    Loss_Train:  [[ 12.83097505]]    Loss_Validation:  [[ 11.11037897]]\n",
      "Loop  1525 :    Loss_Train:  [[ 12.83077927]]    Loss_Validation:  [[ 11.11039137]]\n",
      "Loop  1526 :    Loss_Train:  [[ 12.83058387]]    Loss_Validation:  [[ 11.11040391]]\n",
      "Loop  1527 :    Loss_Train:  [[ 12.83038886]]    Loss_Validation:  [[ 11.11041656]]\n",
      "Loop  1528 :    Loss_Train:  [[ 12.83019424]]    Loss_Validation:  [[ 11.11042933]]\n",
      "Loop  1529 :    Loss_Train:  [[ 12.82999999]]    Loss_Validation:  [[ 11.11044222]]\n",
      "Loop  1530 :    Loss_Train:  [[ 12.82980613]]    Loss_Validation:  [[ 11.11045524]]\n",
      "Loop  1531 :    Loss_Train:  [[ 12.82961265]]    Loss_Validation:  [[ 11.11046837]]\n",
      "Loop  1532 :    Loss_Train:  [[ 12.82941954]]    Loss_Validation:  [[ 11.11048162]]\n",
      "Loop  1533 :    Loss_Train:  [[ 12.82922682]]    Loss_Validation:  [[ 11.11049498]]\n",
      "Loop  1534 :    Loss_Train:  [[ 12.82903448]]    Loss_Validation:  [[ 11.11050847]]\n",
      "Loop  1535 :    Loss_Train:  [[ 12.82884251]]    Loss_Validation:  [[ 11.11052207]]\n",
      "Loop  1536 :    Loss_Train:  [[ 12.82865092]]    Loss_Validation:  [[ 11.11053579]]\n",
      "Loop  1537 :    Loss_Train:  [[ 12.82845971]]    Loss_Validation:  [[ 11.11054962]]\n",
      "Loop  1538 :    Loss_Train:  [[ 12.82826887]]    Loss_Validation:  [[ 11.11056357]]\n",
      "Loop  1539 :    Loss_Train:  [[ 12.8280784]]    Loss_Validation:  [[ 11.11057764]]\n",
      "Loop  1540 :    Loss_Train:  [[ 12.82788831]]    Loss_Validation:  [[ 11.11059181]]\n",
      "Loop  1541 :    Loss_Train:  [[ 12.8276986]]    Loss_Validation:  [[ 11.1106061]]\n",
      "Loop  1542 :    Loss_Train:  [[ 12.82750925]]    Loss_Validation:  [[ 11.11062051]]\n",
      "Loop  1543 :    Loss_Train:  [[ 12.82732028]]    Loss_Validation:  [[ 11.11063502]]\n",
      "Loop  1544 :    Loss_Train:  [[ 12.82713167]]    Loss_Validation:  [[ 11.11064965]]\n",
      "Loop  1545 :    Loss_Train:  [[ 12.82694344]]    Loss_Validation:  [[ 11.11066439]]\n",
      "Loop  1546 :    Loss_Train:  [[ 12.82675557]]    Loss_Validation:  [[ 11.11067924]]\n",
      "Loop  1547 :    Loss_Train:  [[ 12.82656807]]    Loss_Validation:  [[ 11.1106942]]\n",
      "Loop  1548 :    Loss_Train:  [[ 12.82638094]]    Loss_Validation:  [[ 11.11070927]]\n",
      "Loop  1549 :    Loss_Train:  [[ 12.82619417]]    Loss_Validation:  [[ 11.11072444]]\n",
      "Loop  1550 :    Loss_Train:  [[ 12.82600777]]    Loss_Validation:  [[ 11.11073973]]\n",
      "Loop  1551 :    Loss_Train:  [[ 12.82582174]]    Loss_Validation:  [[ 11.11075513]]\n",
      "Loop  1552 :    Loss_Train:  [[ 12.82563607]]    Loss_Validation:  [[ 11.11077063]]\n",
      "Loop  1553 :    Loss_Train:  [[ 12.82545076]]    Loss_Validation:  [[ 11.11078624]]\n",
      "Loop  1554 :    Loss_Train:  [[ 12.82526581]]    Loss_Validation:  [[ 11.11080195]]\n",
      "Loop  1555 :    Loss_Train:  [[ 12.82508123]]    Loss_Validation:  [[ 11.11081778]]\n",
      "Loop  1556 :    Loss_Train:  [[ 12.82489701]]    Loss_Validation:  [[ 11.1108337]]\n",
      "Loop  1557 :    Loss_Train:  [[ 12.82471314]]    Loss_Validation:  [[ 11.11084974]]\n",
      "Loop  1558 :    Loss_Train:  [[ 12.82452964]]    Loss_Validation:  [[ 11.11086587]]\n",
      "Loop  1559 :    Loss_Train:  [[ 12.82434649]]    Loss_Validation:  [[ 11.11088211]]\n",
      "Loop  1560 :    Loss_Train:  [[ 12.8241637]]    Loss_Validation:  [[ 11.11089846]]\n",
      "Loop  1561 :    Loss_Train:  [[ 12.82398127]]    Loss_Validation:  [[ 11.11091491]]\n",
      "Loop  1562 :    Loss_Train:  [[ 12.8237992]]    Loss_Validation:  [[ 11.11093145]]\n",
      "Loop  1563 :    Loss_Train:  [[ 12.82361748]]    Loss_Validation:  [[ 11.11094811]]\n",
      "Loop  1564 :    Loss_Train:  [[ 12.82343611]]    Loss_Validation:  [[ 11.11096486]]\n",
      "Loop  1565 :    Loss_Train:  [[ 12.8232551]]    Loss_Validation:  [[ 11.11098171]]\n",
      "Loop  1566 :    Loss_Train:  [[ 12.82307444]]    Loss_Validation:  [[ 11.11099867]]\n",
      "Loop  1567 :    Loss_Train:  [[ 12.82289413]]    Loss_Validation:  [[ 11.11101572]]\n",
      "Loop  1568 :    Loss_Train:  [[ 12.82271418]]    Loss_Validation:  [[ 11.11103288]]\n",
      "Loop  1569 :    Loss_Train:  [[ 12.82253457]]    Loss_Validation:  [[ 11.11105013]]\n",
      "Loop  1570 :    Loss_Train:  [[ 12.82235532]]    Loss_Validation:  [[ 11.11106748]]\n",
      "Loop  1571 :    Loss_Train:  [[ 12.82217641]]    Loss_Validation:  [[ 11.11108493]]\n",
      "Loop  1572 :    Loss_Train:  [[ 12.82199785]]    Loss_Validation:  [[ 11.11110248]]\n",
      "Loop  1573 :    Loss_Train:  [[ 12.82181964]]    Loss_Validation:  [[ 11.11112012]]\n",
      "Loop  1574 :    Loss_Train:  [[ 12.82164178]]    Loss_Validation:  [[ 11.11113786]]\n",
      "Loop  1575 :    Loss_Train:  [[ 12.82146427]]    Loss_Validation:  [[ 11.1111557]]\n",
      "Loop  1576 :    Loss_Train:  [[ 12.82128709]]    Loss_Validation:  [[ 11.11117363]]\n",
      "Loop  1577 :    Loss_Train:  [[ 12.82111027]]    Loss_Validation:  [[ 11.11119166]]\n",
      "Loop  1578 :    Loss_Train:  [[ 12.82093379]]    Loss_Validation:  [[ 11.11120978]]\n",
      "Loop  1579 :    Loss_Train:  [[ 12.82075765]]    Loss_Validation:  [[ 11.111228]]\n",
      "Loop  1580 :    Loss_Train:  [[ 12.82058185]]    Loss_Validation:  [[ 11.11124631]]\n",
      "Loop  1581 :    Loss_Train:  [[ 12.82040639]]    Loss_Validation:  [[ 11.11126471]]\n",
      "Loop  1582 :    Loss_Train:  [[ 12.82023128]]    Loss_Validation:  [[ 11.11128321]]\n",
      "Loop  1583 :    Loss_Train:  [[ 12.82005651]]    Loss_Validation:  [[ 11.1113018]]\n",
      "Loop  1584 :    Loss_Train:  [[ 12.81988207]]    Loss_Validation:  [[ 11.11132048]]\n",
      "Loop  1585 :    Loss_Train:  [[ 12.81970798]]    Loss_Validation:  [[ 11.11133925]]\n",
      "Loop  1586 :    Loss_Train:  [[ 12.81953422]]    Loss_Validation:  [[ 11.11135811]]\n",
      "Loop  1587 :    Loss_Train:  [[ 12.8193608]]    Loss_Validation:  [[ 11.11137707]]\n",
      "Loop  1588 :    Loss_Train:  [[ 12.81918772]]    Loss_Validation:  [[ 11.11139611]]\n",
      "Loop  1589 :    Loss_Train:  [[ 12.81901497]]    Loss_Validation:  [[ 11.11141525]]\n",
      "Loop  1590 :    Loss_Train:  [[ 12.81884256]]    Loss_Validation:  [[ 11.11143447]]\n",
      "Loop  1591 :    Loss_Train:  [[ 12.81867048]]    Loss_Validation:  [[ 11.11145378]]\n",
      "Loop  1592 :    Loss_Train:  [[ 12.81849873]]    Loss_Validation:  [[ 11.11147318]]\n",
      "Loop  1593 :    Loss_Train:  [[ 12.81832732]]    Loss_Validation:  [[ 11.11149267]]\n",
      "Loop  1594 :    Loss_Train:  [[ 12.81815624]]    Loss_Validation:  [[ 11.11151225]]\n",
      "Loop  1595 :    Loss_Train:  [[ 12.8179855]]    Loss_Validation:  [[ 11.11153191]]\n",
      "Loop  1596 :    Loss_Train:  [[ 12.81781508]]    Loss_Validation:  [[ 11.11155166]]\n",
      "Loop  1597 :    Loss_Train:  [[ 12.81764499]]    Loss_Validation:  [[ 11.1115715]]\n",
      "Loop  1598 :    Loss_Train:  [[ 12.81747524]]    Loss_Validation:  [[ 11.11159142]]\n",
      "Loop  1599 :    Loss_Train:  [[ 12.81730581]]    Loss_Validation:  [[ 11.11161143]]\n",
      "Loop  1600 :    Loss_Train:  [[ 12.81713671]]    Loss_Validation:  [[ 11.11163152]]\n",
      "Loop  1601 :    Loss_Train:  [[ 12.81696794]]    Loss_Validation:  [[ 11.11165169]]\n",
      "Loop  1602 :    Loss_Train:  [[ 12.81679949]]    Loss_Validation:  [[ 11.11167196]]\n",
      "Loop  1603 :    Loss_Train:  [[ 12.81663137]]    Loss_Validation:  [[ 11.1116923]]\n",
      "Loop  1604 :    Loss_Train:  [[ 12.81646357]]    Loss_Validation:  [[ 11.11171273]]\n",
      "Loop  1605 :    Loss_Train:  [[ 12.8162961]]    Loss_Validation:  [[ 11.11173324]]\n",
      "Loop  1606 :    Loss_Train:  [[ 12.81612896]]    Loss_Validation:  [[ 11.11175383]]\n",
      "Loop  1607 :    Loss_Train:  [[ 12.81596213]]    Loss_Validation:  [[ 11.11177451]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1608 :    Loss_Train:  [[ 12.81579563]]    Loss_Validation:  [[ 11.11179526]]\n",
      "Loop  1609 :    Loss_Train:  [[ 12.81562945]]    Loss_Validation:  [[ 11.1118161]]\n",
      "Loop  1610 :    Loss_Train:  [[ 12.8154636]]    Loss_Validation:  [[ 11.11183702]]\n",
      "Loop  1611 :    Loss_Train:  [[ 12.81529806]]    Loss_Validation:  [[ 11.11185802]]\n",
      "Loop  1612 :    Loss_Train:  [[ 12.81513284]]    Loss_Validation:  [[ 11.1118791]]\n",
      "Loop  1613 :    Loss_Train:  [[ 12.81496794]]    Loss_Validation:  [[ 11.11190026]]\n",
      "Loop  1614 :    Loss_Train:  [[ 12.81480336]]    Loss_Validation:  [[ 11.11192149]]\n",
      "Loop  1615 :    Loss_Train:  [[ 12.8146391]]    Loss_Validation:  [[ 11.11194281]]\n",
      "Loop  1616 :    Loss_Train:  [[ 12.81447515]]    Loss_Validation:  [[ 11.11196421]]\n",
      "Loop  1617 :    Loss_Train:  [[ 12.81431153]]    Loss_Validation:  [[ 11.11198568]]\n",
      "Loop  1618 :    Loss_Train:  [[ 12.81414821]]    Loss_Validation:  [[ 11.11200723]]\n",
      "Loop  1619 :    Loss_Train:  [[ 12.81398521]]    Loss_Validation:  [[ 11.11202886]]\n",
      "Loop  1620 :    Loss_Train:  [[ 12.81382253]]    Loss_Validation:  [[ 11.11205056]]\n",
      "Loop  1621 :    Loss_Train:  [[ 12.81366016]]    Loss_Validation:  [[ 11.11207234]]\n",
      "Loop  1622 :    Loss_Train:  [[ 12.8134981]]    Loss_Validation:  [[ 11.1120942]]\n",
      "Loop  1623 :    Loss_Train:  [[ 12.81333636]]    Loss_Validation:  [[ 11.11211613]]\n",
      "Loop  1624 :    Loss_Train:  [[ 12.81317492]]    Loss_Validation:  [[ 11.11213814]]\n",
      "Loop  1625 :    Loss_Train:  [[ 12.8130138]]    Loss_Validation:  [[ 11.11216022]]\n",
      "Loop  1626 :    Loss_Train:  [[ 12.81285299]]    Loss_Validation:  [[ 11.11218238]]\n",
      "Loop  1627 :    Loss_Train:  [[ 12.81269249]]    Loss_Validation:  [[ 11.11220461]]\n",
      "Loop  1628 :    Loss_Train:  [[ 12.81253229]]    Loss_Validation:  [[ 11.11222692]]\n",
      "Loop  1629 :    Loss_Train:  [[ 12.81237241]]    Loss_Validation:  [[ 11.1122493]]\n",
      "Loop  1630 :    Loss_Train:  [[ 12.81221283]]    Loss_Validation:  [[ 11.11227175]]\n",
      "Loop  1631 :    Loss_Train:  [[ 12.81205356]]    Loss_Validation:  [[ 11.11229427]]\n",
      "Loop  1632 :    Loss_Train:  [[ 12.8118946]]    Loss_Validation:  [[ 11.11231687]]\n",
      "Loop  1633 :    Loss_Train:  [[ 12.81173594]]    Loss_Validation:  [[ 11.11233954]]\n",
      "Loop  1634 :    Loss_Train:  [[ 12.81157759]]    Loss_Validation:  [[ 11.11236228]]\n",
      "Loop  1635 :    Loss_Train:  [[ 12.81141954]]    Loss_Validation:  [[ 11.11238509]]\n",
      "Loop  1636 :    Loss_Train:  [[ 12.81126179]]    Loss_Validation:  [[ 11.11240797]]\n",
      "Loop  1637 :    Loss_Train:  [[ 12.81110435]]    Loss_Validation:  [[ 11.11243092]]\n",
      "Loop  1638 :    Loss_Train:  [[ 12.81094721]]    Loss_Validation:  [[ 11.11245395]]\n",
      "Loop  1639 :    Loss_Train:  [[ 12.81079038]]    Loss_Validation:  [[ 11.11247704]]\n",
      "Loop  1640 :    Loss_Train:  [[ 12.81063384]]    Loss_Validation:  [[ 11.1125002]]\n",
      "Loop  1641 :    Loss_Train:  [[ 12.81047761]]    Loss_Validation:  [[ 11.11252343]]\n",
      "Loop  1642 :    Loss_Train:  [[ 12.81032167]]    Loss_Validation:  [[ 11.11254673]]\n",
      "Loop  1643 :    Loss_Train:  [[ 12.81016604]]    Loss_Validation:  [[ 11.1125701]]\n",
      "Loop  1644 :    Loss_Train:  [[ 12.8100107]]    Loss_Validation:  [[ 11.11259353]]\n",
      "Loop  1645 :    Loss_Train:  [[ 12.80985566]]    Loss_Validation:  [[ 11.11261704]]\n",
      "Loop  1646 :    Loss_Train:  [[ 12.80970092]]    Loss_Validation:  [[ 11.11264061]]\n",
      "Loop  1647 :    Loss_Train:  [[ 12.80954648]]    Loss_Validation:  [[ 11.11266425]]\n",
      "Loop  1648 :    Loss_Train:  [[ 12.80939233]]    Loss_Validation:  [[ 11.11268795]]\n",
      "Loop  1649 :    Loss_Train:  [[ 12.80923848]]    Loss_Validation:  [[ 11.11271172]]\n",
      "Loop  1650 :    Loss_Train:  [[ 12.80908492]]    Loss_Validation:  [[ 11.11273556]]\n",
      "Loop  1651 :    Loss_Train:  [[ 12.80893166]]    Loss_Validation:  [[ 11.11275946]]\n",
      "Loop  1652 :    Loss_Train:  [[ 12.80877869]]    Loss_Validation:  [[ 11.11278342]]\n",
      "Loop  1653 :    Loss_Train:  [[ 12.80862601]]    Loss_Validation:  [[ 11.11280746]]\n",
      "Loop  1654 :    Loss_Train:  [[ 12.80847363]]    Loss_Validation:  [[ 11.11283155]]\n",
      "Loop  1655 :    Loss_Train:  [[ 12.80832154]]    Loss_Validation:  [[ 11.11285571]]\n",
      "Loop  1656 :    Loss_Train:  [[ 12.80816974]]    Loss_Validation:  [[ 11.11287994]]\n",
      "Loop  1657 :    Loss_Train:  [[ 12.80801823]]    Loss_Validation:  [[ 11.11290422]]\n",
      "Loop  1658 :    Loss_Train:  [[ 12.80786701]]    Loss_Validation:  [[ 11.11292857]]\n",
      "Loop  1659 :    Loss_Train:  [[ 12.80771608]]    Loss_Validation:  [[ 11.11295299]]\n",
      "Loop  1660 :    Loss_Train:  [[ 12.80756544]]    Loss_Validation:  [[ 11.11297746]]\n",
      "Loop  1661 :    Loss_Train:  [[ 12.80741509]]    Loss_Validation:  [[ 11.113002]]\n",
      "Loop  1662 :    Loss_Train:  [[ 12.80726502]]    Loss_Validation:  [[ 11.1130266]]\n",
      "Loop  1663 :    Loss_Train:  [[ 12.80711525]]    Loss_Validation:  [[ 11.11305126]]\n",
      "Loop  1664 :    Loss_Train:  [[ 12.80696575]]    Loss_Validation:  [[ 11.11307599]]\n",
      "Loop  1665 :    Loss_Train:  [[ 12.80681655]]    Loss_Validation:  [[ 11.11310077]]\n",
      "Loop  1666 :    Loss_Train:  [[ 12.80666763]]    Loss_Validation:  [[ 11.11312561]]\n",
      "Loop  1667 :    Loss_Train:  [[ 12.80651899]]    Loss_Validation:  [[ 11.11315052]]\n",
      "Loop  1668 :    Loss_Train:  [[ 12.80637064]]    Loss_Validation:  [[ 11.11317548]]\n",
      "Loop  1669 :    Loss_Train:  [[ 12.80622257]]    Loss_Validation:  [[ 11.11320051]]\n",
      "Loop  1670 :    Loss_Train:  [[ 12.80607479]]    Loss_Validation:  [[ 11.11322559]]\n",
      "Loop  1671 :    Loss_Train:  [[ 12.80592728]]    Loss_Validation:  [[ 11.11325073]]\n",
      "Loop  1672 :    Loss_Train:  [[ 12.80578006]]    Loss_Validation:  [[ 11.11327593]]\n",
      "Loop  1673 :    Loss_Train:  [[ 12.80563312]]    Loss_Validation:  [[ 11.11330119]]\n",
      "Loop  1674 :    Loss_Train:  [[ 12.80548646]]    Loss_Validation:  [[ 11.11332651]]\n",
      "Loop  1675 :    Loss_Train:  [[ 12.80534008]]    Loss_Validation:  [[ 11.11335189]]\n",
      "Loop  1676 :    Loss_Train:  [[ 12.80519398]]    Loss_Validation:  [[ 11.11337732]]\n",
      "Loop  1677 :    Loss_Train:  [[ 12.80504816]]    Loss_Validation:  [[ 11.11340281]]\n",
      "Loop  1678 :    Loss_Train:  [[ 12.80490262]]    Loss_Validation:  [[ 11.11342836]]\n",
      "Loop  1679 :    Loss_Train:  [[ 12.80475735]]    Loss_Validation:  [[ 11.11345396]]\n",
      "Loop  1680 :    Loss_Train:  [[ 12.80461236]]    Loss_Validation:  [[ 11.11347962]]\n",
      "Loop  1681 :    Loss_Train:  [[ 12.80446765]]    Loss_Validation:  [[ 11.11350533]]\n",
      "Loop  1682 :    Loss_Train:  [[ 12.80432321]]    Loss_Validation:  [[ 11.1135311]]\n",
      "Loop  1683 :    Loss_Train:  [[ 12.80417905]]    Loss_Validation:  [[ 11.11355693]]\n",
      "Loop  1684 :    Loss_Train:  [[ 12.80403517]]    Loss_Validation:  [[ 11.11358281]]\n",
      "Loop  1685 :    Loss_Train:  [[ 12.80389155]]    Loss_Validation:  [[ 11.11360875]]\n",
      "Loop  1686 :    Loss_Train:  [[ 12.80374821]]    Loss_Validation:  [[ 11.11363474]]\n",
      "Loop  1687 :    Loss_Train:  [[ 12.80360515]]    Loss_Validation:  [[ 11.11366078]]\n",
      "Loop  1688 :    Loss_Train:  [[ 12.80346236]]    Loss_Validation:  [[ 11.11368688]]\n",
      "Loop  1689 :    Loss_Train:  [[ 12.80331983]]    Loss_Validation:  [[ 11.11371303]]\n",
      "Loop  1690 :    Loss_Train:  [[ 12.80317758]]    Loss_Validation:  [[ 11.11373924]]\n",
      "Loop  1691 :    Loss_Train:  [[ 12.8030356]]    Loss_Validation:  [[ 11.1137655]]\n",
      "Loop  1692 :    Loss_Train:  [[ 12.80289389]]    Loss_Validation:  [[ 11.11379181]]\n",
      "Loop  1693 :    Loss_Train:  [[ 12.80275246]]    Loss_Validation:  [[ 11.11381817]]\n",
      "Loop  1694 :    Loss_Train:  [[ 12.80261128]]    Loss_Validation:  [[ 11.11384459]]\n",
      "Loop  1695 :    Loss_Train:  [[ 12.80247038]]    Loss_Validation:  [[ 11.11387105]]\n",
      "Loop  1696 :    Loss_Train:  [[ 12.80232975]]    Loss_Validation:  [[ 11.11389757]]\n",
      "Loop  1697 :    Loss_Train:  [[ 12.80218938]]    Loss_Validation:  [[ 11.11392414]]\n",
      "Loop  1698 :    Loss_Train:  [[ 12.80204928]]    Loss_Validation:  [[ 11.11395076]]\n",
      "Loop  1699 :    Loss_Train:  [[ 12.80190945]]    Loss_Validation:  [[ 11.11397743]]\n",
      "Loop  1700 :    Loss_Train:  [[ 12.80176988]]    Loss_Validation:  [[ 11.11400416]]\n",
      "Loop  1701 :    Loss_Train:  [[ 12.80163058]]    Loss_Validation:  [[ 11.11403093]]\n",
      "Loop  1702 :    Loss_Train:  [[ 12.80149154]]    Loss_Validation:  [[ 11.11405775]]\n",
      "Loop  1703 :    Loss_Train:  [[ 12.80135277]]    Loss_Validation:  [[ 11.11408462]]\n",
      "Loop  1704 :    Loss_Train:  [[ 12.80121425]]    Loss_Validation:  [[ 11.11411154]]\n",
      "Loop  1705 :    Loss_Train:  [[ 12.80107601]]    Loss_Validation:  [[ 11.11413851]]\n",
      "Loop  1706 :    Loss_Train:  [[ 12.80093802]]    Loss_Validation:  [[ 11.11416553]]\n",
      "Loop  1707 :    Loss_Train:  [[ 12.8008003]]    Loss_Validation:  [[ 11.1141926]]\n",
      "Loop  1708 :    Loss_Train:  [[ 12.80066284]]    Loss_Validation:  [[ 11.11421972]]\n",
      "Loop  1709 :    Loss_Train:  [[ 12.80052564]]    Loss_Validation:  [[ 11.11424688]]\n",
      "Loop  1710 :    Loss_Train:  [[ 12.8003887]]    Loss_Validation:  [[ 11.11427409]]\n",
      "Loop  1711 :    Loss_Train:  [[ 12.80025201]]    Loss_Validation:  [[ 11.11430135]]\n",
      "Loop  1712 :    Loss_Train:  [[ 12.80011559]]    Loss_Validation:  [[ 11.11432866]]\n",
      "Loop  1713 :    Loss_Train:  [[ 12.79997943]]    Loss_Validation:  [[ 11.11435601]]\n",
      "Loop  1714 :    Loss_Train:  [[ 12.79984352]]    Loss_Validation:  [[ 11.11438341]]\n",
      "Loop  1715 :    Loss_Train:  [[ 12.79970788]]    Loss_Validation:  [[ 11.11441085]]\n",
      "Loop  1716 :    Loss_Train:  [[ 12.79957249]]    Loss_Validation:  [[ 11.11443835]]\n",
      "Loop  1717 :    Loss_Train:  [[ 12.79943735]]    Loss_Validation:  [[ 11.11446588]]\n",
      "Loop  1718 :    Loss_Train:  [[ 12.79930248]]    Loss_Validation:  [[ 11.11449347]]\n",
      "Loop  1719 :    Loss_Train:  [[ 12.79916785]]    Loss_Validation:  [[ 11.1145211]]\n",
      "Loop  1720 :    Loss_Train:  [[ 12.79903349]]    Loss_Validation:  [[ 11.11454877]]\n",
      "Loop  1721 :    Loss_Train:  [[ 12.79889937]]    Loss_Validation:  [[ 11.11457649]]\n",
      "Loop  1722 :    Loss_Train:  [[ 12.79876551]]    Loss_Validation:  [[ 11.11460425]]\n",
      "Loop  1723 :    Loss_Train:  [[ 12.79863191]]    Loss_Validation:  [[ 11.11463206]]\n",
      "Loop  1724 :    Loss_Train:  [[ 12.79849856]]    Loss_Validation:  [[ 11.11465991]]\n",
      "Loop  1725 :    Loss_Train:  [[ 12.79836546]]    Loss_Validation:  [[ 11.11468781]]\n",
      "Loop  1726 :    Loss_Train:  [[ 12.79823261]]    Loss_Validation:  [[ 11.11471575]]\n",
      "Loop  1727 :    Loss_Train:  [[ 12.79810001]]    Loss_Validation:  [[ 11.11474373]]\n",
      "Loop  1728 :    Loss_Train:  [[ 12.79796766]]    Loss_Validation:  [[ 11.11477176]]\n",
      "Loop  1729 :    Loss_Train:  [[ 12.79783557]]    Loss_Validation:  [[ 11.11479983]]\n",
      "Loop  1730 :    Loss_Train:  [[ 12.79770372]]    Loss_Validation:  [[ 11.11482794]]\n",
      "Loop  1731 :    Loss_Train:  [[ 12.79757212]]    Loss_Validation:  [[ 11.11485609]]\n",
      "Loop  1732 :    Loss_Train:  [[ 12.79744078]]    Loss_Validation:  [[ 11.11488429]]\n",
      "Loop  1733 :    Loss_Train:  [[ 12.79730968]]    Loss_Validation:  [[ 11.11491253]]\n",
      "Loop  1734 :    Loss_Train:  [[ 12.79717882]]    Loss_Validation:  [[ 11.1149408]]\n",
      "Loop  1735 :    Loss_Train:  [[ 12.79704822]]    Loss_Validation:  [[ 11.11496913]]\n",
      "Loop  1736 :    Loss_Train:  [[ 12.79691786]]    Loss_Validation:  [[ 11.11499749]]\n",
      "Loop  1737 :    Loss_Train:  [[ 12.79678775]]    Loss_Validation:  [[ 11.11502589]]\n",
      "Loop  1738 :    Loss_Train:  [[ 12.79665788]]    Loss_Validation:  [[ 11.11505433]]\n",
      "Loop  1739 :    Loss_Train:  [[ 12.79652826]]    Loss_Validation:  [[ 11.11508282]]\n",
      "Loop  1740 :    Loss_Train:  [[ 12.79639888]]    Loss_Validation:  [[ 11.11511134]]\n",
      "Loop  1741 :    Loss_Train:  [[ 12.79626975]]    Loss_Validation:  [[ 11.11513991]]\n",
      "Loop  1742 :    Loss_Train:  [[ 12.79614086]]    Loss_Validation:  [[ 11.11516851]]\n",
      "Loop  1743 :    Loss_Train:  [[ 12.79601222]]    Loss_Validation:  [[ 11.11519716]]\n",
      "Loop  1744 :    Loss_Train:  [[ 12.79588381]]    Loss_Validation:  [[ 11.11522584]]\n",
      "Loop  1745 :    Loss_Train:  [[ 12.79575565]]    Loss_Validation:  [[ 11.11525456]]\n",
      "Loop  1746 :    Loss_Train:  [[ 12.79562773]]    Loss_Validation:  [[ 11.11528332]]\n",
      "Loop  1747 :    Loss_Train:  [[ 12.79550005]]    Loss_Validation:  [[ 11.11531212]]\n",
      "Loop  1748 :    Loss_Train:  [[ 12.79537262]]    Loss_Validation:  [[ 11.11534096]]\n",
      "Loop  1749 :    Loss_Train:  [[ 12.79524542]]    Loss_Validation:  [[ 11.11536983]]\n",
      "Loop  1750 :    Loss_Train:  [[ 12.79511846]]    Loss_Validation:  [[ 11.11539875]]\n",
      "Loop  1751 :    Loss_Train:  [[ 12.79499174]]    Loss_Validation:  [[ 11.1154277]]\n",
      "Loop  1752 :    Loss_Train:  [[ 12.79486526]]    Loss_Validation:  [[ 11.11545669]]\n",
      "Loop  1753 :    Loss_Train:  [[ 12.79473902]]    Loss_Validation:  [[ 11.11548572]]\n",
      "Loop  1754 :    Loss_Train:  [[ 12.79461302]]    Loss_Validation:  [[ 11.11551478]]\n",
      "Loop  1755 :    Loss_Train:  [[ 12.79448725]]    Loss_Validation:  [[ 11.11554388]]\n",
      "Loop  1756 :    Loss_Train:  [[ 12.79436172]]    Loss_Validation:  [[ 11.11557302]]\n",
      "Loop  1757 :    Loss_Train:  [[ 12.79423643]]    Loss_Validation:  [[ 11.11560219]]\n",
      "Loop  1758 :    Loss_Train:  [[ 12.79411137]]    Loss_Validation:  [[ 11.1156314]]\n",
      "Loop  1759 :    Loss_Train:  [[ 12.79398655]]    Loss_Validation:  [[ 11.11566064]]\n",
      "Loop  1760 :    Loss_Train:  [[ 12.79386197]]    Loss_Validation:  [[ 11.11568992]]\n",
      "Loop  1761 :    Loss_Train:  [[ 12.79373761]]    Loss_Validation:  [[ 11.11571924]]\n",
      "Loop  1762 :    Loss_Train:  [[ 12.79361349]]    Loss_Validation:  [[ 11.11574859]]\n",
      "Loop  1763 :    Loss_Train:  [[ 12.79348961]]    Loss_Validation:  [[ 11.11577798]]\n",
      "Loop  1764 :    Loss_Train:  [[ 12.79336596]]    Loss_Validation:  [[ 11.1158074]]\n",
      "Loop  1765 :    Loss_Train:  [[ 12.79324254]]    Loss_Validation:  [[ 11.11583686]]\n",
      "Loop  1766 :    Loss_Train:  [[ 12.79311935]]    Loss_Validation:  [[ 11.11586635]]\n",
      "Loop  1767 :    Loss_Train:  [[ 12.79299639]]    Loss_Validation:  [[ 11.11589587]]\n",
      "Loop  1768 :    Loss_Train:  [[ 12.79287367]]    Loss_Validation:  [[ 11.11592543]]\n",
      "Loop  1769 :    Loss_Train:  [[ 12.79275117]]    Loss_Validation:  [[ 11.11595503]]\n",
      "Loop  1770 :    Loss_Train:  [[ 12.79262891]]    Loss_Validation:  [[ 11.11598465]]\n",
      "Loop  1771 :    Loss_Train:  [[ 12.79250688]]    Loss_Validation:  [[ 11.11601431]]\n",
      "Loop  1772 :    Loss_Train:  [[ 12.79238507]]    Loss_Validation:  [[ 11.11604401]]\n",
      "Loop  1773 :    Loss_Train:  [[ 12.79226349]]    Loss_Validation:  [[ 11.11607373]]\n",
      "Loop  1774 :    Loss_Train:  [[ 12.79214214]]    Loss_Validation:  [[ 11.11610349]]\n",
      "Loop  1775 :    Loss_Train:  [[ 12.79202102]]    Loss_Validation:  [[ 11.11613328]]\n",
      "Loop  1776 :    Loss_Train:  [[ 12.79190013]]    Loss_Validation:  [[ 11.11616311]]\n",
      "Loop  1777 :    Loss_Train:  [[ 12.79177946]]    Loss_Validation:  [[ 11.11619296]]\n",
      "Loop  1778 :    Loss_Train:  [[ 12.79165902]]    Loss_Validation:  [[ 11.11622285]]\n",
      "Loop  1779 :    Loss_Train:  [[ 12.79153881]]    Loss_Validation:  [[ 11.11625277]]\n",
      "Loop  1780 :    Loss_Train:  [[ 12.79141882]]    Loss_Validation:  [[ 11.11628272]]\n",
      "Loop  1781 :    Loss_Train:  [[ 12.79129906]]    Loss_Validation:  [[ 11.1163127]]\n",
      "Loop  1782 :    Loss_Train:  [[ 12.79117952]]    Loss_Validation:  [[ 11.11634272]]\n",
      "Loop  1783 :    Loss_Train:  [[ 12.7910602]]    Loss_Validation:  [[ 11.11637276]]\n",
      "Loop  1784 :    Loss_Train:  [[ 12.79094111]]    Loss_Validation:  [[ 11.11640284]]\n",
      "Loop  1785 :    Loss_Train:  [[ 12.79082225]]    Loss_Validation:  [[ 11.11643295]]\n",
      "Loop  1786 :    Loss_Train:  [[ 12.7907036]]    Loss_Validation:  [[ 11.11646308]]\n",
      "Loop  1787 :    Loss_Train:  [[ 12.79058518]]    Loss_Validation:  [[ 11.11649325]]\n",
      "Loop  1788 :    Loss_Train:  [[ 12.79046698]]    Loss_Validation:  [[ 11.11652345]]\n",
      "Loop  1789 :    Loss_Train:  [[ 12.790349]]    Loss_Validation:  [[ 11.11655367]]\n",
      "Loop  1790 :    Loss_Train:  [[ 12.79023124]]    Loss_Validation:  [[ 11.11658393]]\n",
      "Loop  1791 :    Loss_Train:  [[ 12.7901137]]    Loss_Validation:  [[ 11.11661422]]\n",
      "Loop  1792 :    Loss_Train:  [[ 12.78999638]]    Loss_Validation:  [[ 11.11664453]]\n",
      "Loop  1793 :    Loss_Train:  [[ 12.78987928]]    Loss_Validation:  [[ 11.11667488]]\n",
      "Loop  1794 :    Loss_Train:  [[ 12.78976241]]    Loss_Validation:  [[ 11.11670525]]\n",
      "Loop  1795 :    Loss_Train:  [[ 12.78964574]]    Loss_Validation:  [[ 11.11673565]]\n",
      "Loop  1796 :    Loss_Train:  [[ 12.7895293]]    Loss_Validation:  [[ 11.11676608]]\n",
      "Loop  1797 :    Loss_Train:  [[ 12.78941308]]    Loss_Validation:  [[ 11.11679654]]\n",
      "Loop  1798 :    Loss_Train:  [[ 12.78929707]]    Loss_Validation:  [[ 11.11682703]]\n",
      "Loop  1799 :    Loss_Train:  [[ 12.78918128]]    Loss_Validation:  [[ 11.11685754]]\n",
      "Loop  1800 :    Loss_Train:  [[ 12.78906571]]    Loss_Validation:  [[ 11.11688809]]\n",
      "Loop  1801 :    Loss_Train:  [[ 12.78895035]]    Loss_Validation:  [[ 11.11691866]]\n",
      "Loop  1802 :    Loss_Train:  [[ 12.78883521]]    Loss_Validation:  [[ 11.11694926]]\n",
      "Loop  1803 :    Loss_Train:  [[ 12.78872028]]    Loss_Validation:  [[ 11.11697988]]\n",
      "Loop  1804 :    Loss_Train:  [[ 12.78860557]]    Loss_Validation:  [[ 11.11701053]]\n",
      "Loop  1805 :    Loss_Train:  [[ 12.78849108]]    Loss_Validation:  [[ 11.11704121]]\n",
      "Loop  1806 :    Loss_Train:  [[ 12.78837679]]    Loss_Validation:  [[ 11.11707192]]\n",
      "Loop  1807 :    Loss_Train:  [[ 12.78826272]]    Loss_Validation:  [[ 11.11710265]]\n",
      "Loop  1808 :    Loss_Train:  [[ 12.78814887]]    Loss_Validation:  [[ 11.11713341]]\n",
      "Loop  1809 :    Loss_Train:  [[ 12.78803522]]    Loss_Validation:  [[ 11.11716419]]\n",
      "Loop  1810 :    Loss_Train:  [[ 12.78792179]]    Loss_Validation:  [[ 11.117195]]\n",
      "Loop  1811 :    Loss_Train:  [[ 12.78780857]]    Loss_Validation:  [[ 11.11722584]]\n",
      "Loop  1812 :    Loss_Train:  [[ 12.78769556]]    Loss_Validation:  [[ 11.1172567]]\n",
      "Loop  1813 :    Loss_Train:  [[ 12.78758277]]    Loss_Validation:  [[ 11.11728759]]\n",
      "Loop  1814 :    Loss_Train:  [[ 12.78747018]]    Loss_Validation:  [[ 11.1173185]]\n",
      "Loop  1815 :    Loss_Train:  [[ 12.7873578]]    Loss_Validation:  [[ 11.11734944]]\n",
      "Loop  1816 :    Loss_Train:  [[ 12.78724564]]    Loss_Validation:  [[ 11.1173804]]\n",
      "Loop  1817 :    Loss_Train:  [[ 12.78713368]]    Loss_Validation:  [[ 11.11741139]]\n",
      "Loop  1818 :    Loss_Train:  [[ 12.78702193]]    Loss_Validation:  [[ 11.11744241]]\n",
      "Loop  1819 :    Loss_Train:  [[ 12.78691039]]    Loss_Validation:  [[ 11.11747344]]\n",
      "Loop  1820 :    Loss_Train:  [[ 12.78679906]]    Loss_Validation:  [[ 11.1175045]]\n",
      "Loop  1821 :    Loss_Train:  [[ 12.78668793]]    Loss_Validation:  [[ 11.11753559]]\n",
      "Loop  1822 :    Loss_Train:  [[ 12.78657701]]    Loss_Validation:  [[ 11.1175667]]\n",
      "Loop  1823 :    Loss_Train:  [[ 12.7864663]]    Loss_Validation:  [[ 11.11759783]]\n",
      "Loop  1824 :    Loss_Train:  [[ 12.7863558]]    Loss_Validation:  [[ 11.11762899]]\n",
      "Loop  1825 :    Loss_Train:  [[ 12.7862455]]    Loss_Validation:  [[ 11.11766017]]\n",
      "Loop  1826 :    Loss_Train:  [[ 12.78613541]]    Loss_Validation:  [[ 11.11769137]]\n",
      "Loop  1827 :    Loss_Train:  [[ 12.78602552]]    Loss_Validation:  [[ 11.1177226]]\n",
      "Loop  1828 :    Loss_Train:  [[ 12.78591583]]    Loss_Validation:  [[ 11.11775385]]\n",
      "Loop  1829 :    Loss_Train:  [[ 12.78580636]]    Loss_Validation:  [[ 11.11778512]]\n",
      "Loop  1830 :    Loss_Train:  [[ 12.78569708]]    Loss_Validation:  [[ 11.11781642]]\n",
      "Loop  1831 :    Loss_Train:  [[ 12.78558801]]    Loss_Validation:  [[ 11.11784773]]\n",
      "Loop  1832 :    Loss_Train:  [[ 12.78547914]]    Loss_Validation:  [[ 11.11787907]]\n",
      "Loop  1833 :    Loss_Train:  [[ 12.78537047]]    Loss_Validation:  [[ 11.11791044]]\n",
      "Loop  1834 :    Loss_Train:  [[ 12.78526201]]    Loss_Validation:  [[ 11.11794182]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  1835 :    Loss_Train:  [[ 12.78515375]]    Loss_Validation:  [[ 11.11797323]]\n",
      "Loop  1836 :    Loss_Train:  [[ 12.78504569]]    Loss_Validation:  [[ 11.11800465]]\n",
      "Loop  1837 :    Loss_Train:  [[ 12.78493783]]    Loss_Validation:  [[ 11.1180361]]\n",
      "Loop  1838 :    Loss_Train:  [[ 12.78483017]]    Loss_Validation:  [[ 11.11806757]]\n",
      "Loop  1839 :    Loss_Train:  [[ 12.78472271]]    Loss_Validation:  [[ 11.11809906]]\n",
      "Loop  1840 :    Loss_Train:  [[ 12.78461545]]    Loss_Validation:  [[ 11.11813058]]\n",
      "Loop  1841 :    Loss_Train:  [[ 12.78450839]]    Loss_Validation:  [[ 11.11816211]]\n",
      "Loop  1842 :    Loss_Train:  [[ 12.78440153]]    Loss_Validation:  [[ 11.11819366]]\n",
      "Loop  1843 :    Loss_Train:  [[ 12.78429487]]    Loss_Validation:  [[ 11.11822524]]\n",
      "Loop  1844 :    Loss_Train:  [[ 12.7841884]]    Loss_Validation:  [[ 11.11825684]]\n",
      "Loop  1845 :    Loss_Train:  [[ 12.78408214]]    Loss_Validation:  [[ 11.11828845]]\n",
      "Loop  1846 :    Loss_Train:  [[ 12.78397607]]    Loss_Validation:  [[ 11.11832009]]\n",
      "Loop  1847 :    Loss_Train:  [[ 12.7838702]]    Loss_Validation:  [[ 11.11835174]]\n",
      "Loop  1848 :    Loss_Train:  [[ 12.78376452]]    Loss_Validation:  [[ 11.11838342]]\n",
      "Loop  1849 :    Loss_Train:  [[ 12.78365905]]    Loss_Validation:  [[ 11.11841511]]\n",
      "Loop  1850 :    Loss_Train:  [[ 12.78355376]]    Loss_Validation:  [[ 11.11844683]]\n",
      "Loop  1851 :    Loss_Train:  [[ 12.78344868]]    Loss_Validation:  [[ 11.11847856]]\n",
      "Loop  1852 :    Loss_Train:  [[ 12.78334378]]    Loss_Validation:  [[ 11.11851032]]\n",
      "Loop  1853 :    Loss_Train:  [[ 12.78323909]]    Loss_Validation:  [[ 11.11854209]]\n",
      "Loop  1854 :    Loss_Train:  [[ 12.78313458]]    Loss_Validation:  [[ 11.11857388]]\n",
      "Loop  1855 :    Loss_Train:  [[ 12.78303028]]    Loss_Validation:  [[ 11.11860569]]\n",
      "Loop  1856 :    Loss_Train:  [[ 12.78292616]]    Loss_Validation:  [[ 11.11863752]]\n",
      "Loop  1857 :    Loss_Train:  [[ 12.78282224]]    Loss_Validation:  [[ 11.11866937]]\n",
      "Loop  1858 :    Loss_Train:  [[ 12.78271851]]    Loss_Validation:  [[ 11.11870124]]\n",
      "Loop  1859 :    Loss_Train:  [[ 12.78261497]]    Loss_Validation:  [[ 11.11873312]]\n",
      "Loop  1860 :    Loss_Train:  [[ 12.78251163]]    Loss_Validation:  [[ 11.11876502]]\n",
      "Loop  1861 :    Loss_Train:  [[ 12.78240847]]    Loss_Validation:  [[ 11.11879694]]\n",
      "Loop  1862 :    Loss_Train:  [[ 12.78230551]]    Loss_Validation:  [[ 11.11882888]]\n",
      "Loop  1863 :    Loss_Train:  [[ 12.78220274]]    Loss_Validation:  [[ 11.11886084]]\n",
      "Loop  1864 :    Loss_Train:  [[ 12.78210016]]    Loss_Validation:  [[ 11.11889281]]\n",
      "Loop  1865 :    Loss_Train:  [[ 12.78199777]]    Loss_Validation:  [[ 11.1189248]]\n",
      "Loop  1866 :    Loss_Train:  [[ 12.78189557]]    Loss_Validation:  [[ 11.11895681]]\n",
      "Loop  1867 :    Loss_Train:  [[ 12.78179355]]    Loss_Validation:  [[ 11.11898883]]\n",
      "Loop  1868 :    Loss_Train:  [[ 12.78169173]]    Loss_Validation:  [[ 11.11902088]]\n",
      "Loop  1869 :    Loss_Train:  [[ 12.7815901]]    Loss_Validation:  [[ 11.11905294]]\n",
      "Loop  1870 :    Loss_Train:  [[ 12.78148865]]    Loss_Validation:  [[ 11.11908501]]\n",
      "Loop  1871 :    Loss_Train:  [[ 12.78138739]]    Loss_Validation:  [[ 11.1191171]]\n",
      "Loop  1872 :    Loss_Train:  [[ 12.78128632]]    Loss_Validation:  [[ 11.11914921]]\n",
      "Loop  1873 :    Loss_Train:  [[ 12.78118544]]    Loss_Validation:  [[ 11.11918134]]\n",
      "Loop  1874 :    Loss_Train:  [[ 12.78108474]]    Loss_Validation:  [[ 11.11921348]]\n",
      "Loop  1875 :    Loss_Train:  [[ 12.78098423]]    Loss_Validation:  [[ 11.11924564]]\n",
      "Loop  1876 :    Loss_Train:  [[ 12.78088391]]    Loss_Validation:  [[ 11.11927781]]\n",
      "Loop  1877 :    Loss_Train:  [[ 12.78078377]]    Loss_Validation:  [[ 11.11931]]\n",
      "Loop  1878 :    Loss_Train:  [[ 12.78068381]]    Loss_Validation:  [[ 11.1193422]]\n",
      "Loop  1879 :    Loss_Train:  [[ 12.78058405]]    Loss_Validation:  [[ 11.11937442]]\n",
      "Loop  1880 :    Loss_Train:  [[ 12.78048446]]    Loss_Validation:  [[ 11.11940666]]\n",
      "Loop  1881 :    Loss_Train:  [[ 12.78038506]]    Loss_Validation:  [[ 11.11943891]]\n",
      "Loop  1882 :    Loss_Train:  [[ 12.78028585]]    Loss_Validation:  [[ 11.11947117]]\n",
      "Loop  1883 :    Loss_Train:  [[ 12.78018681]]    Loss_Validation:  [[ 11.11950346]]\n",
      "Loop  1884 :    Loss_Train:  [[ 12.78008796]]    Loss_Validation:  [[ 11.11953575]]\n",
      "Loop  1885 :    Loss_Train:  [[ 12.7799893]]    Loss_Validation:  [[ 11.11956806]]\n",
      "Loop  1886 :    Loss_Train:  [[ 12.77989081]]    Loss_Validation:  [[ 11.11960039]]\n",
      "Loop  1887 :    Loss_Train:  [[ 12.77979251]]    Loss_Validation:  [[ 11.11963273]]\n",
      "Loop  1888 :    Loss_Train:  [[ 12.77969439]]    Loss_Validation:  [[ 11.11966508]]\n",
      "Loop  1889 :    Loss_Train:  [[ 12.77959645]]    Loss_Validation:  [[ 11.11969745]]\n",
      "Loop  1890 :    Loss_Train:  [[ 12.77949869]]    Loss_Validation:  [[ 11.11972983]]\n",
      "Loop  1891 :    Loss_Train:  [[ 12.77940111]]    Loss_Validation:  [[ 11.11976223]]\n",
      "Loop  1892 :    Loss_Train:  [[ 12.77930371]]    Loss_Validation:  [[ 11.11979464]]\n",
      "Loop  1893 :    Loss_Train:  [[ 12.7792065]]    Loss_Validation:  [[ 11.11982706]]\n",
      "Loop  1894 :    Loss_Train:  [[ 12.77910946]]    Loss_Validation:  [[ 11.1198595]]\n",
      "Loop  1895 :    Loss_Train:  [[ 12.7790126]]    Loss_Validation:  [[ 11.11989195]]\n",
      "Loop  1896 :    Loss_Train:  [[ 12.77891592]]    Loss_Validation:  [[ 11.11992441]]\n",
      "Loop  1897 :    Loss_Train:  [[ 12.77881941]]    Loss_Validation:  [[ 11.11995689]]\n",
      "Loop  1898 :    Loss_Train:  [[ 12.77872309]]    Loss_Validation:  [[ 11.11998938]]\n",
      "Loop  1899 :    Loss_Train:  [[ 12.77862694]]    Loss_Validation:  [[ 11.12002188]]\n",
      "Loop  1900 :    Loss_Train:  [[ 12.77853097]]    Loss_Validation:  [[ 11.1200544]]\n",
      "Loop  1901 :    Loss_Train:  [[ 12.77843518]]    Loss_Validation:  [[ 11.12008693]]\n",
      "Loop  1902 :    Loss_Train:  [[ 12.77833957]]    Loss_Validation:  [[ 11.12011947]]\n",
      "Loop  1903 :    Loss_Train:  [[ 12.77824413]]    Loss_Validation:  [[ 11.12015202]]\n",
      "Loop  1904 :    Loss_Train:  [[ 12.77814886]]    Loss_Validation:  [[ 11.12018459]]\n",
      "Loop  1905 :    Loss_Train:  [[ 12.77805378]]    Loss_Validation:  [[ 11.12021717]]\n",
      "Loop  1906 :    Loss_Train:  [[ 12.77795886]]    Loss_Validation:  [[ 11.12024976]]\n",
      "Loop  1907 :    Loss_Train:  [[ 12.77786413]]    Loss_Validation:  [[ 11.12028236]]\n",
      "Loop  1908 :    Loss_Train:  [[ 12.77776956]]    Loss_Validation:  [[ 11.12031498]]\n",
      "Loop  1909 :    Loss_Train:  [[ 12.77767517]]    Loss_Validation:  [[ 11.1203476]]\n",
      "Loop  1910 :    Loss_Train:  [[ 12.77758096]]    Loss_Validation:  [[ 11.12038024]]\n",
      "Loop  1911 :    Loss_Train:  [[ 12.77748692]]    Loss_Validation:  [[ 11.12041289]]\n",
      "Loop  1912 :    Loss_Train:  [[ 12.77739305]]    Loss_Validation:  [[ 11.12044555]]\n",
      "Loop  1913 :    Loss_Train:  [[ 12.77729936]]    Loss_Validation:  [[ 11.12047823]]\n",
      "Loop  1914 :    Loss_Train:  [[ 12.77720583]]    Loss_Validation:  [[ 11.12051091]]\n",
      "Loop  1915 :    Loss_Train:  [[ 12.77711248]]    Loss_Validation:  [[ 11.1205436]]\n",
      "Loop  1916 :    Loss_Train:  [[ 12.7770193]]    Loss_Validation:  [[ 11.12057631]]\n",
      "Loop  1917 :    Loss_Train:  [[ 12.7769263]]    Loss_Validation:  [[ 11.12060903]]\n",
      "Loop  1918 :    Loss_Train:  [[ 12.77683346]]    Loss_Validation:  [[ 11.12064176]]\n",
      "Loop  1919 :    Loss_Train:  [[ 12.7767408]]    Loss_Validation:  [[ 11.12067449]]\n",
      "Loop  1920 :    Loss_Train:  [[ 12.7766483]]    Loss_Validation:  [[ 11.12070724]]\n",
      "Loop  1921 :    Loss_Train:  [[ 12.77655598]]    Loss_Validation:  [[ 11.12074]]\n",
      "Loop  1922 :    Loss_Train:  [[ 12.77646382]]    Loss_Validation:  [[ 11.12077277]]\n",
      "Loop  1923 :    Loss_Train:  [[ 12.77637184]]    Loss_Validation:  [[ 11.12080555]]\n",
      "Loop  1924 :    Loss_Train:  [[ 12.77628002]]    Loss_Validation:  [[ 11.12083834]]\n",
      "Loop  1925 :    Loss_Train:  [[ 12.77618838]]    Loss_Validation:  [[ 11.12087114]]\n",
      "Loop  1926 :    Loss_Train:  [[ 12.7760969]]    Loss_Validation:  [[ 11.12090395]]\n",
      "Loop  1927 :    Loss_Train:  [[ 12.77600559]]    Loss_Validation:  [[ 11.12093677]]\n",
      "Loop  1928 :    Loss_Train:  [[ 12.77591445]]    Loss_Validation:  [[ 11.1209696]]\n",
      "Loop  1929 :    Loss_Train:  [[ 12.77582347]]    Loss_Validation:  [[ 11.12100244]]\n",
      "Loop  1930 :    Loss_Train:  [[ 12.77573267]]    Loss_Validation:  [[ 11.12103529]]\n",
      "Loop  1931 :    Loss_Train:  [[ 12.77564202]]    Loss_Validation:  [[ 11.12106815]]\n",
      "Loop  1932 :    Loss_Train:  [[ 12.77555155]]    Loss_Validation:  [[ 11.12110101]]\n",
      "Loop  1933 :    Loss_Train:  [[ 12.77546124]]    Loss_Validation:  [[ 11.12113389]]\n",
      "Loop  1934 :    Loss_Train:  [[ 12.7753711]]    Loss_Validation:  [[ 11.12116677]]\n",
      "Loop  1935 :    Loss_Train:  [[ 12.77528112]]    Loss_Validation:  [[ 11.12119967]]\n",
      "Loop  1936 :    Loss_Train:  [[ 12.77519131]]    Loss_Validation:  [[ 11.12123257]]\n",
      "Loop  1937 :    Loss_Train:  [[ 12.77510167]]    Loss_Validation:  [[ 11.12126548]]\n",
      "Loop  1938 :    Loss_Train:  [[ 12.77501219]]    Loss_Validation:  [[ 11.1212984]]\n",
      "Loop  1939 :    Loss_Train:  [[ 12.77492287]]    Loss_Validation:  [[ 11.12133133]]\n",
      "Loop  1940 :    Loss_Train:  [[ 12.77483372]]    Loss_Validation:  [[ 11.12136427]]\n",
      "Loop  1941 :    Loss_Train:  [[ 12.77474473]]    Loss_Validation:  [[ 11.12139722]]\n",
      "Loop  1942 :    Loss_Train:  [[ 12.7746559]]    Loss_Validation:  [[ 11.12143017]]\n",
      "Loop  1943 :    Loss_Train:  [[ 12.77456724]]    Loss_Validation:  [[ 11.12146313]]\n",
      "Loop  1944 :    Loss_Train:  [[ 12.77447874]]    Loss_Validation:  [[ 11.1214961]]\n",
      "Loop  1945 :    Loss_Train:  [[ 12.7743904]]    Loss_Validation:  [[ 11.12152908]]\n",
      "Loop  1946 :    Loss_Train:  [[ 12.77430222]]    Loss_Validation:  [[ 11.12156206]]\n",
      "Loop  1947 :    Loss_Train:  [[ 12.77421421]]    Loss_Validation:  [[ 11.12159506]]\n",
      "Loop  1948 :    Loss_Train:  [[ 12.77412635]]    Loss_Validation:  [[ 11.12162806]]\n",
      "Loop  1949 :    Loss_Train:  [[ 12.77403866]]    Loss_Validation:  [[ 11.12166107]]\n",
      "Loop  1950 :    Loss_Train:  [[ 12.77395113]]    Loss_Validation:  [[ 11.12169408]]\n",
      "Loop  1951 :    Loss_Train:  [[ 12.77386376]]    Loss_Validation:  [[ 11.12172711]]\n",
      "Loop  1952 :    Loss_Train:  [[ 12.77377655]]    Loss_Validation:  [[ 11.12176014]]\n",
      "Loop  1953 :    Loss_Train:  [[ 12.7736895]]    Loss_Validation:  [[ 11.12179318]]\n",
      "Loop  1954 :    Loss_Train:  [[ 12.77360261]]    Loss_Validation:  [[ 11.12182622]]\n",
      "Loop  1955 :    Loss_Train:  [[ 12.77351587]]    Loss_Validation:  [[ 11.12185927]]\n",
      "Loop  1956 :    Loss_Train:  [[ 12.7734293]]    Loss_Validation:  [[ 11.12189233]]\n",
      "Loop  1957 :    Loss_Train:  [[ 12.77334289]]    Loss_Validation:  [[ 11.1219254]]\n",
      "Loop  1958 :    Loss_Train:  [[ 12.77325663]]    Loss_Validation:  [[ 11.12195847]]\n",
      "Loop  1959 :    Loss_Train:  [[ 12.77317053]]    Loss_Validation:  [[ 11.12199155]]\n",
      "Loop  1960 :    Loss_Train:  [[ 12.77308459]]    Loss_Validation:  [[ 11.12202463]]\n",
      "Loop  1961 :    Loss_Train:  [[ 12.77299881]]    Loss_Validation:  [[ 11.12205772]]\n",
      "Loop  1962 :    Loss_Train:  [[ 12.77291318]]    Loss_Validation:  [[ 11.12209082]]\n",
      "Loop  1963 :    Loss_Train:  [[ 12.77282771]]    Loss_Validation:  [[ 11.12212393]]\n",
      "Loop  1964 :    Loss_Train:  [[ 12.7727424]]    Loss_Validation:  [[ 11.12215704]]\n",
      "Loop  1965 :    Loss_Train:  [[ 12.77265724]]    Loss_Validation:  [[ 11.12219015]]\n",
      "Loop  1966 :    Loss_Train:  [[ 12.77257224]]    Loss_Validation:  [[ 11.12222327]]\n",
      "Loop  1967 :    Loss_Train:  [[ 12.7724874]]    Loss_Validation:  [[ 11.1222564]]\n",
      "Loop  1968 :    Loss_Train:  [[ 12.77240271]]    Loss_Validation:  [[ 11.12228954]]\n",
      "Loop  1969 :    Loss_Train:  [[ 12.77231817]]    Loss_Validation:  [[ 11.12232267]]\n",
      "Loop  1970 :    Loss_Train:  [[ 12.77223379]]    Loss_Validation:  [[ 11.12235582]]\n",
      "Loop  1971 :    Loss_Train:  [[ 12.77214957]]    Loss_Validation:  [[ 11.12238897]]\n",
      "Loop  1972 :    Loss_Train:  [[ 12.77206549]]    Loss_Validation:  [[ 11.12242212]]\n",
      "Loop  1973 :    Loss_Train:  [[ 12.77198157]]    Loss_Validation:  [[ 11.12245529]]\n",
      "Loop  1974 :    Loss_Train:  [[ 12.77189781]]    Loss_Validation:  [[ 11.12248845]]\n",
      "Loop  1975 :    Loss_Train:  [[ 12.7718142]]    Loss_Validation:  [[ 11.12252162]]\n",
      "Loop  1976 :    Loss_Train:  [[ 12.77173074]]    Loss_Validation:  [[ 11.1225548]]\n",
      "Loop  1977 :    Loss_Train:  [[ 12.77164743]]    Loss_Validation:  [[ 11.12258798]]\n",
      "Loop  1978 :    Loss_Train:  [[ 12.77156428]]    Loss_Validation:  [[ 11.12262117]]\n",
      "Loop  1979 :    Loss_Train:  [[ 12.77148128]]    Loss_Validation:  [[ 11.12265436]]\n",
      "Loop  1980 :    Loss_Train:  [[ 12.77139843]]    Loss_Validation:  [[ 11.12268755]]\n",
      "Loop  1981 :    Loss_Train:  [[ 12.77131573]]    Loss_Validation:  [[ 11.12272075]]\n",
      "Loop  1982 :    Loss_Train:  [[ 12.77123318]]    Loss_Validation:  [[ 11.12275396]]\n",
      "Loop  1983 :    Loss_Train:  [[ 12.77115078]]    Loss_Validation:  [[ 11.12278717]]\n",
      "Loop  1984 :    Loss_Train:  [[ 12.77106853]]    Loss_Validation:  [[ 11.12282038]]\n",
      "Loop  1985 :    Loss_Train:  [[ 12.77098644]]    Loss_Validation:  [[ 11.1228536]]\n",
      "Loop  1986 :    Loss_Train:  [[ 12.77090449]]    Loss_Validation:  [[ 11.12288682]]\n",
      "Loop  1987 :    Loss_Train:  [[ 12.77082269]]    Loss_Validation:  [[ 11.12292004]]\n",
      "Loop  1988 :    Loss_Train:  [[ 12.77074104]]    Loss_Validation:  [[ 11.12295327]]\n",
      "Loop  1989 :    Loss_Train:  [[ 12.77065955]]    Loss_Validation:  [[ 11.12298651]]\n",
      "Loop  1990 :    Loss_Train:  [[ 12.7705782]]    Loss_Validation:  [[ 11.12301974]]\n",
      "Loop  1991 :    Loss_Train:  [[ 12.77049699]]    Loss_Validation:  [[ 11.12305299]]\n",
      "Loop  1992 :    Loss_Train:  [[ 12.77041594]]    Loss_Validation:  [[ 11.12308623]]\n",
      "Loop  1993 :    Loss_Train:  [[ 12.77033503]]    Loss_Validation:  [[ 11.12311948]]\n",
      "Loop  1994 :    Loss_Train:  [[ 12.77025428]]    Loss_Validation:  [[ 11.12315273]]\n",
      "Loop  1995 :    Loss_Train:  [[ 12.77017367]]    Loss_Validation:  [[ 11.12318599]]\n",
      "Loop  1996 :    Loss_Train:  [[ 12.7700932]]    Loss_Validation:  [[ 11.12321925]]\n",
      "Loop  1997 :    Loss_Train:  [[ 12.77001288]]    Loss_Validation:  [[ 11.12325251]]\n",
      "Loop  1998 :    Loss_Train:  [[ 12.76993271]]    Loss_Validation:  [[ 11.12328577]]\n",
      "Loop  1999 :    Loss_Train:  [[ 12.76985269]]    Loss_Validation:  [[ 11.12331904]]\n",
      "Loop  2000 :    Loss_Train:  [[ 12.76977281]]    Loss_Validation:  [[ 11.12335231]]\n",
      "Loop  2001 :    Loss_Train:  [[ 12.76969308]]    Loss_Validation:  [[ 11.12338559]]\n",
      "Loop  2002 :    Loss_Train:  [[ 12.76961349]]    Loss_Validation:  [[ 11.12341886]]\n",
      "Loop  2003 :    Loss_Train:  [[ 12.76953405]]    Loss_Validation:  [[ 11.12345214]]\n",
      "Loop  2004 :    Loss_Train:  [[ 12.76945475]]    Loss_Validation:  [[ 11.12348543]]\n",
      "Loop  2005 :    Loss_Train:  [[ 12.7693756]]    Loss_Validation:  [[ 11.12351871]]\n",
      "Loop  2006 :    Loss_Train:  [[ 12.76929659]]    Loss_Validation:  [[ 11.123552]]\n",
      "Loop  2007 :    Loss_Train:  [[ 12.76921772]]    Loss_Validation:  [[ 11.12358529]]\n",
      "Loop  2008 :    Loss_Train:  [[ 12.769139]]    Loss_Validation:  [[ 11.12361858]]\n",
      "Loop  2009 :    Loss_Train:  [[ 12.76906042]]    Loss_Validation:  [[ 11.12365188]]\n",
      "Loop  2010 :    Loss_Train:  [[ 12.76898199]]    Loss_Validation:  [[ 11.12368517]]\n",
      "Loop  2011 :    Loss_Train:  [[ 12.7689037]]    Loss_Validation:  [[ 11.12371847]]\n",
      "Loop  2012 :    Loss_Train:  [[ 12.76882555]]    Loss_Validation:  [[ 11.12375178]]\n",
      "Loop  2013 :    Loss_Train:  [[ 12.76874754]]    Loss_Validation:  [[ 11.12378508]]\n",
      "Loop  2014 :    Loss_Train:  [[ 12.76866967]]    Loss_Validation:  [[ 11.12381838]]\n",
      "Loop  2015 :    Loss_Train:  [[ 12.76859195]]    Loss_Validation:  [[ 11.12385169]]\n",
      "Loop  2016 :    Loss_Train:  [[ 12.76851437]]    Loss_Validation:  [[ 11.123885]]\n",
      "Loop  2017 :    Loss_Train:  [[ 12.76843693]]    Loss_Validation:  [[ 11.12391831]]\n",
      "Loop  2018 :    Loss_Train:  [[ 12.76835963]]    Loss_Validation:  [[ 11.12395162]]\n",
      "Loop  2019 :    Loss_Train:  [[ 12.76828247]]    Loss_Validation:  [[ 11.12398494]]\n",
      "Loop  2020 :    Loss_Train:  [[ 12.76820545]]    Loss_Validation:  [[ 11.12401825]]\n",
      "Loop  2021 :    Loss_Train:  [[ 12.76812857]]    Loss_Validation:  [[ 11.12405157]]\n",
      "Loop  2022 :    Loss_Train:  [[ 12.76805183]]    Loss_Validation:  [[ 11.12408489]]\n",
      "Loop  2023 :    Loss_Train:  [[ 12.76797523]]    Loss_Validation:  [[ 11.12411821]]\n",
      "Loop  2024 :    Loss_Train:  [[ 12.76789877]]    Loss_Validation:  [[ 11.12415153]]\n",
      "Loop  2025 :    Loss_Train:  [[ 12.76782245]]    Loss_Validation:  [[ 11.12418485]]\n",
      "Loop  2026 :    Loss_Train:  [[ 12.76774627]]    Loss_Validation:  [[ 11.12421817]]\n",
      "Loop  2027 :    Loss_Train:  [[ 12.76767022]]    Loss_Validation:  [[ 11.1242515]]\n",
      "Loop  2028 :    Loss_Train:  [[ 12.76759431]]    Loss_Validation:  [[ 11.12428482]]\n",
      "Loop  2029 :    Loss_Train:  [[ 12.76751855]]    Loss_Validation:  [[ 11.12431815]]\n",
      "Loop  2030 :    Loss_Train:  [[ 12.76744292]]    Loss_Validation:  [[ 11.12435147]]\n",
      "Loop  2031 :    Loss_Train:  [[ 12.76736742]]    Loss_Validation:  [[ 11.1243848]]\n",
      "Loop  2032 :    Loss_Train:  [[ 12.76729207]]    Loss_Validation:  [[ 11.12441813]]\n",
      "Loop  2033 :    Loss_Train:  [[ 12.76721685]]    Loss_Validation:  [[ 11.12445146]]\n",
      "Loop  2034 :    Loss_Train:  [[ 12.76714177]]    Loss_Validation:  [[ 11.12448479]]\n",
      "Loop  2035 :    Loss_Train:  [[ 12.76706682]]    Loss_Validation:  [[ 11.12451812]]\n",
      "Loop  2036 :    Loss_Train:  [[ 12.76699201]]    Loss_Validation:  [[ 11.12455145]]\n",
      "Loop  2037 :    Loss_Train:  [[ 12.76691734]]    Loss_Validation:  [[ 11.12458478]]\n",
      "Loop  2038 :    Loss_Train:  [[ 12.7668428]]    Loss_Validation:  [[ 11.12461811]]\n",
      "Loop  2039 :    Loss_Train:  [[ 12.7667684]]    Loss_Validation:  [[ 11.12465144]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  2040 :    Loss_Train:  [[ 12.76669413]]    Loss_Validation:  [[ 11.12468477]]\n",
      "Loop  2041 :    Loss_Train:  [[ 12.76661999]]    Loss_Validation:  [[ 11.1247181]]\n",
      "Loop  2042 :    Loss_Train:  [[ 12.766546]]    Loss_Validation:  [[ 11.12475143]]\n",
      "Loop  2043 :    Loss_Train:  [[ 12.76647213]]    Loss_Validation:  [[ 11.12478476]]\n",
      "Loop  2044 :    Loss_Train:  [[ 12.7663984]]    Loss_Validation:  [[ 11.12481809]]\n",
      "Loop  2045 :    Loss_Train:  [[ 12.7663248]]    Loss_Validation:  [[ 11.12485142]]\n",
      "Loop  2046 :    Loss_Train:  [[ 12.76625134]]    Loss_Validation:  [[ 11.12488475]]\n",
      "Loop  2047 :    Loss_Train:  [[ 12.76617801]]    Loss_Validation:  [[ 11.12491808]]\n",
      "Loop  2048 :    Loss_Train:  [[ 12.76610481]]    Loss_Validation:  [[ 11.12495141]]\n",
      "Loop  2049 :    Loss_Train:  [[ 12.76603175]]    Loss_Validation:  [[ 11.12498474]]\n",
      "Loop  2050 :    Loss_Train:  [[ 12.76595882]]    Loss_Validation:  [[ 11.12501806]]\n",
      "Loop  2051 :    Loss_Train:  [[ 12.76588602]]    Loss_Validation:  [[ 11.12505139]]\n",
      "Loop  2052 :    Loss_Train:  [[ 12.76581335]]    Loss_Validation:  [[ 11.12508472]]\n",
      "Loop  2053 :    Loss_Train:  [[ 12.76574082]]    Loss_Validation:  [[ 11.12511804]]\n",
      "Loop  2054 :    Loss_Train:  [[ 12.76566841]]    Loss_Validation:  [[ 11.12515137]]\n",
      "Loop  2055 :    Loss_Train:  [[ 12.76559614]]    Loss_Validation:  [[ 11.12518469]]\n",
      "Loop  2056 :    Loss_Train:  [[ 12.765524]]    Loss_Validation:  [[ 11.12521802]]\n",
      "Loop  2057 :    Loss_Train:  [[ 12.76545199]]    Loss_Validation:  [[ 11.12525134]]\n",
      "Loop  2058 :    Loss_Train:  [[ 12.76538011]]    Loss_Validation:  [[ 11.12528466]]\n",
      "Loop  2059 :    Loss_Train:  [[ 12.76530836]]    Loss_Validation:  [[ 11.12531798]]\n",
      "Loop  2060 :    Loss_Train:  [[ 12.76523673]]    Loss_Validation:  [[ 11.1253513]]\n",
      "Loop  2061 :    Loss_Train:  [[ 12.76516524]]    Loss_Validation:  [[ 11.12538462]]\n",
      "Loop  2062 :    Loss_Train:  [[ 12.76509388]]    Loss_Validation:  [[ 11.12541793]]\n",
      "Loop  2063 :    Loss_Train:  [[ 12.76502265]]    Loss_Validation:  [[ 11.12545125]]\n",
      "Loop  2064 :    Loss_Train:  [[ 12.76495155]]    Loss_Validation:  [[ 11.12548456]]\n",
      "Loop  2065 :    Loss_Train:  [[ 12.76488057]]    Loss_Validation:  [[ 11.12551787]]\n",
      "Loop  2066 :    Loss_Train:  [[ 12.76480973]]    Loss_Validation:  [[ 11.12555119]]\n",
      "Loop  2067 :    Loss_Train:  [[ 12.76473901]]    Loss_Validation:  [[ 11.12558449]]\n",
      "Loop  2068 :    Loss_Train:  [[ 12.76466842]]    Loss_Validation:  [[ 11.1256178]]\n",
      "Loop  2069 :    Loss_Train:  [[ 12.76459796]]    Loss_Validation:  [[ 11.12565111]]\n",
      "Loop  2070 :    Loss_Train:  [[ 12.76452762]]    Loss_Validation:  [[ 11.12568441]]\n",
      "Loop  2071 :    Loss_Train:  [[ 12.76445742]]    Loss_Validation:  [[ 11.12571771]]\n",
      "Loop  2072 :    Loss_Train:  [[ 12.76438734]]    Loss_Validation:  [[ 11.12575101]]\n",
      "Loop  2073 :    Loss_Train:  [[ 12.76431738]]    Loss_Validation:  [[ 11.12578431]]\n",
      "Loop  2074 :    Loss_Train:  [[ 12.76424756]]    Loss_Validation:  [[ 11.12581761]]\n",
      "Loop  2075 :    Loss_Train:  [[ 12.76417786]]    Loss_Validation:  [[ 11.1258509]]\n",
      "Loop  2076 :    Loss_Train:  [[ 12.76410828]]    Loss_Validation:  [[ 11.12588419]]\n",
      "Loop  2077 :    Loss_Train:  [[ 12.76403883]]    Loss_Validation:  [[ 11.12591748]]\n",
      "Loop  2078 :    Loss_Train:  [[ 12.76396951]]    Loss_Validation:  [[ 11.12595077]]\n",
      "Loop  2079 :    Loss_Train:  [[ 12.76390031]]    Loss_Validation:  [[ 11.12598405]]\n",
      "Loop  2080 :    Loss_Train:  [[ 12.76383124]]    Loss_Validation:  [[ 11.12601734]]\n",
      "Loop  2081 :    Loss_Train:  [[ 12.76376229]]    Loss_Validation:  [[ 11.12605062]]\n",
      "Loop  2082 :    Loss_Train:  [[ 12.76369347]]    Loss_Validation:  [[ 11.12608389]]\n",
      "Loop  2083 :    Loss_Train:  [[ 12.76362477]]    Loss_Validation:  [[ 11.12611717]]\n",
      "Loop  2084 :    Loss_Train:  [[ 12.7635562]]    Loss_Validation:  [[ 11.12615044]]\n",
      "Loop  2085 :    Loss_Train:  [[ 12.76348775]]    Loss_Validation:  [[ 11.12618371]]\n",
      "Loop  2086 :    Loss_Train:  [[ 12.76341942]]    Loss_Validation:  [[ 11.12621698]]\n",
      "Loop  2087 :    Loss_Train:  [[ 12.76335122]]    Loss_Validation:  [[ 11.12625024]]\n",
      "Loop  2088 :    Loss_Train:  [[ 12.76328313]]    Loss_Validation:  [[ 11.12628351]]\n",
      "Loop  2089 :    Loss_Train:  [[ 12.76321518]]    Loss_Validation:  [[ 11.12631676]]\n",
      "Loop  2090 :    Loss_Train:  [[ 12.76314734]]    Loss_Validation:  [[ 11.12635002]]\n",
      "Loop  2091 :    Loss_Train:  [[ 12.76307963]]    Loss_Validation:  [[ 11.12638327]]\n",
      "Loop  2092 :    Loss_Train:  [[ 12.76301204]]    Loss_Validation:  [[ 11.12641652]]\n",
      "Loop  2093 :    Loss_Train:  [[ 12.76294457]]    Loss_Validation:  [[ 11.12644977]]\n",
      "Loop  2094 :    Loss_Train:  [[ 12.76287723]]    Loss_Validation:  [[ 11.12648301]]\n",
      "Loop  2095 :    Loss_Train:  [[ 12.76281]]    Loss_Validation:  [[ 11.12651625]]\n",
      "Loop  2096 :    Loss_Train:  [[ 12.7627429]]    Loss_Validation:  [[ 11.12654949]]\n",
      "Loop  2097 :    Loss_Train:  [[ 12.76267592]]    Loss_Validation:  [[ 11.12658273]]\n",
      "Loop  2098 :    Loss_Train:  [[ 12.76260906]]    Loss_Validation:  [[ 11.12661596]]\n",
      "Loop  2099 :    Loss_Train:  [[ 12.76254232]]    Loss_Validation:  [[ 11.12664918]]\n",
      "Loop  2100 :    Loss_Train:  [[ 12.7624757]]    Loss_Validation:  [[ 11.12668241]]\n",
      "Loop  2101 :    Loss_Train:  [[ 12.7624092]]    Loss_Validation:  [[ 11.12671563]]\n",
      "Loop  2102 :    Loss_Train:  [[ 12.76234282]]    Loss_Validation:  [[ 11.12674884]]\n",
      "Loop  2103 :    Loss_Train:  [[ 12.76227656]]    Loss_Validation:  [[ 11.12678206]]\n",
      "Loop  2104 :    Loss_Train:  [[ 12.76221042]]    Loss_Validation:  [[ 11.12681527]]\n",
      "Loop  2105 :    Loss_Train:  [[ 12.7621444]]    Loss_Validation:  [[ 11.12684847]]\n",
      "Loop  2106 :    Loss_Train:  [[ 12.76207849]]    Loss_Validation:  [[ 11.12688168]]\n",
      "Loop  2107 :    Loss_Train:  [[ 12.76201271]]    Loss_Validation:  [[ 11.12691487]]\n",
      "Loop  2108 :    Loss_Train:  [[ 12.76194705]]    Loss_Validation:  [[ 11.12694807]]\n",
      "Loop  2109 :    Loss_Train:  [[ 12.7618815]]    Loss_Validation:  [[ 11.12698126]]\n",
      "Loop  2110 :    Loss_Train:  [[ 12.76181607]]    Loss_Validation:  [[ 11.12701444]]\n",
      "Loop  2111 :    Loss_Train:  [[ 12.76175076]]    Loss_Validation:  [[ 11.12704763]]\n",
      "Loop  2112 :    Loss_Train:  [[ 12.76168557]]    Loss_Validation:  [[ 11.12708081]]\n",
      "Loop  2113 :    Loss_Train:  [[ 12.76162049]]    Loss_Validation:  [[ 11.12711398]]\n",
      "Loop  2114 :    Loss_Train:  [[ 12.76155553]]    Loss_Validation:  [[ 11.12714715]]\n",
      "Loop  2115 :    Loss_Train:  [[ 12.76149069]]    Loss_Validation:  [[ 11.12718032]]\n",
      "Loop  2116 :    Loss_Train:  [[ 12.76142597]]    Loss_Validation:  [[ 11.12721348]]\n",
      "Loop  2117 :    Loss_Train:  [[ 12.76136136]]    Loss_Validation:  [[ 11.12724664]]\n",
      "Loop  2118 :    Loss_Train:  [[ 12.76129687]]    Loss_Validation:  [[ 11.12727979]]\n",
      "Loop  2119 :    Loss_Train:  [[ 12.7612325]]    Loss_Validation:  [[ 11.12731294]]\n",
      "Loop  2120 :    Loss_Train:  [[ 12.76116824]]    Loss_Validation:  [[ 11.12734608]]\n",
      "Loop  2121 :    Loss_Train:  [[ 12.76110409]]    Loss_Validation:  [[ 11.12737922]]\n",
      "Loop  2122 :    Loss_Train:  [[ 12.76104007]]    Loss_Validation:  [[ 11.12741236]]\n",
      "Loop  2123 :    Loss_Train:  [[ 12.76097615]]    Loss_Validation:  [[ 11.12744549]]\n",
      "Loop  2124 :    Loss_Train:  [[ 12.76091236]]    Loss_Validation:  [[ 11.12747861]]\n",
      "Loop  2125 :    Loss_Train:  [[ 12.76084867]]    Loss_Validation:  [[ 11.12751173]]\n",
      "Loop  2126 :    Loss_Train:  [[ 12.76078511]]    Loss_Validation:  [[ 11.12754485]]\n",
      "Loop  2127 :    Loss_Train:  [[ 12.76072165]]    Loss_Validation:  [[ 11.12757796]]\n",
      "Loop  2128 :    Loss_Train:  [[ 12.76065831]]    Loss_Validation:  [[ 11.12761107]]\n",
      "Loop  2129 :    Loss_Train:  [[ 12.76059509]]    Loss_Validation:  [[ 11.12764417]]\n",
      "Loop  2130 :    Loss_Train:  [[ 12.76053198]]    Loss_Validation:  [[ 11.12767727]]\n",
      "Loop  2131 :    Loss_Train:  [[ 12.76046898]]    Loss_Validation:  [[ 11.12771036]]\n",
      "Loop  2132 :    Loss_Train:  [[ 12.76040609]]    Loss_Validation:  [[ 11.12774345]]\n",
      "Loop  2133 :    Loss_Train:  [[ 12.76034332]]    Loss_Validation:  [[ 11.12777653]]\n",
      "Loop  2134 :    Loss_Train:  [[ 12.76028066]]    Loss_Validation:  [[ 11.12780961]]\n",
      "Loop  2135 :    Loss_Train:  [[ 12.76021812]]    Loss_Validation:  [[ 11.12784268]]\n",
      "Loop  2136 :    Loss_Train:  [[ 12.76015568]]    Loss_Validation:  [[ 11.12787575]]\n",
      "Loop  2137 :    Loss_Train:  [[ 12.76009336]]    Loss_Validation:  [[ 11.12790881]]\n",
      "Loop  2138 :    Loss_Train:  [[ 12.76003115]]    Loss_Validation:  [[ 11.12794186]]\n",
      "Loop  2139 :    Loss_Train:  [[ 12.75996906]]    Loss_Validation:  [[ 11.12797491]]\n",
      "Loop  2140 :    Loss_Train:  [[ 12.75990707]]    Loss_Validation:  [[ 11.12800796]]\n",
      "Loop  2141 :    Loss_Train:  [[ 12.7598452]]    Loss_Validation:  [[ 11.128041]]\n",
      "Loop  2142 :    Loss_Train:  [[ 12.75978343]]    Loss_Validation:  [[ 11.12807403]]\n",
      "Loop  2143 :    Loss_Train:  [[ 12.75972178]]    Loss_Validation:  [[ 11.12810706]]\n",
      "Loop  2144 :    Loss_Train:  [[ 12.75966024]]    Loss_Validation:  [[ 11.12814009]]\n",
      "Loop  2145 :    Loss_Train:  [[ 12.75959881]]    Loss_Validation:  [[ 11.12817311]]\n",
      "Loop  2146 :    Loss_Train:  [[ 12.75953749]]    Loss_Validation:  [[ 11.12820612]]\n",
      "Loop  2147 :    Loss_Train:  [[ 12.75947628]]    Loss_Validation:  [[ 11.12823913]]\n",
      "Loop  2148 :    Loss_Train:  [[ 12.75941518]]    Loss_Validation:  [[ 11.12827213]]\n",
      "Loop  2149 :    Loss_Train:  [[ 12.75935419]]    Loss_Validation:  [[ 11.12830512]]\n",
      "Loop  2150 :    Loss_Train:  [[ 12.7592933]]    Loss_Validation:  [[ 11.12833811]]\n",
      "Loop  2151 :    Loss_Train:  [[ 12.75923253]]    Loss_Validation:  [[ 11.1283711]]\n",
      "Loop  2152 :    Loss_Train:  [[ 12.75917187]]    Loss_Validation:  [[ 11.12840407]]\n",
      "Loop  2153 :    Loss_Train:  [[ 12.75911132]]    Loss_Validation:  [[ 11.12843705]]\n",
      "Loop  2154 :    Loss_Train:  [[ 12.75905087]]    Loss_Validation:  [[ 11.12847001]]\n",
      "Loop  2155 :    Loss_Train:  [[ 12.75899054]]    Loss_Validation:  [[ 11.12850297]]\n",
      "Loop  2156 :    Loss_Train:  [[ 12.75893031]]    Loss_Validation:  [[ 11.12853593]]\n",
      "Loop  2157 :    Loss_Train:  [[ 12.75887019]]    Loss_Validation:  [[ 11.12856887]]\n",
      "Loop  2158 :    Loss_Train:  [[ 12.75881017]]    Loss_Validation:  [[ 11.12860182]]\n",
      "Loop  2159 :    Loss_Train:  [[ 12.75875027]]    Loss_Validation:  [[ 11.12863475]]\n",
      "Loop  2160 :    Loss_Train:  [[ 12.75869047]]    Loss_Validation:  [[ 11.12866768]]\n",
      "Loop  2161 :    Loss_Train:  [[ 12.75863078]]    Loss_Validation:  [[ 11.1287006]]\n",
      "Loop  2162 :    Loss_Train:  [[ 12.7585712]]    Loss_Validation:  [[ 11.12873352]]\n",
      "Loop  2163 :    Loss_Train:  [[ 12.75851173]]    Loss_Validation:  [[ 11.12876643]]\n",
      "Loop  2164 :    Loss_Train:  [[ 12.75845236]]    Loss_Validation:  [[ 11.12879934]]\n",
      "Loop  2165 :    Loss_Train:  [[ 12.75839309]]    Loss_Validation:  [[ 11.12883223]]\n",
      "Loop  2166 :    Loss_Train:  [[ 12.75833394]]    Loss_Validation:  [[ 11.12886513]]\n",
      "Loop  2167 :    Loss_Train:  [[ 12.75827489]]    Loss_Validation:  [[ 11.12889801]]\n",
      "Loop  2168 :    Loss_Train:  [[ 12.75821594]]    Loss_Validation:  [[ 11.12893089]]\n",
      "Loop  2169 :    Loss_Train:  [[ 12.7581571]]    Loss_Validation:  [[ 11.12896376]]\n",
      "Loop  2170 :    Loss_Train:  [[ 12.75809837]]    Loss_Validation:  [[ 11.12899663]]\n",
      "Loop  2171 :    Loss_Train:  [[ 12.75803974]]    Loss_Validation:  [[ 11.12902949]]\n",
      "Loop  2172 :    Loss_Train:  [[ 12.75798122]]    Loss_Validation:  [[ 11.12906234]]\n",
      "Loop  2173 :    Loss_Train:  [[ 12.7579228]]    Loss_Validation:  [[ 11.12909518]]\n",
      "Loop  2174 :    Loss_Train:  [[ 12.75786449]]    Loss_Validation:  [[ 11.12912802]]\n",
      "Loop  2175 :    Loss_Train:  [[ 12.75780628]]    Loss_Validation:  [[ 11.12916085]]\n",
      "Loop  2176 :    Loss_Train:  [[ 12.75774818]]    Loss_Validation:  [[ 11.12919368]]\n",
      "Loop  2177 :    Loss_Train:  [[ 12.75769018]]    Loss_Validation:  [[ 11.1292265]]\n",
      "Loop  2178 :    Loss_Train:  [[ 12.75763228]]    Loss_Validation:  [[ 11.12925931]]\n",
      "Loop  2179 :    Loss_Train:  [[ 12.75757449]]    Loss_Validation:  [[ 11.12929211]]\n",
      "Loop  2180 :    Loss_Train:  [[ 12.7575168]]    Loss_Validation:  [[ 11.12932491]]\n",
      "Loop  2181 :    Loss_Train:  [[ 12.75745921]]    Loss_Validation:  [[ 11.1293577]]\n",
      "Loop  2182 :    Loss_Train:  [[ 12.75740173]]    Loss_Validation:  [[ 11.12939048]]\n",
      "Loop  2183 :    Loss_Train:  [[ 12.75734435]]    Loss_Validation:  [[ 11.12942326]]\n",
      "Loop  2184 :    Loss_Train:  [[ 12.75728707]]    Loss_Validation:  [[ 11.12945603]]\n",
      "Loop  2185 :    Loss_Train:  [[ 12.7572299]]    Loss_Validation:  [[ 11.12948879]]\n",
      "Loop  2186 :    Loss_Train:  [[ 12.75717283]]    Loss_Validation:  [[ 11.12952155]]\n",
      "Loop  2187 :    Loss_Train:  [[ 12.75711586]]    Loss_Validation:  [[ 11.1295543]]\n",
      "Loop  2188 :    Loss_Train:  [[ 12.75705899]]    Loss_Validation:  [[ 11.12958704]]\n",
      "Loop  2189 :    Loss_Train:  [[ 12.75700222]]    Loss_Validation:  [[ 11.12961977]]\n",
      "Loop  2190 :    Loss_Train:  [[ 12.75694556]]    Loss_Validation:  [[ 11.1296525]]\n",
      "Loop  2191 :    Loss_Train:  [[ 12.75688899]]    Loss_Validation:  [[ 11.12968521]]\n",
      "Loop  2192 :    Loss_Train:  [[ 12.75683253]]    Loss_Validation:  [[ 11.12971792]]\n",
      "Loop  2193 :    Loss_Train:  [[ 12.75677617]]    Loss_Validation:  [[ 11.12975063]]\n",
      "Loop  2194 :    Loss_Train:  [[ 12.75671991]]    Loss_Validation:  [[ 11.12978332]]\n",
      "Loop  2195 :    Loss_Train:  [[ 12.75666375]]    Loss_Validation:  [[ 11.12981601]]\n",
      "Loop  2196 :    Loss_Train:  [[ 12.75660769]]    Loss_Validation:  [[ 11.12984869]]\n",
      "Loop  2197 :    Loss_Train:  [[ 12.75655173]]    Loss_Validation:  [[ 11.12988137]]\n",
      "Loop  2198 :    Loss_Train:  [[ 12.75649587]]    Loss_Validation:  [[ 11.12991403]]\n",
      "Loop  2199 :    Loss_Train:  [[ 12.75644011]]    Loss_Validation:  [[ 11.12994669]]\n",
      "Loop  2200 :    Loss_Train:  [[ 12.75638446]]    Loss_Validation:  [[ 11.12997934]]\n",
      "Loop  2201 :    Loss_Train:  [[ 12.7563289]]    Loss_Validation:  [[ 11.13001199]]\n",
      "Loop  2202 :    Loss_Train:  [[ 12.75627344]]    Loss_Validation:  [[ 11.13004462]]\n",
      "Loop  2203 :    Loss_Train:  [[ 12.75621807]]    Loss_Validation:  [[ 11.13007725]]\n",
      "Loop  2204 :    Loss_Train:  [[ 12.75616281]]    Loss_Validation:  [[ 11.13010987]]\n",
      "Loop  2205 :    Loss_Train:  [[ 12.75610765]]    Loss_Validation:  [[ 11.13014248]]\n",
      "Loop  2206 :    Loss_Train:  [[ 12.75605258]]    Loss_Validation:  [[ 11.13017509]]\n",
      "Loop  2207 :    Loss_Train:  [[ 12.75599762]]    Loss_Validation:  [[ 11.13020768]]\n",
      "Loop  2208 :    Loss_Train:  [[ 12.75594275]]    Loss_Validation:  [[ 11.13024027]]\n",
      "Loop  2209 :    Loss_Train:  [[ 12.75588798]]    Loss_Validation:  [[ 11.13027285]]\n",
      "Loop  2210 :    Loss_Train:  [[ 12.75583331]]    Loss_Validation:  [[ 11.13030542]]\n",
      "Loop  2211 :    Loss_Train:  [[ 12.75577873]]    Loss_Validation:  [[ 11.13033799]]\n",
      "Loop  2212 :    Loss_Train:  [[ 12.75572425]]    Loss_Validation:  [[ 11.13037054]]\n",
      "Loop  2213 :    Loss_Train:  [[ 12.75566987]]    Loss_Validation:  [[ 11.13040309]]\n",
      "Loop  2214 :    Loss_Train:  [[ 12.75561559]]    Loss_Validation:  [[ 11.13043563]]\n",
      "Loop  2215 :    Loss_Train:  [[ 12.75556141]]    Loss_Validation:  [[ 11.13046816]]\n",
      "Loop  2216 :    Loss_Train:  [[ 12.75550732]]    Loss_Validation:  [[ 11.13050069]]\n",
      "Loop  2217 :    Loss_Train:  [[ 12.75545332]]    Loss_Validation:  [[ 11.1305332]]\n",
      "Loop  2218 :    Loss_Train:  [[ 12.75539943]]    Loss_Validation:  [[ 11.13056571]]\n",
      "Loop  2219 :    Loss_Train:  [[ 12.75534563]]    Loss_Validation:  [[ 11.13059821]]\n",
      "Loop  2220 :    Loss_Train:  [[ 12.75529192]]    Loss_Validation:  [[ 11.1306307]]\n",
      "Loop  2221 :    Loss_Train:  [[ 12.75523832]]    Loss_Validation:  [[ 11.13066318]]\n",
      "Loop  2222 :    Loss_Train:  [[ 12.7551848]]    Loss_Validation:  [[ 11.13069566]]\n",
      "Loop  2223 :    Loss_Train:  [[ 12.75513139]]    Loss_Validation:  [[ 11.13072812]]\n",
      "Loop  2224 :    Loss_Train:  [[ 12.75507807]]    Loss_Validation:  [[ 11.13076058]]\n",
      "Loop  2225 :    Loss_Train:  [[ 12.75502484]]    Loss_Validation:  [[ 11.13079303]]\n",
      "Loop  2226 :    Loss_Train:  [[ 12.75497171]]    Loss_Validation:  [[ 11.13082547]]\n",
      "Loop  2227 :    Loss_Train:  [[ 12.75491867]]    Loss_Validation:  [[ 11.1308579]]\n",
      "Loop  2228 :    Loss_Train:  [[ 12.75486573]]    Loss_Validation:  [[ 11.13089033]]\n",
      "Loop  2229 :    Loss_Train:  [[ 12.75481288]]    Loss_Validation:  [[ 11.13092274]]\n",
      "Loop  2230 :    Loss_Train:  [[ 12.75476013]]    Loss_Validation:  [[ 11.13095515]]\n",
      "Loop  2231 :    Loss_Train:  [[ 12.75470747]]    Loss_Validation:  [[ 11.13098754]]\n",
      "Loop  2232 :    Loss_Train:  [[ 12.7546549]]    Loss_Validation:  [[ 11.13101993]]\n",
      "Loop  2233 :    Loss_Train:  [[ 12.75460243]]    Loss_Validation:  [[ 11.13105231]]\n",
      "Loop  2234 :    Loss_Train:  [[ 12.75455006]]    Loss_Validation:  [[ 11.13108468]]\n",
      "Loop  2235 :    Loss_Train:  [[ 12.75449777]]    Loss_Validation:  [[ 11.13111705]]\n",
      "Loop  2236 :    Loss_Train:  [[ 12.75444558]]    Loss_Validation:  [[ 11.1311494]]\n",
      "Loop  2237 :    Loss_Train:  [[ 12.75439348]]    Loss_Validation:  [[ 11.13118175]]\n",
      "Loop  2238 :    Loss_Train:  [[ 12.75434148]]    Loss_Validation:  [[ 11.13121408]]\n",
      "Loop  2239 :    Loss_Train:  [[ 12.75428956]]    Loss_Validation:  [[ 11.13124641]]\n",
      "Loop  2240 :    Loss_Train:  [[ 12.75423774]]    Loss_Validation:  [[ 11.13127873]]\n",
      "Loop  2241 :    Loss_Train:  [[ 12.75418601]]    Loss_Validation:  [[ 11.13131104]]\n",
      "Loop  2242 :    Loss_Train:  [[ 12.75413438]]    Loss_Validation:  [[ 11.13134334]]\n",
      "Loop  2243 :    Loss_Train:  [[ 12.75408284]]    Loss_Validation:  [[ 11.13137563]]\n",
      "Loop  2244 :    Loss_Train:  [[ 12.75403138]]    Loss_Validation:  [[ 11.13140791]]\n",
      "Loop  2245 :    Loss_Train:  [[ 12.75398002]]    Loss_Validation:  [[ 11.13144019]]\n",
      "Loop  2246 :    Loss_Train:  [[ 12.75392875]]    Loss_Validation:  [[ 11.13147245]]\n",
      "Loop  2247 :    Loss_Train:  [[ 12.75387758]]    Loss_Validation:  [[ 11.13150471]]\n",
      "Loop  2248 :    Loss_Train:  [[ 12.75382649]]    Loss_Validation:  [[ 11.13153695]]\n",
      "Loop  2249 :    Loss_Train:  [[ 12.7537755]]    Loss_Validation:  [[ 11.13156919]]\n",
      "Loop  2250 :    Loss_Train:  [[ 12.75372459]]    Loss_Validation:  [[ 11.13160142]]\n",
      "Loop  2251 :    Loss_Train:  [[ 12.75367378]]    Loss_Validation:  [[ 11.13163364]]\n",
      "Loop  2252 :    Loss_Train:  [[ 12.75362305]]    Loss_Validation:  [[ 11.13166585]]\n",
      "Loop  2253 :    Loss_Train:  [[ 12.75357242]]    Loss_Validation:  [[ 11.13169805]]\n",
      "Loop  2254 :    Loss_Train:  [[ 12.75352188]]    Loss_Validation:  [[ 11.13173024]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  2255 :    Loss_Train:  [[ 12.75347142]]    Loss_Validation:  [[ 11.13176242]]\n",
      "Loop  2256 :    Loss_Train:  [[ 12.75342106]]    Loss_Validation:  [[ 11.13179459]]\n",
      "Loop  2257 :    Loss_Train:  [[ 12.75337079]]    Loss_Validation:  [[ 11.13182676]]\n",
      "Loop  2258 :    Loss_Train:  [[ 12.7533206]]    Loss_Validation:  [[ 11.13185891]]\n",
      "Loop  2259 :    Loss_Train:  [[ 12.75327051]]    Loss_Validation:  [[ 11.13189105]]\n",
      "Loop  2260 :    Loss_Train:  [[ 12.7532205]]    Loss_Validation:  [[ 11.13192319]]\n",
      "Loop  2261 :    Loss_Train:  [[ 12.75317059]]    Loss_Validation:  [[ 11.13195532]]\n",
      "Loop  2262 :    Loss_Train:  [[ 12.75312076]]    Loss_Validation:  [[ 11.13198743]]\n",
      "Loop  2263 :    Loss_Train:  [[ 12.75307102]]    Loss_Validation:  [[ 11.13201954]]\n",
      "Loop  2264 :    Loss_Train:  [[ 12.75302137]]    Loss_Validation:  [[ 11.13205164]]\n",
      "Loop  2265 :    Loss_Train:  [[ 12.75297181]]    Loss_Validation:  [[ 11.13208372]]\n",
      "Loop  2266 :    Loss_Train:  [[ 12.75292233]]    Loss_Validation:  [[ 11.1321158]]\n",
      "Loop  2267 :    Loss_Train:  [[ 12.75287295]]    Loss_Validation:  [[ 11.13214787]]\n",
      "Loop  2268 :    Loss_Train:  [[ 12.75282365]]    Loss_Validation:  [[ 11.13217993]]\n",
      "Loop  2269 :    Loss_Train:  [[ 12.75277444]]    Loss_Validation:  [[ 11.13221198]]\n",
      "Loop  2270 :    Loss_Train:  [[ 12.75272531]]    Loss_Validation:  [[ 11.13224402]]\n",
      "Loop  2271 :    Loss_Train:  [[ 12.75267628]]    Loss_Validation:  [[ 11.13227605]]\n",
      "Loop  2272 :    Loss_Train:  [[ 12.75262733]]    Loss_Validation:  [[ 11.13230807]]\n",
      "Loop  2273 :    Loss_Train:  [[ 12.75257847]]    Loss_Validation:  [[ 11.13234008]]\n",
      "Loop  2274 :    Loss_Train:  [[ 12.75252969]]    Loss_Validation:  [[ 11.13237208]]\n",
      "Loop  2275 :    Loss_Train:  [[ 12.752481]]    Loss_Validation:  [[ 11.13240407]]\n",
      "Loop  2276 :    Loss_Train:  [[ 12.7524324]]    Loss_Validation:  [[ 11.13243605]]\n",
      "Loop  2277 :    Loss_Train:  [[ 12.75238389]]    Loss_Validation:  [[ 11.13246802]]\n",
      "Loop  2278 :    Loss_Train:  [[ 12.75233546]]    Loss_Validation:  [[ 11.13249998]]\n",
      "Loop  2279 :    Loss_Train:  [[ 12.75228711]]    Loss_Validation:  [[ 11.13253193]]\n",
      "Loop  2280 :    Loss_Train:  [[ 12.75223886]]    Loss_Validation:  [[ 11.13256387]]\n",
      "Loop  2281 :    Loss_Train:  [[ 12.75219069]]    Loss_Validation:  [[ 11.13259581]]\n",
      "Loop  2282 :    Loss_Train:  [[ 12.7521426]]    Loss_Validation:  [[ 11.13262773]]\n",
      "Loop  2283 :    Loss_Train:  [[ 12.7520946]]    Loss_Validation:  [[ 11.13265964]]\n",
      "Loop  2284 :    Loss_Train:  [[ 12.75204668]]    Loss_Validation:  [[ 11.13269154]]\n",
      "Loop  2285 :    Loss_Train:  [[ 12.75199885]]    Loss_Validation:  [[ 11.13272343]]\n",
      "Loop  2286 :    Loss_Train:  [[ 12.75195111]]    Loss_Validation:  [[ 11.13275531]]\n",
      "Loop  2287 :    Loss_Train:  [[ 12.75190345]]    Loss_Validation:  [[ 11.13278719]]\n",
      "Loop  2288 :    Loss_Train:  [[ 12.75185587]]    Loss_Validation:  [[ 11.13281905]]\n",
      "Loop  2289 :    Loss_Train:  [[ 12.75180838]]    Loss_Validation:  [[ 11.1328509]]\n",
      "Loop  2290 :    Loss_Train:  [[ 12.75176097]]    Loss_Validation:  [[ 11.13288274]]\n",
      "Loop  2291 :    Loss_Train:  [[ 12.75171365]]    Loss_Validation:  [[ 11.13291457]]\n",
      "Loop  2292 :    Loss_Train:  [[ 12.75166641]]    Loss_Validation:  [[ 11.13294639]]\n",
      "Loop  2293 :    Loss_Train:  [[ 12.75161926]]    Loss_Validation:  [[ 11.1329782]]\n",
      "Loop  2294 :    Loss_Train:  [[ 12.75157219]]    Loss_Validation:  [[ 11.13301]]\n",
      "Loop  2295 :    Loss_Train:  [[ 12.7515252]]    Loss_Validation:  [[ 11.13304179]]\n",
      "Loop  2296 :    Loss_Train:  [[ 12.75147829]]    Loss_Validation:  [[ 11.13307357]]\n",
      "Loop  2297 :    Loss_Train:  [[ 12.75143147]]    Loss_Validation:  [[ 11.13310534]]\n",
      "Loop  2298 :    Loss_Train:  [[ 12.75138473]]    Loss_Validation:  [[ 11.1331371]]\n",
      "Loop  2299 :    Loss_Train:  [[ 12.75133808]]    Loss_Validation:  [[ 11.13316885]]\n",
      "Loop  2300 :    Loss_Train:  [[ 12.75129151]]    Loss_Validation:  [[ 11.13320059]]\n",
      "Loop  2301 :    Loss_Train:  [[ 12.75124502]]    Loss_Validation:  [[ 11.13323232]]\n",
      "Loop  2302 :    Loss_Train:  [[ 12.75119861]]    Loss_Validation:  [[ 11.13326404]]\n",
      "Loop  2303 :    Loss_Train:  [[ 12.75115228]]    Loss_Validation:  [[ 11.13329574]]\n",
      "Loop  2304 :    Loss_Train:  [[ 12.75110604]]    Loss_Validation:  [[ 11.13332744]]\n",
      "Loop  2305 :    Loss_Train:  [[ 12.75105988]]    Loss_Validation:  [[ 11.13335913]]\n",
      "Loop  2306 :    Loss_Train:  [[ 12.7510138]]    Loss_Validation:  [[ 11.1333908]]\n",
      "Loop  2307 :    Loss_Train:  [[ 12.7509678]]    Loss_Validation:  [[ 11.13342247]]\n",
      "Loop  2308 :    Loss_Train:  [[ 12.75092189]]    Loss_Validation:  [[ 11.13345412]]\n",
      "Loop  2309 :    Loss_Train:  [[ 12.75087605]]    Loss_Validation:  [[ 11.13348577]]\n",
      "Loop  2310 :    Loss_Train:  [[ 12.7508303]]    Loss_Validation:  [[ 11.1335174]]\n",
      "Loop  2311 :    Loss_Train:  [[ 12.75078463]]    Loss_Validation:  [[ 11.13354903]]\n",
      "Loop  2312 :    Loss_Train:  [[ 12.75073903]]    Loss_Validation:  [[ 11.13358064]]\n",
      "Loop  2313 :    Loss_Train:  [[ 12.75069352]]    Loss_Validation:  [[ 11.13361224]]\n",
      "Loop  2314 :    Loss_Train:  [[ 12.75064809]]    Loss_Validation:  [[ 11.13364383]]\n",
      "Loop  2315 :    Loss_Train:  [[ 12.75060275]]    Loss_Validation:  [[ 11.13367541]]\n",
      "Loop  2316 :    Loss_Train:  [[ 12.75055748]]    Loss_Validation:  [[ 11.13370698]]\n",
      "Loop  2317 :    Loss_Train:  [[ 12.75051229]]    Loss_Validation:  [[ 11.13373854]]\n",
      "Loop  2318 :    Loss_Train:  [[ 12.75046718]]    Loss_Validation:  [[ 11.13377009]]\n",
      "Loop  2319 :    Loss_Train:  [[ 12.75042215]]    Loss_Validation:  [[ 11.13380163]]\n",
      "Loop  2320 :    Loss_Train:  [[ 12.7503772]]    Loss_Validation:  [[ 11.13383316]]\n",
      "Loop  2321 :    Loss_Train:  [[ 12.75033233]]    Loss_Validation:  [[ 11.13386467]]\n",
      "Loop  2322 :    Loss_Train:  [[ 12.75028754]]    Loss_Validation:  [[ 11.13389618]]\n",
      "Loop  2323 :    Loss_Train:  [[ 12.75024283]]    Loss_Validation:  [[ 11.13392767]]\n",
      "Loop  2324 :    Loss_Train:  [[ 12.7501982]]    Loss_Validation:  [[ 11.13395916]]\n",
      "Loop  2325 :    Loss_Train:  [[ 12.75015365]]    Loss_Validation:  [[ 11.13399063]]\n",
      "Loop  2326 :    Loss_Train:  [[ 12.75010917]]    Loss_Validation:  [[ 11.13402209]]\n",
      "Loop  2327 :    Loss_Train:  [[ 12.75006478]]    Loss_Validation:  [[ 11.13405355]]\n",
      "Loop  2328 :    Loss_Train:  [[ 12.75002046]]    Loss_Validation:  [[ 11.13408499]]\n",
      "Loop  2329 :    Loss_Train:  [[ 12.74997622]]    Loss_Validation:  [[ 11.13411642]]\n",
      "Loop  2330 :    Loss_Train:  [[ 12.74993206]]    Loss_Validation:  [[ 11.13414784]]\n",
      "Loop  2331 :    Loss_Train:  [[ 12.74988798]]    Loss_Validation:  [[ 11.13417924]]\n",
      "Loop  2332 :    Loss_Train:  [[ 12.74984398]]    Loss_Validation:  [[ 11.13421064]]\n",
      "Loop  2333 :    Loss_Train:  [[ 12.74980005]]    Loss_Validation:  [[ 11.13424203]]\n",
      "Loop  2334 :    Loss_Train:  [[ 12.74975621]]    Loss_Validation:  [[ 11.1342734]]\n",
      "Loop  2335 :    Loss_Train:  [[ 12.74971244]]    Loss_Validation:  [[ 11.13430477]]\n",
      "Loop  2336 :    Loss_Train:  [[ 12.74966874]]    Loss_Validation:  [[ 11.13433612]]\n",
      "Loop  2337 :    Loss_Train:  [[ 12.74962513]]    Loss_Validation:  [[ 11.13436746]]\n",
      "Loop  2338 :    Loss_Train:  [[ 12.74958159]]    Loss_Validation:  [[ 11.13439879]]\n",
      "Loop  2339 :    Loss_Train:  [[ 12.74953813]]    Loss_Validation:  [[ 11.13443011]]\n",
      "Loop  2340 :    Loss_Train:  [[ 12.74949474]]    Loss_Validation:  [[ 11.13446142]]\n",
      "Loop  2341 :    Loss_Train:  [[ 12.74945144]]    Loss_Validation:  [[ 11.13449272]]\n",
      "Loop  2342 :    Loss_Train:  [[ 12.7494082]]    Loss_Validation:  [[ 11.134524]]\n",
      "Loop  2343 :    Loss_Train:  [[ 12.74936505]]    Loss_Validation:  [[ 11.13455528]]\n",
      "Loop  2344 :    Loss_Train:  [[ 12.74932197]]    Loss_Validation:  [[ 11.13458654]]\n",
      "Loop  2345 :    Loss_Train:  [[ 12.74927897]]    Loss_Validation:  [[ 11.13461779]]\n",
      "Loop  2346 :    Loss_Train:  [[ 12.74923604]]    Loss_Validation:  [[ 11.13464904]]\n",
      "Loop  2347 :    Loss_Train:  [[ 12.74919319]]    Loss_Validation:  [[ 11.13468027]]\n",
      "Loop  2348 :    Loss_Train:  [[ 12.74915042]]    Loss_Validation:  [[ 11.13471149]]\n",
      "Loop  2349 :    Loss_Train:  [[ 12.74910772]]    Loss_Validation:  [[ 11.13474269]]\n",
      "Loop  2350 :    Loss_Train:  [[ 12.7490651]]    Loss_Validation:  [[ 11.13477389]]\n",
      "Loop  2351 :    Loss_Train:  [[ 12.74902255]]    Loss_Validation:  [[ 11.13480507]]\n",
      "Loop  2352 :    Loss_Train:  [[ 12.74898007]]    Loss_Validation:  [[ 11.13483625]]\n",
      "Loop  2353 :    Loss_Train:  [[ 12.74893768]]    Loss_Validation:  [[ 11.13486741]]\n",
      "Loop  2354 :    Loss_Train:  [[ 12.74889535]]    Loss_Validation:  [[ 11.13489856]]\n",
      "Loop  2355 :    Loss_Train:  [[ 12.74885311]]    Loss_Validation:  [[ 11.1349297]]\n",
      "Loop  2356 :    Loss_Train:  [[ 12.74881093]]    Loss_Validation:  [[ 11.13496083]]\n",
      "Loop  2357 :    Loss_Train:  [[ 12.74876883]]    Loss_Validation:  [[ 11.13499195]]\n",
      "Loop  2358 :    Loss_Train:  [[ 12.74872681]]    Loss_Validation:  [[ 11.13502305]]\n",
      "Loop  2359 :    Loss_Train:  [[ 12.74868486]]    Loss_Validation:  [[ 11.13505415]]\n",
      "Loop  2360 :    Loss_Train:  [[ 12.74864298]]    Loss_Validation:  [[ 11.13508523]]\n",
      "Loop  2361 :    Loss_Train:  [[ 12.74860118]]    Loss_Validation:  [[ 11.1351163]]\n",
      "Loop  2362 :    Loss_Train:  [[ 12.74855945]]    Loss_Validation:  [[ 11.13514736]]\n",
      "Loop  2363 :    Loss_Train:  [[ 12.74851779]]    Loss_Validation:  [[ 11.13517841]]\n",
      "Loop  2364 :    Loss_Train:  [[ 12.74847621]]    Loss_Validation:  [[ 11.13520945]]\n",
      "Loop  2365 :    Loss_Train:  [[ 12.74843471]]    Loss_Validation:  [[ 11.13524047]]\n",
      "Loop  2366 :    Loss_Train:  [[ 12.74839327]]    Loss_Validation:  [[ 11.13527149]]\n",
      "Loop  2367 :    Loss_Train:  [[ 12.74835191]]    Loss_Validation:  [[ 11.13530249]]\n",
      "Loop  2368 :    Loss_Train:  [[ 12.74831062]]    Loss_Validation:  [[ 11.13533348]]\n",
      "Loop  2369 :    Loss_Train:  [[ 12.7482694]]    Loss_Validation:  [[ 11.13536446]]\n",
      "Loop  2370 :    Loss_Train:  [[ 12.74822826]]    Loss_Validation:  [[ 11.13539543]]\n",
      "Loop  2371 :    Loss_Train:  [[ 12.74818719]]    Loss_Validation:  [[ 11.13542638]]\n",
      "Loop  2372 :    Loss_Train:  [[ 12.74814619]]    Loss_Validation:  [[ 11.13545733]]\n",
      "Loop  2373 :    Loss_Train:  [[ 12.74810527]]    Loss_Validation:  [[ 11.13548826]]\n",
      "Loop  2374 :    Loss_Train:  [[ 12.74806441]]    Loss_Validation:  [[ 11.13551918]]\n",
      "Loop  2375 :    Loss_Train:  [[ 12.74802363]]    Loss_Validation:  [[ 11.13555009]]\n",
      "Loop  2376 :    Loss_Train:  [[ 12.74798292]]    Loss_Validation:  [[ 11.13558099]]\n",
      "Loop  2377 :    Loss_Train:  [[ 12.74794229]]    Loss_Validation:  [[ 11.13561187]]\n",
      "Loop  2378 :    Loss_Train:  [[ 12.74790172]]    Loss_Validation:  [[ 11.13564275]]\n",
      "Loop  2379 :    Loss_Train:  [[ 12.74786123]]    Loss_Validation:  [[ 11.13567361]]\n",
      "Loop  2380 :    Loss_Train:  [[ 12.7478208]]    Loss_Validation:  [[ 11.13570446]]\n",
      "Loop  2381 :    Loss_Train:  [[ 12.74778045]]    Loss_Validation:  [[ 11.1357353]]\n",
      "Loop  2382 :    Loss_Train:  [[ 12.74774017]]    Loss_Validation:  [[ 11.13576613]]\n",
      "Loop  2383 :    Loss_Train:  [[ 12.74769996]]    Loss_Validation:  [[ 11.13579694]]\n",
      "Loop  2384 :    Loss_Train:  [[ 12.74765982]]    Loss_Validation:  [[ 11.13582775]]\n",
      "Loop  2385 :    Loss_Train:  [[ 12.74761976]]    Loss_Validation:  [[ 11.13585854]]\n",
      "Loop  2386 :    Loss_Train:  [[ 12.74757976]]    Loss_Validation:  [[ 11.13588932]]\n",
      "Loop  2387 :    Loss_Train:  [[ 12.74753983]]    Loss_Validation:  [[ 11.13592009]]\n",
      "Loop  2388 :    Loss_Train:  [[ 12.74749998]]    Loss_Validation:  [[ 11.13595084]]\n",
      "Loop  2389 :    Loss_Train:  [[ 12.74746019]]    Loss_Validation:  [[ 11.13598159]]\n",
      "Loop  2390 :    Loss_Train:  [[ 12.74742047]]    Loss_Validation:  [[ 11.13601232]]\n",
      "Loop  2391 :    Loss_Train:  [[ 12.74738083]]    Loss_Validation:  [[ 11.13604304]]\n",
      "Loop  2392 :    Loss_Train:  [[ 12.74734125]]    Loss_Validation:  [[ 11.13607375]]\n",
      "Loop  2393 :    Loss_Train:  [[ 12.74730175]]    Loss_Validation:  [[ 11.13610445]]\n",
      "Loop  2394 :    Loss_Train:  [[ 12.74726231]]    Loss_Validation:  [[ 11.13613513]]\n",
      "Loop  2395 :    Loss_Train:  [[ 12.74722294]]    Loss_Validation:  [[ 11.1361658]]\n",
      "Loop  2396 :    Loss_Train:  [[ 12.74718365]]    Loss_Validation:  [[ 11.13619647]]\n",
      "Loop  2397 :    Loss_Train:  [[ 12.74714442]]    Loss_Validation:  [[ 11.13622711]]\n",
      "Loop  2398 :    Loss_Train:  [[ 12.74710526]]    Loss_Validation:  [[ 11.13625775]]\n",
      "Loop  2399 :    Loss_Train:  [[ 12.74706617]]    Loss_Validation:  [[ 11.13628838]]\n",
      "Loop  2400 :    Loss_Train:  [[ 12.74702715]]    Loss_Validation:  [[ 11.13631899]]\n",
      "Loop  2401 :    Loss_Train:  [[ 12.74698819]]    Loss_Validation:  [[ 11.13634959]]\n",
      "Loop  2402 :    Loss_Train:  [[ 12.74694931]]    Loss_Validation:  [[ 11.13638018]]\n",
      "Loop  2403 :    Loss_Train:  [[ 12.74691049]]    Loss_Validation:  [[ 11.13641076]]\n",
      "Loop  2404 :    Loss_Train:  [[ 12.74687174]]    Loss_Validation:  [[ 11.13644132]]\n",
      "Loop  2405 :    Loss_Train:  [[ 12.74683307]]    Loss_Validation:  [[ 11.13647188]]\n",
      "Loop  2406 :    Loss_Train:  [[ 12.74679445]]    Loss_Validation:  [[ 11.13650242]]\n",
      "Loop  2407 :    Loss_Train:  [[ 12.74675591]]    Loss_Validation:  [[ 11.13653295]]\n",
      "Loop  2408 :    Loss_Train:  [[ 12.74671744]]    Loss_Validation:  [[ 11.13656346]]\n",
      "Loop  2409 :    Loss_Train:  [[ 12.74667903]]    Loss_Validation:  [[ 11.13659397]]\n",
      "Loop  2410 :    Loss_Train:  [[ 12.74664069]]    Loss_Validation:  [[ 11.13662446]]\n",
      "Loop  2411 :    Loss_Train:  [[ 12.74660242]]    Loss_Validation:  [[ 11.13665494]]\n",
      "Loop  2412 :    Loss_Train:  [[ 12.74656421]]    Loss_Validation:  [[ 11.13668541]]\n",
      "Loop  2413 :    Loss_Train:  [[ 12.74652607]]    Loss_Validation:  [[ 11.13671586]]\n",
      "Loop  2414 :    Loss_Train:  [[ 12.746488]]    Loss_Validation:  [[ 11.13674631]]\n",
      "Loop  2415 :    Loss_Train:  [[ 12.74645]]    Loss_Validation:  [[ 11.13677674]]\n",
      "Loop  2416 :    Loss_Train:  [[ 12.74641206]]    Loss_Validation:  [[ 11.13680716]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  2417 :    Loss_Train:  [[ 12.74637419]]    Loss_Validation:  [[ 11.13683757]]\n",
      "Loop  2418 :    Loss_Train:  [[ 12.74633639]]    Loss_Validation:  [[ 11.13686796]]\n",
      "Loop  2419 :    Loss_Train:  [[ 12.74629865]]    Loss_Validation:  [[ 11.13689835]]\n",
      "Loop  2420 :    Loss_Train:  [[ 12.74626098]]    Loss_Validation:  [[ 11.13692872]]\n",
      "Loop  2421 :    Loss_Train:  [[ 12.74622338]]    Loss_Validation:  [[ 11.13695907]]\n",
      "Loop  2422 :    Loss_Train:  [[ 12.74618584]]    Loss_Validation:  [[ 11.13698942]]\n",
      "Loop  2423 :    Loss_Train:  [[ 12.74614836]]    Loss_Validation:  [[ 11.13701975]]\n",
      "Loop  2424 :    Loss_Train:  [[ 12.74611096]]    Loss_Validation:  [[ 11.13705008]]\n",
      "Loop  2425 :    Loss_Train:  [[ 12.74607362]]    Loss_Validation:  [[ 11.13708039]]\n",
      "Loop  2426 :    Loss_Train:  [[ 12.74603634]]    Loss_Validation:  [[ 11.13711068]]\n",
      "Loop  2427 :    Loss_Train:  [[ 12.74599913]]    Loss_Validation:  [[ 11.13714097]]\n",
      "Loop  2428 :    Loss_Train:  [[ 12.74596199]]    Loss_Validation:  [[ 11.13717124]]\n",
      "Loop  2429 :    Loss_Train:  [[ 12.74592491]]    Loss_Validation:  [[ 11.1372015]]\n",
      "Loop  2430 :    Loss_Train:  [[ 12.7458879]]    Loss_Validation:  [[ 11.13723175]]\n",
      "Loop  2431 :    Loss_Train:  [[ 12.74585095]]    Loss_Validation:  [[ 11.13726198]]\n",
      "Loop  2432 :    Loss_Train:  [[ 12.74581406]]    Loss_Validation:  [[ 11.13729221]]\n",
      "Loop  2433 :    Loss_Train:  [[ 12.74577725]]    Loss_Validation:  [[ 11.13732242]]\n",
      "Loop  2434 :    Loss_Train:  [[ 12.74574049]]    Loss_Validation:  [[ 11.13735261]]\n",
      "Loop  2435 :    Loss_Train:  [[ 12.7457038]]    Loss_Validation:  [[ 11.1373828]]\n",
      "Loop  2436 :    Loss_Train:  [[ 12.74566718]]    Loss_Validation:  [[ 11.13741297]]\n",
      "Loop  2437 :    Loss_Train:  [[ 12.74563062]]    Loss_Validation:  [[ 11.13744313]]\n",
      "Loop  2438 :    Loss_Train:  [[ 12.74559412]]    Loss_Validation:  [[ 11.13747328]]\n",
      "Loop  2439 :    Loss_Train:  [[ 12.74555769]]    Loss_Validation:  [[ 11.13750342]]\n",
      "Loop  2440 :    Loss_Train:  [[ 12.74552132]]    Loss_Validation:  [[ 11.13753354]]\n",
      "Loop  2441 :    Loss_Train:  [[ 12.74548502]]    Loss_Validation:  [[ 11.13756365]]\n",
      "Loop  2442 :    Loss_Train:  [[ 12.74544878]]    Loss_Validation:  [[ 11.13759375]]\n",
      "Loop  2443 :    Loss_Train:  [[ 12.7454126]]    Loss_Validation:  [[ 11.13762384]]\n",
      "Loop  2444 :    Loss_Train:  [[ 12.74537648]]    Loss_Validation:  [[ 11.13765391]]\n",
      "Loop  2445 :    Loss_Train:  [[ 12.74534043]]    Loss_Validation:  [[ 11.13768397]]\n",
      "Loop  2446 :    Loss_Train:  [[ 12.74530445]]    Loss_Validation:  [[ 11.13771402]]\n",
      "Loop  2447 :    Loss_Train:  [[ 12.74526852]]    Loss_Validation:  [[ 11.13774406]]\n",
      "Loop  2448 :    Loss_Train:  [[ 12.74523266]]    Loss_Validation:  [[ 11.13777408]]\n",
      "Loop  2449 :    Loss_Train:  [[ 12.74519687]]    Loss_Validation:  [[ 11.13780409]]\n",
      "Loop  2450 :    Loss_Train:  [[ 12.74516113]]    Loss_Validation:  [[ 11.13783409]]\n",
      "Loop  2451 :    Loss_Train:  [[ 12.74512546]]    Loss_Validation:  [[ 11.13786408]]\n",
      "Loop  2452 :    Loss_Train:  [[ 12.74508985]]    Loss_Validation:  [[ 11.13789405]]\n",
      "Loop  2453 :    Loss_Train:  [[ 12.7450543]]    Loss_Validation:  [[ 11.13792401]]\n",
      "Loop  2454 :    Loss_Train:  [[ 12.74501882]]    Loss_Validation:  [[ 11.13795396]]\n",
      "Loop  2455 :    Loss_Train:  [[ 12.7449834]]    Loss_Validation:  [[ 11.1379839]]\n",
      "Loop  2456 :    Loss_Train:  [[ 12.74494804]]    Loss_Validation:  [[ 11.13801382]]\n",
      "Loop  2457 :    Loss_Train:  [[ 12.74491274]]    Loss_Validation:  [[ 11.13804373]]\n",
      "Loop  2458 :    Loss_Train:  [[ 12.7448775]]    Loss_Validation:  [[ 11.13807363]]\n",
      "Loop  2459 :    Loss_Train:  [[ 12.74484233]]    Loss_Validation:  [[ 11.13810351]]\n",
      "Loop  2460 :    Loss_Train:  [[ 12.74480722]]    Loss_Validation:  [[ 11.13813338]]\n",
      "Loop  2461 :    Loss_Train:  [[ 12.74477217]]    Loss_Validation:  [[ 11.13816324]]\n",
      "Loop  2462 :    Loss_Train:  [[ 12.74473718]]    Loss_Validation:  [[ 11.13819309]]\n",
      "Loop  2463 :    Loss_Train:  [[ 12.74470225]]    Loss_Validation:  [[ 11.13822293]]\n",
      "Loop  2464 :    Loss_Train:  [[ 12.74466738]]    Loss_Validation:  [[ 11.13825275]]\n",
      "Loop  2465 :    Loss_Train:  [[ 12.74463258]]    Loss_Validation:  [[ 11.13828256]]\n",
      "Loop  2466 :    Loss_Train:  [[ 12.74459783]]    Loss_Validation:  [[ 11.13831235]]\n",
      "Loop  2467 :    Loss_Train:  [[ 12.74456315]]    Loss_Validation:  [[ 11.13834214]]\n",
      "Loop  2468 :    Loss_Train:  [[ 12.74452853]]    Loss_Validation:  [[ 11.13837191]]\n",
      "Loop  2469 :    Loss_Train:  [[ 12.74449396]]    Loss_Validation:  [[ 11.13840167]]\n",
      "Loop  2470 :    Loss_Train:  [[ 12.74445946]]    Loss_Validation:  [[ 11.13843141]]\n",
      "Loop  2471 :    Loss_Train:  [[ 12.74442502]]    Loss_Validation:  [[ 11.13846115]]\n",
      "Loop  2472 :    Loss_Train:  [[ 12.74439064]]    Loss_Validation:  [[ 11.13849087]]\n",
      "Loop  2473 :    Loss_Train:  [[ 12.74435632]]    Loss_Validation:  [[ 11.13852057]]\n",
      "Loop  2474 :    Loss_Train:  [[ 12.74432206]]    Loss_Validation:  [[ 11.13855027]]\n",
      "Loop  2475 :    Loss_Train:  [[ 12.74428786]]    Loss_Validation:  [[ 11.13857995]]\n",
      "Loop  2476 :    Loss_Train:  [[ 12.74425372]]    Loss_Validation:  [[ 11.13860962]]\n",
      "Loop  2477 :    Loss_Train:  [[ 12.74421964]]    Loss_Validation:  [[ 11.13863928]]\n",
      "Loop  2478 :    Loss_Train:  [[ 12.74418562]]    Loss_Validation:  [[ 11.13866892]]\n",
      "Loop  2479 :    Loss_Train:  [[ 12.74415166]]    Loss_Validation:  [[ 11.13869855]]\n",
      "Loop  2480 :    Loss_Train:  [[ 12.74411776]]    Loss_Validation:  [[ 11.13872817]]\n",
      "Loop  2481 :    Loss_Train:  [[ 12.74408391]]    Loss_Validation:  [[ 11.13875777]]\n",
      "Loop  2482 :    Loss_Train:  [[ 12.74405013]]    Loss_Validation:  [[ 11.13878737]]\n",
      "Loop  2483 :    Loss_Train:  [[ 12.74401641]]    Loss_Validation:  [[ 11.13881695]]\n",
      "Loop  2484 :    Loss_Train:  [[ 12.74398274]]    Loss_Validation:  [[ 11.13884651]]\n",
      "Loop  2485 :    Loss_Train:  [[ 12.74394914]]    Loss_Validation:  [[ 11.13887607]]\n",
      "Loop  2486 :    Loss_Train:  [[ 12.74391559]]    Loss_Validation:  [[ 11.13890561]]\n",
      "Loop  2487 :    Loss_Train:  [[ 12.7438821]]    Loss_Validation:  [[ 11.13893514]]\n",
      "Loop  2488 :    Loss_Train:  [[ 12.74384867]]    Loss_Validation:  [[ 11.13896465]]\n",
      "Loop  2489 :    Loss_Train:  [[ 12.7438153]]    Loss_Validation:  [[ 11.13899415]]\n",
      "Loop  2490 :    Loss_Train:  [[ 12.74378199]]    Loss_Validation:  [[ 11.13902364]]\n",
      "Loop  2491 :    Loss_Train:  [[ 12.74374874]]    Loss_Validation:  [[ 11.13905312]]\n",
      "Loop  2492 :    Loss_Train:  [[ 12.74371554]]    Loss_Validation:  [[ 11.13908258]]\n",
      "Loop  2493 :    Loss_Train:  [[ 12.7436824]]    Loss_Validation:  [[ 11.13911204]]\n",
      "Loop  2494 :    Loss_Train:  [[ 12.74364932]]    Loss_Validation:  [[ 11.13914147]]\n",
      "Loop  2495 :    Loss_Train:  [[ 12.7436163]]    Loss_Validation:  [[ 11.1391709]]\n",
      "Loop  2496 :    Loss_Train:  [[ 12.74358334]]    Loss_Validation:  [[ 11.13920031]]\n",
      "Loop  2497 :    Loss_Train:  [[ 12.74355043]]    Loss_Validation:  [[ 11.13922971]]\n",
      "Loop  2498 :    Loss_Train:  [[ 12.74351758]]    Loss_Validation:  [[ 11.1392591]]\n",
      "Loop  2499 :    Loss_Train:  [[ 12.74348479]]    Loss_Validation:  [[ 11.13928847]]\n",
      "Loop  2500 :    Loss_Train:  [[ 12.74345206]]    Loss_Validation:  [[ 11.13931783]]\n",
      "Loop  2501 :    Loss_Train:  [[ 12.74341938]]    Loss_Validation:  [[ 11.13934718]]\n",
      "Loop  2502 :    Loss_Train:  [[ 12.74338676]]    Loss_Validation:  [[ 11.13937651]]\n",
      "Loop  2503 :    Loss_Train:  [[ 12.7433542]]    Loss_Validation:  [[ 11.13940583]]\n",
      "Loop  2504 :    Loss_Train:  [[ 12.74332169]]    Loss_Validation:  [[ 11.13943514]]\n",
      "Loop  2505 :    Loss_Train:  [[ 12.74328924]]    Loss_Validation:  [[ 11.13946444]]\n",
      "Loop  2506 :    Loss_Train:  [[ 12.74325685]]    Loss_Validation:  [[ 11.13949372]]\n",
      "Loop  2507 :    Loss_Train:  [[ 12.74322452]]    Loss_Validation:  [[ 11.13952299]]\n",
      "Loop  2508 :    Loss_Train:  [[ 12.74319224]]    Loss_Validation:  [[ 11.13955225]]\n",
      "Loop  2509 :    Loss_Train:  [[ 12.74316002]]    Loss_Validation:  [[ 11.13958149]]\n",
      "Loop  2510 :    Loss_Train:  [[ 12.74312785]]    Loss_Validation:  [[ 11.13961072]]\n",
      "Loop  2511 :    Loss_Train:  [[ 12.74309574]]    Loss_Validation:  [[ 11.13963994]]\n",
      "Loop  2512 :    Loss_Train:  [[ 12.74306369]]    Loss_Validation:  [[ 11.13966914]]\n",
      "Loop  2513 :    Loss_Train:  [[ 12.74303169]]    Loss_Validation:  [[ 11.13969833]]\n",
      "Loop  2514 :    Loss_Train:  [[ 12.74299975]]    Loss_Validation:  [[ 11.13972751]]\n",
      "Loop  2515 :    Loss_Train:  [[ 12.74296787]]    Loss_Validation:  [[ 11.13975668]]\n",
      "Loop  2516 :    Loss_Train:  [[ 12.74293604]]    Loss_Validation:  [[ 11.13978583]]\n",
      "Loop  2517 :    Loss_Train:  [[ 12.74290426]]    Loss_Validation:  [[ 11.13981497]]\n",
      "Loop  2518 :    Loss_Train:  [[ 12.74287254]]    Loss_Validation:  [[ 11.1398441]]\n",
      "Loop  2519 :    Loss_Train:  [[ 12.74284088]]    Loss_Validation:  [[ 11.13987321]]\n",
      "Loop  2520 :    Loss_Train:  [[ 12.74280927]]    Loss_Validation:  [[ 11.13990231]]\n",
      "Loop  2521 :    Loss_Train:  [[ 12.74277772]]    Loss_Validation:  [[ 11.1399314]]\n",
      "Loop  2522 :    Loss_Train:  [[ 12.74274622]]    Loss_Validation:  [[ 11.13996047]]\n",
      "Loop  2523 :    Loss_Train:  [[ 12.74271478]]    Loss_Validation:  [[ 11.13998953]]\n",
      "Loop  2524 :    Loss_Train:  [[ 12.74268339]]    Loss_Validation:  [[ 11.14001858]]\n",
      "Loop  2525 :    Loss_Train:  [[ 12.74265206]]    Loss_Validation:  [[ 11.14004761]]\n",
      "Loop  2526 :    Loss_Train:  [[ 12.74262078]]    Loss_Validation:  [[ 11.14007664]]\n",
      "Loop  2527 :    Loss_Train:  [[ 12.74258956]]    Loss_Validation:  [[ 11.14010564]]\n",
      "Loop  2528 :    Loss_Train:  [[ 12.74255839]]    Loss_Validation:  [[ 11.14013464]]\n",
      "Loop  2529 :    Loss_Train:  [[ 12.74252728]]    Loss_Validation:  [[ 11.14016362]]\n",
      "Loop  2530 :    Loss_Train:  [[ 12.74249622]]    Loss_Validation:  [[ 11.14019259]]\n",
      "Loop  2531 :    Loss_Train:  [[ 12.74246521]]    Loss_Validation:  [[ 11.14022155]]\n",
      "Loop  2532 :    Loss_Train:  [[ 12.74243426]]    Loss_Validation:  [[ 11.14025049]]\n",
      "Loop  2533 :    Loss_Train:  [[ 12.74240337]]    Loss_Validation:  [[ 11.14027942]]\n",
      "Loop  2534 :    Loss_Train:  [[ 12.74237252]]    Loss_Validation:  [[ 11.14030834]]\n",
      "Loop  2535 :    Loss_Train:  [[ 12.74234173]]    Loss_Validation:  [[ 11.14033724]]\n",
      "Loop  2536 :    Loss_Train:  [[ 12.742311]]    Loss_Validation:  [[ 11.14036613]]\n",
      "Loop  2537 :    Loss_Train:  [[ 12.74228032]]    Loss_Validation:  [[ 11.14039501]]\n",
      "Loop  2538 :    Loss_Train:  [[ 12.74224969]]    Loss_Validation:  [[ 11.14042387]]\n",
      "Loop  2539 :    Loss_Train:  [[ 12.74221911]]    Loss_Validation:  [[ 11.14045272]]\n",
      "Loop  2540 :    Loss_Train:  [[ 12.74218859]]    Loss_Validation:  [[ 11.14048156]]\n",
      "Loop  2541 :    Loss_Train:  [[ 12.74215812]]    Loss_Validation:  [[ 11.14051038]]\n",
      "Loop  2542 :    Loss_Train:  [[ 12.74212771]]    Loss_Validation:  [[ 11.14053919]]\n",
      "Loop  2543 :    Loss_Train:  [[ 12.74209735]]    Loss_Validation:  [[ 11.14056799]]\n",
      "Loop  2544 :    Loss_Train:  [[ 12.74206704]]    Loss_Validation:  [[ 11.14059678]]\n",
      "Loop  2545 :    Loss_Train:  [[ 12.74203678]]    Loss_Validation:  [[ 11.14062555]]\n",
      "Loop  2546 :    Loss_Train:  [[ 12.74200658]]    Loss_Validation:  [[ 11.14065431]]\n",
      "Loop  2547 :    Loss_Train:  [[ 12.74197643]]    Loss_Validation:  [[ 11.14068305]]\n",
      "Loop  2548 :    Loss_Train:  [[ 12.74194633]]    Loss_Validation:  [[ 11.14071178]]\n",
      "Loop  2549 :    Loss_Train:  [[ 12.74191629]]    Loss_Validation:  [[ 11.1407405]]\n",
      "Loop  2550 :    Loss_Train:  [[ 12.74188629]]    Loss_Validation:  [[ 11.14076921]]\n",
      "Loop  2551 :    Loss_Train:  [[ 12.74185635]]    Loss_Validation:  [[ 11.1407979]]\n",
      "Loop  2552 :    Loss_Train:  [[ 12.74182646]]    Loss_Validation:  [[ 11.14082658]]\n",
      "Loop  2553 :    Loss_Train:  [[ 12.74179663]]    Loss_Validation:  [[ 11.14085524]]\n",
      "Loop  2554 :    Loss_Train:  [[ 12.74176684]]    Loss_Validation:  [[ 11.1408839]]\n",
      "Loop  2555 :    Loss_Train:  [[ 12.74173711]]    Loss_Validation:  [[ 11.14091253]]\n",
      "Loop  2556 :    Loss_Train:  [[ 12.74170743]]    Loss_Validation:  [[ 11.14094116]]\n",
      "Loop  2557 :    Loss_Train:  [[ 12.7416778]]    Loss_Validation:  [[ 11.14096977]]\n",
      "Loop  2558 :    Loss_Train:  [[ 12.74164823]]    Loss_Validation:  [[ 11.14099837]]\n",
      "Loop  2559 :    Loss_Train:  [[ 12.7416187]]    Loss_Validation:  [[ 11.14102696]]\n",
      "Loop  2560 :    Loss_Train:  [[ 12.74158923]]    Loss_Validation:  [[ 11.14105553]]\n",
      "Loop  2561 :    Loss_Train:  [[ 12.7415598]]    Loss_Validation:  [[ 11.14108409]]\n",
      "Loop  2562 :    Loss_Train:  [[ 12.74153043]]    Loss_Validation:  [[ 11.14111264]]\n",
      "Loop  2563 :    Loss_Train:  [[ 12.74150111]]    Loss_Validation:  [[ 11.14114117]]\n",
      "Loop  2564 :    Loss_Train:  [[ 12.74147185]]    Loss_Validation:  [[ 11.14116969]]\n",
      "Loop  2565 :    Loss_Train:  [[ 12.74144263]]    Loss_Validation:  [[ 11.1411982]]\n",
      "Loop  2566 :    Loss_Train:  [[ 12.74141346]]    Loss_Validation:  [[ 11.14122669]]\n",
      "Loop  2567 :    Loss_Train:  [[ 12.74138434]]    Loss_Validation:  [[ 11.14125517]]\n",
      "Loop  2568 :    Loss_Train:  [[ 12.74135528]]    Loss_Validation:  [[ 11.14128364]]\n",
      "Loop  2569 :    Loss_Train:  [[ 12.74132627]]    Loss_Validation:  [[ 11.14131209]]\n",
      "Loop  2570 :    Loss_Train:  [[ 12.7412973]]    Loss_Validation:  [[ 11.14134053]]\n",
      "Loop  2571 :    Loss_Train:  [[ 12.74126839]]    Loss_Validation:  [[ 11.14136896]]\n",
      "Loop  2572 :    Loss_Train:  [[ 12.74123952]]    Loss_Validation:  [[ 11.14139737]]\n",
      "Loop  2573 :    Loss_Train:  [[ 12.74121071]]    Loss_Validation:  [[ 11.14142577]]\n",
      "Loop  2574 :    Loss_Train:  [[ 12.74118195]]    Loss_Validation:  [[ 11.14145416]]\n",
      "Loop  2575 :    Loss_Train:  [[ 12.74115324]]    Loss_Validation:  [[ 11.14148253]]\n",
      "Loop  2576 :    Loss_Train:  [[ 12.74112457]]    Loss_Validation:  [[ 11.14151089]]\n",
      "Loop  2577 :    Loss_Train:  [[ 12.74109596]]    Loss_Validation:  [[ 11.14153923]]\n",
      "Loop  2578 :    Loss_Train:  [[ 12.7410674]]    Loss_Validation:  [[ 11.14156757]]\n",
      "Loop  2579 :    Loss_Train:  [[ 12.74103889]]    Loss_Validation:  [[ 11.14159589]]\n",
      "Loop  2580 :    Loss_Train:  [[ 12.74101042]]    Loss_Validation:  [[ 11.14162419]]\n",
      "Loop  2581 :    Loss_Train:  [[ 12.74098201]]    Loss_Validation:  [[ 11.14165249]]\n",
      "Loop  2582 :    Loss_Train:  [[ 12.74095365]]    Loss_Validation:  [[ 11.14168077]]\n",
      "Loop  2583 :    Loss_Train:  [[ 12.74092533]]    Loss_Validation:  [[ 11.14170903]]\n",
      "Loop  2584 :    Loss_Train:  [[ 12.74089707]]    Loss_Validation:  [[ 11.14173729]]\n",
      "Loop  2585 :    Loss_Train:  [[ 12.74086885]]    Loss_Validation:  [[ 11.14176553]]\n",
      "Loop  2586 :    Loss_Train:  [[ 12.74084068]]    Loss_Validation:  [[ 11.14179375]]\n",
      "Loop  2587 :    Loss_Train:  [[ 12.74081256]]    Loss_Validation:  [[ 11.14182197]]\n",
      "Loop  2588 :    Loss_Train:  [[ 12.74078449]]    Loss_Validation:  [[ 11.14185017]]\n",
      "Loop  2589 :    Loss_Train:  [[ 12.74075647]]    Loss_Validation:  [[ 11.14187835]]\n",
      "Loop  2590 :    Loss_Train:  [[ 12.7407285]]    Loss_Validation:  [[ 11.14190653]]\n",
      "Loop  2591 :    Loss_Train:  [[ 12.74070058]]    Loss_Validation:  [[ 11.14193468]]\n",
      "Loop  2592 :    Loss_Train:  [[ 12.74067271]]    Loss_Validation:  [[ 11.14196283]]\n",
      "Loop  2593 :    Loss_Train:  [[ 12.74064488]]    Loss_Validation:  [[ 11.14199096]]\n",
      "Loop  2594 :    Loss_Train:  [[ 12.7406171]]    Loss_Validation:  [[ 11.14201908]]\n",
      "Loop  2595 :    Loss_Train:  [[ 12.74058937]]    Loss_Validation:  [[ 11.14204719]]\n",
      "Loop  2596 :    Loss_Train:  [[ 12.74056169]]    Loss_Validation:  [[ 11.14207528]]\n",
      "Loop  2597 :    Loss_Train:  [[ 12.74053406]]    Loss_Validation:  [[ 11.14210336]]\n",
      "Loop  2598 :    Loss_Train:  [[ 12.74050648]]    Loss_Validation:  [[ 11.14213143]]\n",
      "Loop  2599 :    Loss_Train:  [[ 12.74047894]]    Loss_Validation:  [[ 11.14215948]]\n",
      "Loop  2600 :    Loss_Train:  [[ 12.74045145]]    Loss_Validation:  [[ 11.14218752]]\n",
      "Loop  2601 :    Loss_Train:  [[ 12.74042401]]    Loss_Validation:  [[ 11.14221554]]\n",
      "Loop  2602 :    Loss_Train:  [[ 12.74039662]]    Loss_Validation:  [[ 11.14224355]]\n",
      "Loop  2603 :    Loss_Train:  [[ 12.74036927]]    Loss_Validation:  [[ 11.14227155]]\n",
      "Loop  2604 :    Loss_Train:  [[ 12.74034198]]    Loss_Validation:  [[ 11.14229954]]\n",
      "Loop  2605 :    Loss_Train:  [[ 12.74031473]]    Loss_Validation:  [[ 11.14232751]]\n",
      "Loop  2606 :    Loss_Train:  [[ 12.74028752]]    Loss_Validation:  [[ 11.14235547]]\n",
      "Loop  2607 :    Loss_Train:  [[ 12.74026037]]    Loss_Validation:  [[ 11.14238341]]\n",
      "Loop  2608 :    Loss_Train:  [[ 12.74023326]]    Loss_Validation:  [[ 11.14241134]]\n",
      "Loop  2609 :    Loss_Train:  [[ 12.7402062]]    Loss_Validation:  [[ 11.14243926]]\n",
      "Loop  2610 :    Loss_Train:  [[ 12.74017919]]    Loss_Validation:  [[ 11.14246717]]\n",
      "Loop  2611 :    Loss_Train:  [[ 12.74015222]]    Loss_Validation:  [[ 11.14249506]]\n",
      "Loop  2612 :    Loss_Train:  [[ 12.7401253]]    Loss_Validation:  [[ 11.14252294]]\n",
      "Loop  2613 :    Loss_Train:  [[ 12.74009843]]    Loss_Validation:  [[ 11.1425508]]\n",
      "Loop  2614 :    Loss_Train:  [[ 12.7400716]]    Loss_Validation:  [[ 11.14257865]]\n",
      "Loop  2615 :    Loss_Train:  [[ 12.74004482]]    Loss_Validation:  [[ 11.14260649]]\n",
      "Loop  2616 :    Loss_Train:  [[ 12.74001809]]    Loss_Validation:  [[ 11.14263431]]\n",
      "Loop  2617 :    Loss_Train:  [[ 12.7399914]]    Loss_Validation:  [[ 11.14266212]]\n",
      "Loop  2618 :    Loss_Train:  [[ 12.73996476]]    Loss_Validation:  [[ 11.14268992]]\n",
      "Loop  2619 :    Loss_Train:  [[ 12.73993816]]    Loss_Validation:  [[ 11.1427177]]\n",
      "Loop  2620 :    Loss_Train:  [[ 12.73991162]]    Loss_Validation:  [[ 11.14274547]]\n",
      "Loop  2621 :    Loss_Train:  [[ 12.73988511]]    Loss_Validation:  [[ 11.14277323]]\n",
      "Loop  2622 :    Loss_Train:  [[ 12.73985866]]    Loss_Validation:  [[ 11.14280097]]\n",
      "Loop  2623 :    Loss_Train:  [[ 12.73983225]]    Loss_Validation:  [[ 11.1428287]]\n",
      "Loop  2624 :    Loss_Train:  [[ 12.73980588]]    Loss_Validation:  [[ 11.14285642]]\n",
      "Loop  2625 :    Loss_Train:  [[ 12.73977957]]    Loss_Validation:  [[ 11.14288412]]\n",
      "Loop  2626 :    Loss_Train:  [[ 12.73975329]]    Loss_Validation:  [[ 11.14291181]]\n",
      "Loop  2627 :    Loss_Train:  [[ 12.73972707]]    Loss_Validation:  [[ 11.14293949]]\n",
      "Loop  2628 :    Loss_Train:  [[ 12.73970089]]    Loss_Validation:  [[ 11.14296715]]\n",
      "Loop  2629 :    Loss_Train:  [[ 12.73967475]]    Loss_Validation:  [[ 11.1429948]]\n",
      "Loop  2630 :    Loss_Train:  [[ 12.73964866]]    Loss_Validation:  [[ 11.14302243]]\n",
      "Loop  2631 :    Loss_Train:  [[ 12.73962262]]    Loss_Validation:  [[ 11.14305006]]\n",
      "Loop  2632 :    Loss_Train:  [[ 12.73959662]]    Loss_Validation:  [[ 11.14307766]]\n",
      "Loop  2633 :    Loss_Train:  [[ 12.73957066]]    Loss_Validation:  [[ 11.14310526]]\n",
      "Loop  2634 :    Loss_Train:  [[ 12.73954475]]    Loss_Validation:  [[ 11.14313284]]\n",
      "Loop  2635 :    Loss_Train:  [[ 12.73951889]]    Loss_Validation:  [[ 11.14316041]]\n",
      "Loop  2636 :    Loss_Train:  [[ 12.73949307]]    Loss_Validation:  [[ 11.14318796]]\n",
      "Loop  2637 :    Loss_Train:  [[ 12.73946729]]    Loss_Validation:  [[ 11.1432155]]\n",
      "Loop  2638 :    Loss_Train:  [[ 12.73944156]]    Loss_Validation:  [[ 11.14324303]]\n",
      "Loop  2639 :    Loss_Train:  [[ 12.73941588]]    Loss_Validation:  [[ 11.14327055]]\n",
      "Loop  2640 :    Loss_Train:  [[ 12.73939024]]    Loss_Validation:  [[ 11.14329805]]\n",
      "Loop  2641 :    Loss_Train:  [[ 12.73936464]]    Loss_Validation:  [[ 11.14332553]]\n",
      "Loop  2642 :    Loss_Train:  [[ 12.73933909]]    Loss_Validation:  [[ 11.14335301]]\n",
      "Loop  2643 :    Loss_Train:  [[ 12.73931358]]    Loss_Validation:  [[ 11.14338047]]\n",
      "Loop  2644 :    Loss_Train:  [[ 12.73928812]]    Loss_Validation:  [[ 11.14340791]]\n",
      "Loop  2645 :    Loss_Train:  [[ 12.7392627]]    Loss_Validation:  [[ 11.14343535]]\n",
      "Loop  2646 :    Loss_Train:  [[ 12.73923733]]    Loss_Validation:  [[ 11.14346277]]\n",
      "Loop  2647 :    Loss_Train:  [[ 12.73921199]]    Loss_Validation:  [[ 11.14349017]]\n",
      "Loop  2648 :    Loss_Train:  [[ 12.73918671]]    Loss_Validation:  [[ 11.14351756]]\n",
      "Loop  2649 :    Loss_Train:  [[ 12.73916147]]    Loss_Validation:  [[ 11.14354494]]\n",
      "Loop  2650 :    Loss_Train:  [[ 12.73913627]]    Loss_Validation:  [[ 11.14357231]]\n",
      "Loop  2651 :    Loss_Train:  [[ 12.73911111]]    Loss_Validation:  [[ 11.14359966]]\n",
      "Loop  2652 :    Loss_Train:  [[ 12.739086]]    Loss_Validation:  [[ 11.143627]]\n",
      "Loop  2653 :    Loss_Train:  [[ 12.73906093]]    Loss_Validation:  [[ 11.14365432]]\n",
      "Loop  2654 :    Loss_Train:  [[ 12.73903591]]    Loss_Validation:  [[ 11.14368164]]\n",
      "Loop  2655 :    Loss_Train:  [[ 12.73901093]]    Loss_Validation:  [[ 11.14370893]]\n",
      "Loop  2656 :    Loss_Train:  [[ 12.73898599]]    Loss_Validation:  [[ 11.14373622]]\n",
      "Loop  2657 :    Loss_Train:  [[ 12.73896109]]    Loss_Validation:  [[ 11.14376349]]\n",
      "Loop  2658 :    Loss_Train:  [[ 12.73893624]]    Loss_Validation:  [[ 11.14379075]]\n",
      "Loop  2659 :    Loss_Train:  [[ 12.73891143]]    Loss_Validation:  [[ 11.14381799]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  2660 :    Loss_Train:  [[ 12.73888667]]    Loss_Validation:  [[ 11.14384522]]\n",
      "Loop  2661 :    Loss_Train:  [[ 12.73886195]]    Loss_Validation:  [[ 11.14387244]]\n",
      "Loop  2662 :    Loss_Train:  [[ 12.73883727]]    Loss_Validation:  [[ 11.14389964]]\n",
      "Loop  2663 :    Loss_Train:  [[ 12.73881263]]    Loss_Validation:  [[ 11.14392683]]\n",
      "Loop  2664 :    Loss_Train:  [[ 12.73878804]]    Loss_Validation:  [[ 11.14395401]]\n",
      "Loop  2665 :    Loss_Train:  [[ 12.73876348]]    Loss_Validation:  [[ 11.14398117]]\n",
      "Loop  2666 :    Loss_Train:  [[ 12.73873898]]    Loss_Validation:  [[ 11.14400832]]\n",
      "Loop  2667 :    Loss_Train:  [[ 12.73871451]]    Loss_Validation:  [[ 11.14403545]]\n",
      "Loop  2668 :    Loss_Train:  [[ 12.73869008]]    Loss_Validation:  [[ 11.14406258]]\n",
      "Loop  2669 :    Loss_Train:  [[ 12.7386657]]    Loss_Validation:  [[ 11.14408968]]\n",
      "Loop  2670 :    Loss_Train:  [[ 12.73864136]]    Loss_Validation:  [[ 11.14411678]]\n",
      "Loop  2671 :    Loss_Train:  [[ 12.73861707]]    Loss_Validation:  [[ 11.14414386]]\n",
      "Loop  2672 :    Loss_Train:  [[ 12.73859281]]    Loss_Validation:  [[ 11.14417093]]\n",
      "Loop  2673 :    Loss_Train:  [[ 12.7385686]]    Loss_Validation:  [[ 11.14419798]]\n",
      "Loop  2674 :    Loss_Train:  [[ 12.73854443]]    Loss_Validation:  [[ 11.14422502]]\n",
      "Loop  2675 :    Loss_Train:  [[ 12.7385203]]    Loss_Validation:  [[ 11.14425205]]\n",
      "Loop  2676 :    Loss_Train:  [[ 12.73849621]]    Loss_Validation:  [[ 11.14427907]]\n",
      "Loop  2677 :    Loss_Train:  [[ 12.73847217]]    Loss_Validation:  [[ 11.14430607]]\n",
      "Loop  2678 :    Loss_Train:  [[ 12.73844816]]    Loss_Validation:  [[ 11.14433305]]\n",
      "Loop  2679 :    Loss_Train:  [[ 12.7384242]]    Loss_Validation:  [[ 11.14436003]]\n",
      "Loop  2680 :    Loss_Train:  [[ 12.73840028]]    Loss_Validation:  [[ 11.14438699]]\n",
      "Loop  2681 :    Loss_Train:  [[ 12.7383764]]    Loss_Validation:  [[ 11.14441393]]\n",
      "Loop  2682 :    Loss_Train:  [[ 12.73835256]]    Loss_Validation:  [[ 11.14444086]]\n",
      "Loop  2683 :    Loss_Train:  [[ 12.73832877]]    Loss_Validation:  [[ 11.14446778]]\n",
      "Loop  2684 :    Loss_Train:  [[ 12.73830501]]    Loss_Validation:  [[ 11.14449469]]\n",
      "Loop  2685 :    Loss_Train:  [[ 12.7382813]]    Loss_Validation:  [[ 11.14452158]]\n",
      "Loop  2686 :    Loss_Train:  [[ 12.73825762]]    Loss_Validation:  [[ 11.14454846]]\n",
      "Loop  2687 :    Loss_Train:  [[ 12.73823399]]    Loss_Validation:  [[ 11.14457532]]\n",
      "Loop  2688 :    Loss_Train:  [[ 12.7382104]]    Loss_Validation:  [[ 11.14460218]]\n",
      "Loop  2689 :    Loss_Train:  [[ 12.73818685]]    Loss_Validation:  [[ 11.14462901]]\n",
      "Loop  2690 :    Loss_Train:  [[ 12.73816334]]    Loss_Validation:  [[ 11.14465584]]\n",
      "Loop  2691 :    Loss_Train:  [[ 12.73813987]]    Loss_Validation:  [[ 11.14468265]]\n",
      "Loop  2692 :    Loss_Train:  [[ 12.73811644]]    Loss_Validation:  [[ 11.14470945]]\n",
      "Loop  2693 :    Loss_Train:  [[ 12.73809306]]    Loss_Validation:  [[ 11.14473623]]\n",
      "Loop  2694 :    Loss_Train:  [[ 12.73806971]]    Loss_Validation:  [[ 11.144763]]\n",
      "Loop  2695 :    Loss_Train:  [[ 12.7380464]]    Loss_Validation:  [[ 11.14478976]]\n",
      "Loop  2696 :    Loss_Train:  [[ 12.73802314]]    Loss_Validation:  [[ 11.1448165]]\n",
      "Loop  2697 :    Loss_Train:  [[ 12.73799991]]    Loss_Validation:  [[ 11.14484323]]\n",
      "Loop  2698 :    Loss_Train:  [[ 12.73797672]]    Loss_Validation:  [[ 11.14486994]]\n",
      "Loop  2699 :    Loss_Train:  [[ 12.73795358]]    Loss_Validation:  [[ 11.14489665]]\n",
      "Loop  2700 :    Loss_Train:  [[ 12.73793047]]    Loss_Validation:  [[ 11.14492334]]\n",
      "Loop  2701 :    Loss_Train:  [[ 12.73790741]]    Loss_Validation:  [[ 11.14495001]]\n",
      "Loop  2702 :    Loss_Train:  [[ 12.73788438]]    Loss_Validation:  [[ 11.14497667]]\n",
      "Loop  2703 :    Loss_Train:  [[ 12.73786139]]    Loss_Validation:  [[ 11.14500332]]\n",
      "Loop  2704 :    Loss_Train:  [[ 12.73783845]]    Loss_Validation:  [[ 11.14502996]]\n",
      "Loop  2705 :    Loss_Train:  [[ 12.73781554]]    Loss_Validation:  [[ 11.14505658]]\n",
      "Loop  2706 :    Loss_Train:  [[ 12.73779268]]    Loss_Validation:  [[ 11.14508318]]\n",
      "Loop  2707 :    Loss_Train:  [[ 12.73776985]]    Loss_Validation:  [[ 11.14510978]]\n",
      "Loop  2708 :    Loss_Train:  [[ 12.73774706]]    Loss_Validation:  [[ 11.14513636]]\n",
      "Loop  2709 :    Loss_Train:  [[ 12.73772431]]    Loss_Validation:  [[ 11.14516293]]\n",
      "Loop  2710 :    Loss_Train:  [[ 12.7377016]]    Loss_Validation:  [[ 11.14518948]]\n",
      "Loop  2711 :    Loss_Train:  [[ 12.73767893]]    Loss_Validation:  [[ 11.14521602]]\n",
      "Loop  2712 :    Loss_Train:  [[ 12.7376563]]    Loss_Validation:  [[ 11.14524255]]\n",
      "Loop  2713 :    Loss_Train:  [[ 12.73763371]]    Loss_Validation:  [[ 11.14526906]]\n",
      "Loop  2714 :    Loss_Train:  [[ 12.73761116]]    Loss_Validation:  [[ 11.14529556]]\n",
      "Loop  2715 :    Loss_Train:  [[ 12.73758865]]    Loss_Validation:  [[ 11.14532204]]\n",
      "Loop  2716 :    Loss_Train:  [[ 12.73756617]]    Loss_Validation:  [[ 11.14534852]]\n",
      "Loop  2717 :    Loss_Train:  [[ 12.73754374]]    Loss_Validation:  [[ 11.14537497]]\n",
      "Loop  2718 :    Loss_Train:  [[ 12.73752134]]    Loss_Validation:  [[ 11.14540142]]\n",
      "Loop  2719 :    Loss_Train:  [[ 12.73749898]]    Loss_Validation:  [[ 11.14542785]]\n",
      "Loop  2720 :    Loss_Train:  [[ 12.73747666]]    Loss_Validation:  [[ 11.14545427]]\n",
      "Loop  2721 :    Loss_Train:  [[ 12.73745438]]    Loss_Validation:  [[ 11.14548067]]\n",
      "Loop  2722 :    Loss_Train:  [[ 12.73743214]]    Loss_Validation:  [[ 11.14550706]]\n",
      "Loop  2723 :    Loss_Train:  [[ 12.73740994]]    Loss_Validation:  [[ 11.14553344]]\n",
      "Loop  2724 :    Loss_Train:  [[ 12.73738777]]    Loss_Validation:  [[ 11.14555981]]\n",
      "Loop  2725 :    Loss_Train:  [[ 12.73736564]]    Loss_Validation:  [[ 11.14558616]]\n",
      "Loop  2726 :    Loss_Train:  [[ 12.73734355]]    Loss_Validation:  [[ 11.14561249]]\n",
      "Loop  2727 :    Loss_Train:  [[ 12.7373215]]    Loss_Validation:  [[ 11.14563882]]\n",
      "Loop  2728 :    Loss_Train:  [[ 12.73729949]]    Loss_Validation:  [[ 11.14566513]]\n",
      "Loop  2729 :    Loss_Train:  [[ 12.73727752]]    Loss_Validation:  [[ 11.14569142]]\n",
      "Loop  2730 :    Loss_Train:  [[ 12.73725558]]    Loss_Validation:  [[ 11.14571771]]\n",
      "Loop  2731 :    Loss_Train:  [[ 12.73723368]]    Loss_Validation:  [[ 11.14574397]]\n",
      "Loop  2732 :    Loss_Train:  [[ 12.73721182]]    Loss_Validation:  [[ 11.14577023]]\n",
      "Loop  2733 :    Loss_Train:  [[ 12.73719]]    Loss_Validation:  [[ 11.14579647]]\n",
      "Loop  2734 :    Loss_Train:  [[ 12.73716821]]    Loss_Validation:  [[ 11.1458227]]\n",
      "Loop  2735 :    Loss_Train:  [[ 12.73714647]]    Loss_Validation:  [[ 11.14584892]]\n",
      "Loop  2736 :    Loss_Train:  [[ 12.73712476]]    Loss_Validation:  [[ 11.14587512]]\n",
      "Loop  2737 :    Loss_Train:  [[ 12.73710308]]    Loss_Validation:  [[ 11.14590131]]\n",
      "Loop  2738 :    Loss_Train:  [[ 12.73708145]]    Loss_Validation:  [[ 11.14592748]]\n",
      "Loop  2739 :    Loss_Train:  [[ 12.73705985]]    Loss_Validation:  [[ 11.14595364]]\n",
      "Loop  2740 :    Loss_Train:  [[ 12.73703829]]    Loss_Validation:  [[ 11.14597979]]\n",
      "Loop  2741 :    Loss_Train:  [[ 12.73701677]]    Loss_Validation:  [[ 11.14600592]]\n",
      "Loop  2742 :    Loss_Train:  [[ 12.73699528]]    Loss_Validation:  [[ 11.14603204]]\n",
      "Loop  2743 :    Loss_Train:  [[ 12.73697383]]    Loss_Validation:  [[ 11.14605815]]\n",
      "Loop  2744 :    Loss_Train:  [[ 12.73695242]]    Loss_Validation:  [[ 11.14608424]]\n",
      "Loop  2745 :    Loss_Train:  [[ 12.73693105]]    Loss_Validation:  [[ 11.14611032]]\n",
      "Loop  2746 :    Loss_Train:  [[ 12.73690971]]    Loss_Validation:  [[ 11.14613639]]\n",
      "Loop  2747 :    Loss_Train:  [[ 12.73688841]]    Loss_Validation:  [[ 11.14616244]]\n",
      "Loop  2748 :    Loss_Train:  [[ 12.73686714]]    Loss_Validation:  [[ 11.14618848]]\n",
      "Loop  2749 :    Loss_Train:  [[ 12.73684592]]    Loss_Validation:  [[ 11.14621451]]\n",
      "Loop  2750 :    Loss_Train:  [[ 12.73682473]]    Loss_Validation:  [[ 11.14624052]]\n",
      "Loop  2751 :    Loss_Train:  [[ 12.73680357]]    Loss_Validation:  [[ 11.14626652]]\n",
      "Loop  2752 :    Loss_Train:  [[ 12.73678245]]    Loss_Validation:  [[ 11.1462925]]\n",
      "Loop  2753 :    Loss_Train:  [[ 12.73676137]]    Loss_Validation:  [[ 11.14631847]]\n",
      "Loop  2754 :    Loss_Train:  [[ 12.73674033]]    Loss_Validation:  [[ 11.14634443]]\n",
      "Loop  2755 :    Loss_Train:  [[ 12.73671932]]    Loss_Validation:  [[ 11.14637038]]\n",
      "Loop  2756 :    Loss_Train:  [[ 12.73669835]]    Loss_Validation:  [[ 11.14639631]]\n",
      "Loop  2757 :    Loss_Train:  [[ 12.73667741]]    Loss_Validation:  [[ 11.14642223]]\n",
      "Loop  2758 :    Loss_Train:  [[ 12.73665651]]    Loss_Validation:  [[ 11.14644813]]\n",
      "Loop  2759 :    Loss_Train:  [[ 12.73663565]]    Loss_Validation:  [[ 11.14647402]]\n",
      "Loop  2760 :    Loss_Train:  [[ 12.73661482]]    Loss_Validation:  [[ 11.1464999]]\n",
      "Loop  2761 :    Loss_Train:  [[ 12.73659403]]    Loss_Validation:  [[ 11.14652576]]\n",
      "Loop  2762 :    Loss_Train:  [[ 12.73657327]]    Loss_Validation:  [[ 11.14655161]]\n",
      "Loop  2763 :    Loss_Train:  [[ 12.73655255]]    Loss_Validation:  [[ 11.14657745]]\n",
      "Loop  2764 :    Loss_Train:  [[ 12.73653187]]    Loss_Validation:  [[ 11.14660327]]\n",
      "Loop  2765 :    Loss_Train:  [[ 12.73651122]]    Loss_Validation:  [[ 11.14662908]]\n",
      "Loop  2766 :    Loss_Train:  [[ 12.7364906]]    Loss_Validation:  [[ 11.14665488]]\n",
      "Loop  2767 :    Loss_Train:  [[ 12.73647003]]    Loss_Validation:  [[ 11.14668066]]\n",
      "Loop  2768 :    Loss_Train:  [[ 12.73644948]]    Loss_Validation:  [[ 11.14670643]]\n",
      "Loop  2769 :    Loss_Train:  [[ 12.73642898]]    Loss_Validation:  [[ 11.14673218]]\n",
      "Loop  2770 :    Loss_Train:  [[ 12.73640851]]    Loss_Validation:  [[ 11.14675793]]\n",
      "Loop  2771 :    Loss_Train:  [[ 12.73638807]]    Loss_Validation:  [[ 11.14678365]]\n",
      "Loop  2772 :    Loss_Train:  [[ 12.73636767]]    Loss_Validation:  [[ 11.14680937]]\n",
      "Loop  2773 :    Loss_Train:  [[ 12.7363473]]    Loss_Validation:  [[ 11.14683507]]\n",
      "Loop  2774 :    Loss_Train:  [[ 12.73632697]]    Loss_Validation:  [[ 11.14686076]]\n",
      "Loop  2775 :    Loss_Train:  [[ 12.73630668]]    Loss_Validation:  [[ 11.14688643]]\n",
      "Loop  2776 :    Loss_Train:  [[ 12.73628642]]    Loss_Validation:  [[ 11.14691209]]\n",
      "Loop  2777 :    Loss_Train:  [[ 12.73626619]]    Loss_Validation:  [[ 11.14693774]]\n",
      "Loop  2778 :    Loss_Train:  [[ 12.736246]]    Loss_Validation:  [[ 11.14696338]]\n",
      "Loop  2779 :    Loss_Train:  [[ 12.73622585]]    Loss_Validation:  [[ 11.146989]]\n",
      "Loop  2780 :    Loss_Train:  [[ 12.73620572]]    Loss_Validation:  [[ 11.1470146]]\n",
      "Loop  2781 :    Loss_Train:  [[ 12.73618564]]    Loss_Validation:  [[ 11.1470402]]\n",
      "Loop  2782 :    Loss_Train:  [[ 12.73616559]]    Loss_Validation:  [[ 11.14706578]]\n",
      "Loop  2783 :    Loss_Train:  [[ 12.73614557]]    Loss_Validation:  [[ 11.14709135]]\n",
      "Loop  2784 :    Loss_Train:  [[ 12.73612559]]    Loss_Validation:  [[ 11.1471169]]\n",
      "Loop  2785 :    Loss_Train:  [[ 12.73610564]]    Loss_Validation:  [[ 11.14714244]]\n",
      "Loop  2786 :    Loss_Train:  [[ 12.73608573]]    Loss_Validation:  [[ 11.14716797]]\n",
      "Loop  2787 :    Loss_Train:  [[ 12.73606585]]    Loss_Validation:  [[ 11.14719348]]\n",
      "Loop  2788 :    Loss_Train:  [[ 12.736046]]    Loss_Validation:  [[ 11.14721898]]\n",
      "Loop  2789 :    Loss_Train:  [[ 12.73602619]]    Loss_Validation:  [[ 11.14724446]]\n",
      "Loop  2790 :    Loss_Train:  [[ 12.73600641]]    Loss_Validation:  [[ 11.14726994]]\n",
      "Loop  2791 :    Loss_Train:  [[ 12.73598667]]    Loss_Validation:  [[ 11.1472954]]\n",
      "Loop  2792 :    Loss_Train:  [[ 12.73596696]]    Loss_Validation:  [[ 11.14732084]]\n",
      "Loop  2793 :    Loss_Train:  [[ 12.73594729]]    Loss_Validation:  [[ 11.14734627]]\n",
      "Loop  2794 :    Loss_Train:  [[ 12.73592765]]    Loss_Validation:  [[ 11.14737169]]\n",
      "Loop  2795 :    Loss_Train:  [[ 12.73590804]]    Loss_Validation:  [[ 11.1473971]]\n",
      "Loop  2796 :    Loss_Train:  [[ 12.73588847]]    Loss_Validation:  [[ 11.14742249]]\n",
      "Loop  2797 :    Loss_Train:  [[ 12.73586893]]    Loss_Validation:  [[ 11.14744787]]\n",
      "Loop  2798 :    Loss_Train:  [[ 12.73584942]]    Loss_Validation:  [[ 11.14747323]]\n",
      "Loop  2799 :    Loss_Train:  [[ 12.73582995]]    Loss_Validation:  [[ 11.14749858]]\n",
      "Loop  2800 :    Loss_Train:  [[ 12.73581051]]    Loss_Validation:  [[ 11.14752392]]\n",
      "Loop  2801 :    Loss_Train:  [[ 12.7357911]]    Loss_Validation:  [[ 11.14754925]]\n",
      "Loop  2802 :    Loss_Train:  [[ 12.73577173]]    Loss_Validation:  [[ 11.14757456]]\n",
      "Loop  2803 :    Loss_Train:  [[ 12.73575239]]    Loss_Validation:  [[ 11.14759986]]\n",
      "Loop  2804 :    Loss_Train:  [[ 12.73573309]]    Loss_Validation:  [[ 11.14762514]]\n",
      "Loop  2805 :    Loss_Train:  [[ 12.73571382]]    Loss_Validation:  [[ 11.14765041]]\n",
      "Loop  2806 :    Loss_Train:  [[ 12.73569458]]    Loss_Validation:  [[ 11.14767567]]\n",
      "Loop  2807 :    Loss_Train:  [[ 12.73567537]]    Loss_Validation:  [[ 11.14770091]]\n",
      "Loop  2808 :    Loss_Train:  [[ 12.7356562]]    Loss_Validation:  [[ 11.14772614]]\n",
      "Loop  2809 :    Loss_Train:  [[ 12.73563706]]    Loss_Validation:  [[ 11.14775136]]\n",
      "Loop  2810 :    Loss_Train:  [[ 12.73561795]]    Loss_Validation:  [[ 11.14777656]]\n",
      "Loop  2811 :    Loss_Train:  [[ 12.73559888]]    Loss_Validation:  [[ 11.14780175]]\n",
      "Loop  2812 :    Loss_Train:  [[ 12.73557984]]    Loss_Validation:  [[ 11.14782693]]\n",
      "Loop  2813 :    Loss_Train:  [[ 12.73556083]]    Loss_Validation:  [[ 11.14785209]]\n",
      "Loop  2814 :    Loss_Train:  [[ 12.73554186]]    Loss_Validation:  [[ 11.14787724]]\n",
      "Loop  2815 :    Loss_Train:  [[ 12.73552291]]    Loss_Validation:  [[ 11.14790238]]\n",
      "Loop  2816 :    Loss_Train:  [[ 12.735504]]    Loss_Validation:  [[ 11.1479275]]\n",
      "Loop  2817 :    Loss_Train:  [[ 12.73548513]]    Loss_Validation:  [[ 11.14795261]]\n",
      "Loop  2818 :    Loss_Train:  [[ 12.73546628]]    Loss_Validation:  [[ 11.14797771]]\n",
      "Loop  2819 :    Loss_Train:  [[ 12.73544747]]    Loss_Validation:  [[ 11.14800279]]\n",
      "Loop  2820 :    Loss_Train:  [[ 12.73542869]]    Loss_Validation:  [[ 11.14802786]]\n",
      "Loop  2821 :    Loss_Train:  [[ 12.73540994]]    Loss_Validation:  [[ 11.14805292]]\n",
      "Loop  2822 :    Loss_Train:  [[ 12.73539122]]    Loss_Validation:  [[ 11.14807796]]\n",
      "Loop  2823 :    Loss_Train:  [[ 12.73537254]]    Loss_Validation:  [[ 11.14810299]]\n",
      "Loop  2824 :    Loss_Train:  [[ 12.73535389]]    Loss_Validation:  [[ 11.14812801]]\n",
      "Loop  2825 :    Loss_Train:  [[ 12.73533527]]    Loss_Validation:  [[ 11.14815301]]\n",
      "Loop  2826 :    Loss_Train:  [[ 12.73531668]]    Loss_Validation:  [[ 11.148178]]\n",
      "Loop  2827 :    Loss_Train:  [[ 12.73529813]]    Loss_Validation:  [[ 11.14820297]]\n",
      "Loop  2828 :    Loss_Train:  [[ 12.7352796]]    Loss_Validation:  [[ 11.14822794]]\n",
      "Loop  2829 :    Loss_Train:  [[ 12.73526111]]    Loss_Validation:  [[ 11.14825289]]\n",
      "Loop  2830 :    Loss_Train:  [[ 12.73524265]]    Loss_Validation:  [[ 11.14827782]]\n",
      "Loop  2831 :    Loss_Train:  [[ 12.73522423]]    Loss_Validation:  [[ 11.14830274]]\n",
      "Loop  2832 :    Loss_Train:  [[ 12.73520583]]    Loss_Validation:  [[ 11.14832765]]\n",
      "Loop  2833 :    Loss_Train:  [[ 12.73518746]]    Loss_Validation:  [[ 11.14835255]]\n",
      "Loop  2834 :    Loss_Train:  [[ 12.73516913]]    Loss_Validation:  [[ 11.14837743]]\n",
      "Loop  2835 :    Loss_Train:  [[ 12.73515083]]    Loss_Validation:  [[ 11.1484023]]\n",
      "Loop  2836 :    Loss_Train:  [[ 12.73513256]]    Loss_Validation:  [[ 11.14842715]]\n",
      "Loop  2837 :    Loss_Train:  [[ 12.73511432]]    Loss_Validation:  [[ 11.148452]]\n",
      "Loop  2838 :    Loss_Train:  [[ 12.73509611]]    Loss_Validation:  [[ 11.14847683]]\n",
      "Loop  2839 :    Loss_Train:  [[ 12.73507794]]    Loss_Validation:  [[ 11.14850164]]\n",
      "Loop  2840 :    Loss_Train:  [[ 12.73505979]]    Loss_Validation:  [[ 11.14852644]]\n",
      "Loop  2841 :    Loss_Train:  [[ 12.73504168]]    Loss_Validation:  [[ 11.14855123]]\n",
      "Loop  2842 :    Loss_Train:  [[ 12.7350236]]    Loss_Validation:  [[ 11.14857601]]\n",
      "Loop  2843 :    Loss_Train:  [[ 12.73500555]]    Loss_Validation:  [[ 11.14860077]]\n",
      "Loop  2844 :    Loss_Train:  [[ 12.73498753]]    Loss_Validation:  [[ 11.14862552]]\n",
      "Loop  2845 :    Loss_Train:  [[ 12.73496954]]    Loss_Validation:  [[ 11.14865025]]\n",
      "Loop  2846 :    Loss_Train:  [[ 12.73495158]]    Loss_Validation:  [[ 11.14867497]]\n",
      "Loop  2847 :    Loss_Train:  [[ 12.73493365]]    Loss_Validation:  [[ 11.14869968]]\n",
      "Loop  2848 :    Loss_Train:  [[ 12.73491576]]    Loss_Validation:  [[ 11.14872438]]\n",
      "Loop  2849 :    Loss_Train:  [[ 12.73489789]]    Loss_Validation:  [[ 11.14874906]]\n",
      "Loop  2850 :    Loss_Train:  [[ 12.73488006]]    Loss_Validation:  [[ 11.14877373]]\n",
      "Loop  2851 :    Loss_Train:  [[ 12.73486225]]    Loss_Validation:  [[ 11.14879838]]\n",
      "Loop  2852 :    Loss_Train:  [[ 12.73484448]]    Loss_Validation:  [[ 11.14882303]]\n",
      "Loop  2853 :    Loss_Train:  [[ 12.73482673]]    Loss_Validation:  [[ 11.14884765]]\n",
      "Loop  2854 :    Loss_Train:  [[ 12.73480902]]    Loss_Validation:  [[ 11.14887227]]\n",
      "Loop  2855 :    Loss_Train:  [[ 12.73479134]]    Loss_Validation:  [[ 11.14889687]]\n",
      "Loop  2856 :    Loss_Train:  [[ 12.73477369]]    Loss_Validation:  [[ 11.14892146]]\n",
      "Loop  2857 :    Loss_Train:  [[ 12.73475607]]    Loss_Validation:  [[ 11.14894603]]\n",
      "Loop  2858 :    Loss_Train:  [[ 12.73473847]]    Loss_Validation:  [[ 11.1489706]]\n",
      "Loop  2859 :    Loss_Train:  [[ 12.73472091]]    Loss_Validation:  [[ 11.14899515]]\n",
      "Loop  2860 :    Loss_Train:  [[ 12.73470338]]    Loss_Validation:  [[ 11.14901968]]\n",
      "Loop  2861 :    Loss_Train:  [[ 12.73468588]]    Loss_Validation:  [[ 11.1490442]]\n",
      "Loop  2862 :    Loss_Train:  [[ 12.73466841]]    Loss_Validation:  [[ 11.14906871]]\n",
      "Loop  2863 :    Loss_Train:  [[ 12.73465097]]    Loss_Validation:  [[ 11.14909321]]\n",
      "Loop  2864 :    Loss_Train:  [[ 12.73463356]]    Loss_Validation:  [[ 11.14911769]]\n",
      "Loop  2865 :    Loss_Train:  [[ 12.73461618]]    Loss_Validation:  [[ 11.14914216]]\n",
      "Loop  2866 :    Loss_Train:  [[ 12.73459883]]    Loss_Validation:  [[ 11.14916661]]\n",
      "Loop  2867 :    Loss_Train:  [[ 12.73458151]]    Loss_Validation:  [[ 11.14919106]]\n",
      "Loop  2868 :    Loss_Train:  [[ 12.73456421]]    Loss_Validation:  [[ 11.14921548]]\n",
      "Loop  2869 :    Loss_Train:  [[ 12.73454695]]    Loss_Validation:  [[ 11.1492399]]\n",
      "Loop  2870 :    Loss_Train:  [[ 12.73452972]]    Loss_Validation:  [[ 11.1492643]]\n",
      "Loop  2871 :    Loss_Train:  [[ 12.73451252]]    Loss_Validation:  [[ 11.14928869]]\n",
      "Loop  2872 :    Loss_Train:  [[ 12.73449534]]    Loss_Validation:  [[ 11.14931307]]\n",
      "Loop  2873 :    Loss_Train:  [[ 12.7344782]]    Loss_Validation:  [[ 11.14933743]]\n",
      "Loop  2874 :    Loss_Train:  [[ 12.73446109]]    Loss_Validation:  [[ 11.14936178]]\n",
      "Loop  2875 :    Loss_Train:  [[ 12.734444]]    Loss_Validation:  [[ 11.14938612]]\n",
      "Loop  2876 :    Loss_Train:  [[ 12.73442695]]    Loss_Validation:  [[ 11.14941044]]\n",
      "Loop  2877 :    Loss_Train:  [[ 12.73440992]]    Loss_Validation:  [[ 11.14943475]]\n",
      "Loop  2878 :    Loss_Train:  [[ 12.73439292]]    Loss_Validation:  [[ 11.14945904]]\n",
      "Loop  2879 :    Loss_Train:  [[ 12.73437596]]    Loss_Validation:  [[ 11.14948333]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  2880 :    Loss_Train:  [[ 12.73435902]]    Loss_Validation:  [[ 11.1495076]]\n",
      "Loop  2881 :    Loss_Train:  [[ 12.73434211]]    Loss_Validation:  [[ 11.14953185]]\n",
      "Loop  2882 :    Loss_Train:  [[ 12.73432523]]    Loss_Validation:  [[ 11.1495561]]\n",
      "Loop  2883 :    Loss_Train:  [[ 12.73430837]]    Loss_Validation:  [[ 11.14958033]]\n",
      "Loop  2884 :    Loss_Train:  [[ 12.73429155]]    Loss_Validation:  [[ 11.14960454]]\n",
      "Loop  2885 :    Loss_Train:  [[ 12.73427476]]    Loss_Validation:  [[ 11.14962875]]\n",
      "Loop  2886 :    Loss_Train:  [[ 12.73425799]]    Loss_Validation:  [[ 11.14965294]]\n",
      "Loop  2887 :    Loss_Train:  [[ 12.73424126]]    Loss_Validation:  [[ 11.14967711]]\n",
      "Loop  2888 :    Loss_Train:  [[ 12.73422455]]    Loss_Validation:  [[ 11.14970128]]\n",
      "Loop  2889 :    Loss_Train:  [[ 12.73420787]]    Loss_Validation:  [[ 11.14972543]]\n",
      "Loop  2890 :    Loss_Train:  [[ 12.73419122]]    Loss_Validation:  [[ 11.14974957]]\n",
      "Loop  2891 :    Loss_Train:  [[ 12.7341746]]    Loss_Validation:  [[ 11.14977369]]\n",
      "Loop  2892 :    Loss_Train:  [[ 12.73415801]]    Loss_Validation:  [[ 11.1497978]]\n",
      "Loop  2893 :    Loss_Train:  [[ 12.73414144]]    Loss_Validation:  [[ 11.1498219]]\n",
      "Loop  2894 :    Loss_Train:  [[ 12.7341249]]    Loss_Validation:  [[ 11.14984598]]\n",
      "Loop  2895 :    Loss_Train:  [[ 12.7341084]]    Loss_Validation:  [[ 11.14987005]]\n",
      "Loop  2896 :    Loss_Train:  [[ 12.73409192]]    Loss_Validation:  [[ 11.14989411]]\n",
      "Loop  2897 :    Loss_Train:  [[ 12.73407547]]    Loss_Validation:  [[ 11.14991816]]\n",
      "Loop  2898 :    Loss_Train:  [[ 12.73405904]]    Loss_Validation:  [[ 11.14994219]]\n",
      "Loop  2899 :    Loss_Train:  [[ 12.73404265]]    Loss_Validation:  [[ 11.14996621]]\n",
      "Loop  2900 :    Loss_Train:  [[ 12.73402628]]    Loss_Validation:  [[ 11.14999021]]\n",
      "Loop  2901 :    Loss_Train:  [[ 12.73400994]]    Loss_Validation:  [[ 11.15001421]]\n",
      "Loop  2902 :    Loss_Train:  [[ 12.73399363]]    Loss_Validation:  [[ 11.15003818]]\n",
      "Loop  2903 :    Loss_Train:  [[ 12.73397735]]    Loss_Validation:  [[ 11.15006215]]\n",
      "Loop  2904 :    Loss_Train:  [[ 12.73396109]]    Loss_Validation:  [[ 11.1500861]]\n",
      "Loop  2905 :    Loss_Train:  [[ 12.73394487]]    Loss_Validation:  [[ 11.15011004]]\n",
      "Loop  2906 :    Loss_Train:  [[ 12.73392867]]    Loss_Validation:  [[ 11.15013397]]\n",
      "Loop  2907 :    Loss_Train:  [[ 12.7339125]]    Loss_Validation:  [[ 11.15015788]]\n",
      "Loop  2908 :    Loss_Train:  [[ 12.73389635]]    Loss_Validation:  [[ 11.15018178]]\n",
      "Loop  2909 :    Loss_Train:  [[ 12.73388024]]    Loss_Validation:  [[ 11.15020567]]\n",
      "Loop  2910 :    Loss_Train:  [[ 12.73386415]]    Loss_Validation:  [[ 11.15022954]]\n",
      "Loop  2911 :    Loss_Train:  [[ 12.73384809]]    Loss_Validation:  [[ 11.1502534]]\n",
      "Loop  2912 :    Loss_Train:  [[ 12.73383205]]    Loss_Validation:  [[ 11.15027725]]\n",
      "Loop  2913 :    Loss_Train:  [[ 12.73381605]]    Loss_Validation:  [[ 11.15030108]]\n",
      "Loop  2914 :    Loss_Train:  [[ 12.73380007]]    Loss_Validation:  [[ 11.15032491]]\n",
      "Loop  2915 :    Loss_Train:  [[ 12.73378412]]    Loss_Validation:  [[ 11.15034871]]\n",
      "Loop  2916 :    Loss_Train:  [[ 12.7337682]]    Loss_Validation:  [[ 11.15037251]]\n",
      "Loop  2917 :    Loss_Train:  [[ 12.7337523]]    Loss_Validation:  [[ 11.15039629]]\n",
      "Loop  2918 :    Loss_Train:  [[ 12.73373643]]    Loss_Validation:  [[ 11.15042006]]\n",
      "Loop  2919 :    Loss_Train:  [[ 12.73372059]]    Loss_Validation:  [[ 11.15044381]]\n",
      "Loop  2920 :    Loss_Train:  [[ 12.73370477]]    Loss_Validation:  [[ 11.15046756]]\n",
      "Loop  2921 :    Loss_Train:  [[ 12.73368899]]    Loss_Validation:  [[ 11.15049128]]\n",
      "Loop  2922 :    Loss_Train:  [[ 12.73367323]]    Loss_Validation:  [[ 11.150515]]\n",
      "Loop  2923 :    Loss_Train:  [[ 12.73365749]]    Loss_Validation:  [[ 11.1505387]]\n",
      "Loop  2924 :    Loss_Train:  [[ 12.73364178]]    Loss_Validation:  [[ 11.15056239]]\n",
      "Loop  2925 :    Loss_Train:  [[ 12.7336261]]    Loss_Validation:  [[ 11.15058607]]\n",
      "Loop  2926 :    Loss_Train:  [[ 12.73361045]]    Loss_Validation:  [[ 11.15060973]]\n",
      "Loop  2927 :    Loss_Train:  [[ 12.73359483]]    Loss_Validation:  [[ 11.15063338]]\n",
      "Loop  2928 :    Loss_Train:  [[ 12.73357923]]    Loss_Validation:  [[ 11.15065702]]\n",
      "Loop  2929 :    Loss_Train:  [[ 12.73356365]]    Loss_Validation:  [[ 11.15068065]]\n",
      "Loop  2930 :    Loss_Train:  [[ 12.73354811]]    Loss_Validation:  [[ 11.15070426]]\n",
      "Loop  2931 :    Loss_Train:  [[ 12.73353259]]    Loss_Validation:  [[ 11.15072785]]\n",
      "Loop  2932 :    Loss_Train:  [[ 12.7335171]]    Loss_Validation:  [[ 11.15075144]]\n",
      "Loop  2933 :    Loss_Train:  [[ 12.73350163]]    Loss_Validation:  [[ 11.15077501]]\n",
      "Loop  2934 :    Loss_Train:  [[ 12.73348619]]    Loss_Validation:  [[ 11.15079857]]\n",
      "Loop  2935 :    Loss_Train:  [[ 12.73347078]]    Loss_Validation:  [[ 11.15082212]]\n",
      "Loop  2936 :    Loss_Train:  [[ 12.73345539]]    Loss_Validation:  [[ 11.15084565]]\n",
      "Loop  2937 :    Loss_Train:  [[ 12.73344003]]    Loss_Validation:  [[ 11.15086917]]\n",
      "Loop  2938 :    Loss_Train:  [[ 12.7334247]]    Loss_Validation:  [[ 11.15089267]]\n",
      "Loop  2939 :    Loss_Train:  [[ 12.73340939]]    Loss_Validation:  [[ 11.15091617]]\n",
      "Loop  2940 :    Loss_Train:  [[ 12.73339411]]    Loss_Validation:  [[ 11.15093965]]\n",
      "Loop  2941 :    Loss_Train:  [[ 12.73337885]]    Loss_Validation:  [[ 11.15096312]]\n",
      "Loop  2942 :    Loss_Train:  [[ 12.73336362]]    Loss_Validation:  [[ 11.15098657]]\n",
      "Loop  2943 :    Loss_Train:  [[ 12.73334842]]    Loss_Validation:  [[ 11.15101001]]\n",
      "Loop  2944 :    Loss_Train:  [[ 12.73333324]]    Loss_Validation:  [[ 11.15103344]]\n",
      "Loop  2945 :    Loss_Train:  [[ 12.73331809]]    Loss_Validation:  [[ 11.15105686]]\n",
      "Loop  2946 :    Loss_Train:  [[ 12.73330296]]    Loss_Validation:  [[ 11.15108026]]\n",
      "Loop  2947 :    Loss_Train:  [[ 12.73328786]]    Loss_Validation:  [[ 11.15110365]]\n",
      "Loop  2948 :    Loss_Train:  [[ 12.73327279]]    Loss_Validation:  [[ 11.15112702]]\n",
      "Loop  2949 :    Loss_Train:  [[ 12.73325774]]    Loss_Validation:  [[ 11.15115039]]\n",
      "Loop  2950 :    Loss_Train:  [[ 12.73324272]]    Loss_Validation:  [[ 11.15117374]]\n",
      "Loop  2951 :    Loss_Train:  [[ 12.73322772]]    Loss_Validation:  [[ 11.15119707]]\n",
      "Loop  2952 :    Loss_Train:  [[ 12.73321275]]    Loss_Validation:  [[ 11.1512204]]\n",
      "Loop  2953 :    Loss_Train:  [[ 12.73319781]]    Loss_Validation:  [[ 11.15124371]]\n",
      "Loop  2954 :    Loss_Train:  [[ 12.73318289]]    Loss_Validation:  [[ 11.15126701]]\n",
      "Loop  2955 :    Loss_Train:  [[ 12.73316799]]    Loss_Validation:  [[ 11.15129029]]\n",
      "Loop  2956 :    Loss_Train:  [[ 12.73315312]]    Loss_Validation:  [[ 11.15131356]]\n",
      "Loop  2957 :    Loss_Train:  [[ 12.73313828]]    Loss_Validation:  [[ 11.15133682]]\n",
      "Loop  2958 :    Loss_Train:  [[ 12.73312346]]    Loss_Validation:  [[ 11.15136007]]\n",
      "Loop  2959 :    Loss_Train:  [[ 12.73310867]]    Loss_Validation:  [[ 11.1513833]]\n",
      "Loop  2960 :    Loss_Train:  [[ 12.7330939]]    Loss_Validation:  [[ 11.15140652]]\n",
      "Loop  2961 :    Loss_Train:  [[ 12.73307916]]    Loss_Validation:  [[ 11.15142973]]\n",
      "Loop  2962 :    Loss_Train:  [[ 12.73306444]]    Loss_Validation:  [[ 11.15145292]]\n",
      "Loop  2963 :    Loss_Train:  [[ 12.73304975]]    Loss_Validation:  [[ 11.1514761]]\n",
      "Loop  2964 :    Loss_Train:  [[ 12.73303509]]    Loss_Validation:  [[ 11.15149927]]\n",
      "Loop  2965 :    Loss_Train:  [[ 12.73302045]]    Loss_Validation:  [[ 11.15152243]]\n",
      "Loop  2966 :    Loss_Train:  [[ 12.73300583]]    Loss_Validation:  [[ 11.15154557]]\n",
      "Loop  2967 :    Loss_Train:  [[ 12.73299124]]    Loss_Validation:  [[ 11.1515687]]\n",
      "Loop  2968 :    Loss_Train:  [[ 12.73297667]]    Loss_Validation:  [[ 11.15159181]]\n",
      "Loop  2969 :    Loss_Train:  [[ 12.73296213]]    Loss_Validation:  [[ 11.15161492]]\n",
      "Loop  2970 :    Loss_Train:  [[ 12.73294761]]    Loss_Validation:  [[ 11.15163801]]\n",
      "Loop  2971 :    Loss_Train:  [[ 12.73293312]]    Loss_Validation:  [[ 11.15166108]]\n",
      "Loop  2972 :    Loss_Train:  [[ 12.73291865]]    Loss_Validation:  [[ 11.15168415]]\n",
      "Loop  2973 :    Loss_Train:  [[ 12.73290421]]    Loss_Validation:  [[ 11.1517072]]\n",
      "Loop  2974 :    Loss_Train:  [[ 12.73288979]]    Loss_Validation:  [[ 11.15173024]]\n",
      "Loop  2975 :    Loss_Train:  [[ 12.7328754]]    Loss_Validation:  [[ 11.15175326]]\n",
      "Loop  2976 :    Loss_Train:  [[ 12.73286103]]    Loss_Validation:  [[ 11.15177628]]\n",
      "Loop  2977 :    Loss_Train:  [[ 12.73284669]]    Loss_Validation:  [[ 11.15179928]]\n",
      "Loop  2978 :    Loss_Train:  [[ 12.73283237]]    Loss_Validation:  [[ 11.15182226]]\n",
      "Loop  2979 :    Loss_Train:  [[ 12.73281808]]    Loss_Validation:  [[ 11.15184524]]\n",
      "Loop  2980 :    Loss_Train:  [[ 12.73280381]]    Loss_Validation:  [[ 11.1518682]]\n",
      "Loop  2981 :    Loss_Train:  [[ 12.73278956]]    Loss_Validation:  [[ 11.15189115]]\n",
      "Loop  2982 :    Loss_Train:  [[ 12.73277534]]    Loss_Validation:  [[ 11.15191408]]\n",
      "Loop  2983 :    Loss_Train:  [[ 12.73276114]]    Loss_Validation:  [[ 11.15193701]]\n",
      "Loop  2984 :    Loss_Train:  [[ 12.73274697]]    Loss_Validation:  [[ 11.15195991]]\n",
      "Loop  2985 :    Loss_Train:  [[ 12.73273282]]    Loss_Validation:  [[ 11.15198281]]\n",
      "Loop  2986 :    Loss_Train:  [[ 12.73271869]]    Loss_Validation:  [[ 11.1520057]]\n",
      "Loop  2987 :    Loss_Train:  [[ 12.73270459]]    Loss_Validation:  [[ 11.15202857]]\n",
      "Loop  2988 :    Loss_Train:  [[ 12.73269052]]    Loss_Validation:  [[ 11.15205142]]\n",
      "Loop  2989 :    Loss_Train:  [[ 12.73267647]]    Loss_Validation:  [[ 11.15207427]]\n",
      "Loop  2990 :    Loss_Train:  [[ 12.73266244]]    Loss_Validation:  [[ 11.1520971]]\n",
      "Loop  2991 :    Loss_Train:  [[ 12.73264843]]    Loss_Validation:  [[ 11.15211992]]\n",
      "Loop  2992 :    Loss_Train:  [[ 12.73263445]]    Loss_Validation:  [[ 11.15214273]]\n",
      "Loop  2993 :    Loss_Train:  [[ 12.7326205]]    Loss_Validation:  [[ 11.15216552]]\n",
      "Loop  2994 :    Loss_Train:  [[ 12.73260656]]    Loss_Validation:  [[ 11.1521883]]\n",
      "Loop  2995 :    Loss_Train:  [[ 12.73259265]]    Loss_Validation:  [[ 11.15221107]]\n",
      "Loop  2996 :    Loss_Train:  [[ 12.73257877]]    Loss_Validation:  [[ 11.15223382]]\n",
      "Loop  2997 :    Loss_Train:  [[ 12.73256491]]    Loss_Validation:  [[ 11.15225657]]\n",
      "Loop  2998 :    Loss_Train:  [[ 12.73255107]]    Loss_Validation:  [[ 11.1522793]]\n",
      "Loop  2999 :    Loss_Train:  [[ 12.73253726]]    Loss_Validation:  [[ 11.15230201]]\n",
      "Loop  3000 :    Loss_Train:  [[ 12.73252346]]    Loss_Validation:  [[ 11.15232472]]\n",
      "Loop  3001 :    Loss_Train:  [[ 12.7325097]]    Loss_Validation:  [[ 11.15234741]]\n",
      "Loop  3002 :    Loss_Train:  [[ 12.73249595]]    Loss_Validation:  [[ 11.15237009]]\n",
      "Loop  3003 :    Loss_Train:  [[ 12.73248223]]    Loss_Validation:  [[ 11.15239275]]\n",
      "Loop  3004 :    Loss_Train:  [[ 12.73246854]]    Loss_Validation:  [[ 11.1524154]]\n",
      "Loop  3005 :    Loss_Train:  [[ 12.73245486]]    Loss_Validation:  [[ 11.15243804]]\n",
      "Loop  3006 :    Loss_Train:  [[ 12.73244121]]    Loss_Validation:  [[ 11.15246067]]\n",
      "Loop  3007 :    Loss_Train:  [[ 12.73242759]]    Loss_Validation:  [[ 11.15248328]]\n",
      "Loop  3008 :    Loss_Train:  [[ 12.73241398]]    Loss_Validation:  [[ 11.15250588]]\n",
      "Loop  3009 :    Loss_Train:  [[ 12.7324004]]    Loss_Validation:  [[ 11.15252847]]\n",
      "Loop  3010 :    Loss_Train:  [[ 12.73238685]]    Loss_Validation:  [[ 11.15255105]]\n",
      "Loop  3011 :    Loss_Train:  [[ 12.73237331]]    Loss_Validation:  [[ 11.15257361]]\n",
      "Loop  3012 :    Loss_Train:  [[ 12.7323598]]    Loss_Validation:  [[ 11.15259616]]\n",
      "Loop  3013 :    Loss_Train:  [[ 12.73234632]]    Loss_Validation:  [[ 11.1526187]]\n",
      "Loop  3014 :    Loss_Train:  [[ 12.73233285]]    Loss_Validation:  [[ 11.15264122]]\n",
      "Loop  3015 :    Loss_Train:  [[ 12.73231941]]    Loss_Validation:  [[ 11.15266373]]\n",
      "Loop  3016 :    Loss_Train:  [[ 12.73230599]]    Loss_Validation:  [[ 11.15268623]]\n",
      "Loop  3017 :    Loss_Train:  [[ 12.7322926]]    Loss_Validation:  [[ 11.15270872]]\n",
      "Loop  3018 :    Loss_Train:  [[ 12.73227922]]    Loss_Validation:  [[ 11.15273119]]\n",
      "Loop  3019 :    Loss_Train:  [[ 12.73226587]]    Loss_Validation:  [[ 11.15275365]]\n",
      "Loop  3020 :    Loss_Train:  [[ 12.73225255]]    Loss_Validation:  [[ 11.1527761]]\n",
      "Loop  3021 :    Loss_Train:  [[ 12.73223924]]    Loss_Validation:  [[ 11.15279854]]\n",
      "Loop  3022 :    Loss_Train:  [[ 12.73222596]]    Loss_Validation:  [[ 11.15282096]]\n",
      "Loop  3023 :    Loss_Train:  [[ 12.7322127]]    Loss_Validation:  [[ 11.15284337]]\n",
      "Loop  3024 :    Loss_Train:  [[ 12.73219946]]    Loss_Validation:  [[ 11.15286577]]\n",
      "Loop  3025 :    Loss_Train:  [[ 12.73218625]]    Loss_Validation:  [[ 11.15288815]]\n",
      "Loop  3026 :    Loss_Train:  [[ 12.73217306]]    Loss_Validation:  [[ 11.15291052]]\n",
      "Loop  3027 :    Loss_Train:  [[ 12.73215989]]    Loss_Validation:  [[ 11.15293288]]\n",
      "Loop  3028 :    Loss_Train:  [[ 12.73214674]]    Loss_Validation:  [[ 11.15295523]]\n",
      "Loop  3029 :    Loss_Train:  [[ 12.73213362]]    Loss_Validation:  [[ 11.15297756]]\n",
      "Loop  3030 :    Loss_Train:  [[ 12.73212052]]    Loss_Validation:  [[ 11.15299988]]\n",
      "Loop  3031 :    Loss_Train:  [[ 12.73210744]]    Loss_Validation:  [[ 11.15302219]]\n",
      "Loop  3032 :    Loss_Train:  [[ 12.73209438]]    Loss_Validation:  [[ 11.15304448]]\n",
      "Loop  3033 :    Loss_Train:  [[ 12.73208135]]    Loss_Validation:  [[ 11.15306677]]\n",
      "Loop  3034 :    Loss_Train:  [[ 12.73206833]]    Loss_Validation:  [[ 11.15308904]]\n",
      "Loop  3035 :    Loss_Train:  [[ 12.73205534]]    Loss_Validation:  [[ 11.15311129]]\n",
      "Loop  3036 :    Loss_Train:  [[ 12.73204237]]    Loss_Validation:  [[ 11.15313354]]\n",
      "Loop  3037 :    Loss_Train:  [[ 12.73202943]]    Loss_Validation:  [[ 11.15315577]]\n",
      "Loop  3038 :    Loss_Train:  [[ 12.73201651]]    Loss_Validation:  [[ 11.15317799]]\n",
      "Loop  3039 :    Loss_Train:  [[ 12.7320036]]    Loss_Validation:  [[ 11.1532002]]\n",
      "Loop  3040 :    Loss_Train:  [[ 12.73199072]]    Loss_Validation:  [[ 11.15322239]]\n",
      "Loop  3041 :    Loss_Train:  [[ 12.73197787]]    Loss_Validation:  [[ 11.15324457]]\n",
      "Loop  3042 :    Loss_Train:  [[ 12.73196503]]    Loss_Validation:  [[ 11.15326674]]\n",
      "Loop  3043 :    Loss_Train:  [[ 12.73195222]]    Loss_Validation:  [[ 11.15328889]]\n",
      "Loop  3044 :    Loss_Train:  [[ 12.73193942]]    Loss_Validation:  [[ 11.15331104]]\n",
      "Loop  3045 :    Loss_Train:  [[ 12.73192665]]    Loss_Validation:  [[ 11.15333317]]\n",
      "Loop  3046 :    Loss_Train:  [[ 12.7319139]]    Loss_Validation:  [[ 11.15335529]]\n",
      "Loop  3047 :    Loss_Train:  [[ 12.73190118]]    Loss_Validation:  [[ 11.15337739]]\n",
      "Loop  3048 :    Loss_Train:  [[ 12.73188847]]    Loss_Validation:  [[ 11.15339948]]\n",
      "Loop  3049 :    Loss_Train:  [[ 12.73187579]]    Loss_Validation:  [[ 11.15342156]]\n",
      "Loop  3050 :    Loss_Train:  [[ 12.73186313]]    Loss_Validation:  [[ 11.15344363]]\n",
      "Loop  3051 :    Loss_Train:  [[ 12.73185049]]    Loss_Validation:  [[ 11.15346569]]\n",
      "Loop  3052 :    Loss_Train:  [[ 12.73183787]]    Loss_Validation:  [[ 11.15348773]]\n",
      "Loop  3053 :    Loss_Train:  [[ 12.73182527]]    Loss_Validation:  [[ 11.15350976]]\n",
      "Loop  3054 :    Loss_Train:  [[ 12.73181269]]    Loss_Validation:  [[ 11.15353177]]\n",
      "Loop  3055 :    Loss_Train:  [[ 12.73180014]]    Loss_Validation:  [[ 11.15355378]]\n",
      "Loop  3056 :    Loss_Train:  [[ 12.73178761]]    Loss_Validation:  [[ 11.15357577]]\n",
      "Loop  3057 :    Loss_Train:  [[ 12.73177509]]    Loss_Validation:  [[ 11.15359775]]\n",
      "Loop  3058 :    Loss_Train:  [[ 12.7317626]]    Loss_Validation:  [[ 11.15361972]]\n",
      "Loop  3059 :    Loss_Train:  [[ 12.73175013]]    Loss_Validation:  [[ 11.15364167]]\n",
      "Loop  3060 :    Loss_Train:  [[ 12.73173769]]    Loss_Validation:  [[ 11.15366361]]\n",
      "Loop  3061 :    Loss_Train:  [[ 12.73172526]]    Loss_Validation:  [[ 11.15368554]]\n",
      "Loop  3062 :    Loss_Train:  [[ 12.73171285]]    Loss_Validation:  [[ 11.15370746]]\n",
      "Loop  3063 :    Loss_Train:  [[ 12.73170047]]    Loss_Validation:  [[ 11.15372936]]\n",
      "Loop  3064 :    Loss_Train:  [[ 12.73168811]]    Loss_Validation:  [[ 11.15375125]]\n",
      "Loop  3065 :    Loss_Train:  [[ 12.73167576]]    Loss_Validation:  [[ 11.15377313]]\n",
      "Loop  3066 :    Loss_Train:  [[ 12.73166344]]    Loss_Validation:  [[ 11.153795]]\n",
      "Loop  3067 :    Loss_Train:  [[ 12.73165114]]    Loss_Validation:  [[ 11.15381685]]\n",
      "Loop  3068 :    Loss_Train:  [[ 12.73163886]]    Loss_Validation:  [[ 11.15383869]]\n",
      "Loop  3069 :    Loss_Train:  [[ 12.73162661]]    Loss_Validation:  [[ 11.15386052]]\n",
      "Loop  3070 :    Loss_Train:  [[ 12.73161437]]    Loss_Validation:  [[ 11.15388233]]\n",
      "Loop  3071 :    Loss_Train:  [[ 12.73160215]]    Loss_Validation:  [[ 11.15390414]]\n",
      "Loop  3072 :    Loss_Train:  [[ 12.73158996]]    Loss_Validation:  [[ 11.15392593]]\n",
      "Loop  3073 :    Loss_Train:  [[ 12.73157778]]    Loss_Validation:  [[ 11.15394771]]\n",
      "Loop  3074 :    Loss_Train:  [[ 12.73156563]]    Loss_Validation:  [[ 11.15396947]]\n",
      "Loop  3075 :    Loss_Train:  [[ 12.73155349]]    Loss_Validation:  [[ 11.15399123]]\n",
      "Loop  3076 :    Loss_Train:  [[ 12.73154138]]    Loss_Validation:  [[ 11.15401297]]\n",
      "Loop  3077 :    Loss_Train:  [[ 12.73152929]]    Loss_Validation:  [[ 11.15403469]]\n",
      "Loop  3078 :    Loss_Train:  [[ 12.73151722]]    Loss_Validation:  [[ 11.15405641]]\n",
      "Loop  3079 :    Loss_Train:  [[ 12.73150516]]    Loss_Validation:  [[ 11.15407811]]\n",
      "Loop  3080 :    Loss_Train:  [[ 12.73149313]]    Loss_Validation:  [[ 11.1540998]]\n",
      "Loop  3081 :    Loss_Train:  [[ 12.73148112]]    Loss_Validation:  [[ 11.15412148]]\n",
      "Loop  3082 :    Loss_Train:  [[ 12.73146913]]    Loss_Validation:  [[ 11.15414315]]\n",
      "Loop  3083 :    Loss_Train:  [[ 12.73145716]]    Loss_Validation:  [[ 11.1541648]]\n",
      "Loop  3084 :    Loss_Train:  [[ 12.73144522]]    Loss_Validation:  [[ 11.15418644]]\n",
      "Loop  3085 :    Loss_Train:  [[ 12.73143329]]    Loss_Validation:  [[ 11.15420807]]\n",
      "Loop  3086 :    Loss_Train:  [[ 12.73142138]]    Loss_Validation:  [[ 11.15422969]]\n",
      "Loop  3087 :    Loss_Train:  [[ 12.73140949]]    Loss_Validation:  [[ 11.15425129]]\n",
      "Loop  3088 :    Loss_Train:  [[ 12.73139762]]    Loss_Validation:  [[ 11.15427288]]\n",
      "Loop  3089 :    Loss_Train:  [[ 12.73138578]]    Loss_Validation:  [[ 11.15429446]]\n",
      "Loop  3090 :    Loss_Train:  [[ 12.73137395]]    Loss_Validation:  [[ 11.15431602]]\n",
      "Loop  3091 :    Loss_Train:  [[ 12.73136214]]    Loss_Validation:  [[ 11.15433758]]\n",
      "Loop  3092 :    Loss_Train:  [[ 12.73135035]]    Loss_Validation:  [[ 11.15435912]]\n",
      "Loop  3093 :    Loss_Train:  [[ 12.73133859]]    Loss_Validation:  [[ 11.15438065]]\n",
      "Loop  3094 :    Loss_Train:  [[ 12.73132684]]    Loss_Validation:  [[ 11.15440216]]\n",
      "Loop  3095 :    Loss_Train:  [[ 12.73131511]]    Loss_Validation:  [[ 11.15442367]]\n",
      "Loop  3096 :    Loss_Train:  [[ 12.7313034]]    Loss_Validation:  [[ 11.15444516]]\n",
      "Loop  3097 :    Loss_Train:  [[ 12.73129172]]    Loss_Validation:  [[ 11.15446664]]\n",
      "Loop  3098 :    Loss_Train:  [[ 12.73128005]]    Loss_Validation:  [[ 11.1544881]]\n",
      "Loop  3099 :    Loss_Train:  [[ 12.7312684]]    Loss_Validation:  [[ 11.15450956]]\n",
      "Loop  3100 :    Loss_Train:  [[ 12.73125677]]    Loss_Validation:  [[ 11.154531]]\n",
      "Loop  3101 :    Loss_Train:  [[ 12.73124517]]    Loss_Validation:  [[ 11.15455243]]\n",
      "Loop  3102 :    Loss_Train:  [[ 12.73123358]]    Loss_Validation:  [[ 11.15457385]]\n",
      "Loop  3103 :    Loss_Train:  [[ 12.73122201]]    Loss_Validation:  [[ 11.15459525]]\n",
      "Loop  3104 :    Loss_Train:  [[ 12.73121046]]    Loss_Validation:  [[ 11.15461664]]\n",
      "Loop  3105 :    Loss_Train:  [[ 12.73119893]]    Loss_Validation:  [[ 11.15463802]]\n",
      "Loop  3106 :    Loss_Train:  [[ 12.73118742]]    Loss_Validation:  [[ 11.15465939]]\n",
      "Loop  3107 :    Loss_Train:  [[ 12.73117593]]    Loss_Validation:  [[ 11.15468075]]\n",
      "Loop  3108 :    Loss_Train:  [[ 12.73116446]]    Loss_Validation:  [[ 11.15470209]]\n",
      "Loop  3109 :    Loss_Train:  [[ 12.73115301]]    Loss_Validation:  [[ 11.15472342]]\n",
      "Loop  3110 :    Loss_Train:  [[ 12.73114158]]    Loss_Validation:  [[ 11.15474474]]\n",
      "Loop  3111 :    Loss_Train:  [[ 12.73113017]]    Loss_Validation:  [[ 11.15476604]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  3112 :    Loss_Train:  [[ 12.73111877]]    Loss_Validation:  [[ 11.15478733]]\n",
      "Loop  3113 :    Loss_Train:  [[ 12.7311074]]    Loss_Validation:  [[ 11.15480862]]\n",
      "Loop  3114 :    Loss_Train:  [[ 12.73109605]]    Loss_Validation:  [[ 11.15482988]]\n",
      "Loop  3115 :    Loss_Train:  [[ 12.73108471]]    Loss_Validation:  [[ 11.15485114]]\n",
      "Loop  3116 :    Loss_Train:  [[ 12.7310734]]    Loss_Validation:  [[ 11.15487238]]\n",
      "Loop  3117 :    Loss_Train:  [[ 12.7310621]]    Loss_Validation:  [[ 11.15489362]]\n",
      "Loop  3118 :    Loss_Train:  [[ 12.73105082]]    Loss_Validation:  [[ 11.15491484]]\n",
      "Loop  3119 :    Loss_Train:  [[ 12.73103957]]    Loss_Validation:  [[ 11.15493604]]\n",
      "Loop  3120 :    Loss_Train:  [[ 12.73102833]]    Loss_Validation:  [[ 11.15495724]]\n",
      "Loop  3121 :    Loss_Train:  [[ 12.73101711]]    Loss_Validation:  [[ 11.15497842]]\n",
      "Loop  3122 :    Loss_Train:  [[ 12.73100591]]    Loss_Validation:  [[ 11.15499959]]\n",
      "Loop  3123 :    Loss_Train:  [[ 12.73099473]]    Loss_Validation:  [[ 11.15502075]]\n",
      "Loop  3124 :    Loss_Train:  [[ 12.73098356]]    Loss_Validation:  [[ 11.15504189]]\n",
      "Loop  3125 :    Loss_Train:  [[ 12.73097242]]    Loss_Validation:  [[ 11.15506303]]\n",
      "Loop  3126 :    Loss_Train:  [[ 12.7309613]]    Loss_Validation:  [[ 11.15508415]]\n",
      "Loop  3127 :    Loss_Train:  [[ 12.73095019]]    Loss_Validation:  [[ 11.15510526]]\n",
      "Loop  3128 :    Loss_Train:  [[ 12.7309391]]    Loss_Validation:  [[ 11.15512635]]\n",
      "Loop  3129 :    Loss_Train:  [[ 12.73092804]]    Loss_Validation:  [[ 11.15514744]]\n",
      "Loop  3130 :    Loss_Train:  [[ 12.73091699]]    Loss_Validation:  [[ 11.15516851]]\n",
      "Loop  3131 :    Loss_Train:  [[ 12.73090596]]    Loss_Validation:  [[ 11.15518957]]\n",
      "Loop  3132 :    Loss_Train:  [[ 12.73089495]]    Loss_Validation:  [[ 11.15521061]]\n",
      "Loop  3133 :    Loss_Train:  [[ 12.73088395]]    Loss_Validation:  [[ 11.15523165]]\n",
      "Loop  3134 :    Loss_Train:  [[ 12.73087298]]    Loss_Validation:  [[ 11.15525267]]\n",
      "Loop  3135 :    Loss_Train:  [[ 12.73086202]]    Loss_Validation:  [[ 11.15527368]]\n",
      "Loop  3136 :    Loss_Train:  [[ 12.73085109]]    Loss_Validation:  [[ 11.15529468]]\n",
      "Loop  3137 :    Loss_Train:  [[ 12.73084017]]    Loss_Validation:  [[ 11.15531567]]\n",
      "Loop  3138 :    Loss_Train:  [[ 12.73082927]]    Loss_Validation:  [[ 11.15533664]]\n",
      "Loop  3139 :    Loss_Train:  [[ 12.73081839]]    Loss_Validation:  [[ 11.1553576]]\n",
      "Loop  3140 :    Loss_Train:  [[ 12.73080752]]    Loss_Validation:  [[ 11.15537855]]\n",
      "Loop  3141 :    Loss_Train:  [[ 12.73079668]]    Loss_Validation:  [[ 11.15539949]]\n",
      "Loop  3142 :    Loss_Train:  [[ 12.73078585]]    Loss_Validation:  [[ 11.15542041]]\n",
      "Loop  3143 :    Loss_Train:  [[ 12.73077505]]    Loss_Validation:  [[ 11.15544133]]\n",
      "Loop  3144 :    Loss_Train:  [[ 12.73076426]]    Loss_Validation:  [[ 11.15546223]]\n",
      "Loop  3145 :    Loss_Train:  [[ 12.73075349]]    Loss_Validation:  [[ 11.15548312]]\n",
      "Loop  3146 :    Loss_Train:  [[ 12.73074273]]    Loss_Validation:  [[ 11.15550399]]\n",
      "Loop  3147 :    Loss_Train:  [[ 12.730732]]    Loss_Validation:  [[ 11.15552486]]\n",
      "Loop  3148 :    Loss_Train:  [[ 12.73072128]]    Loss_Validation:  [[ 11.15554571]]\n",
      "Loop  3149 :    Loss_Train:  [[ 12.73071059]]    Loss_Validation:  [[ 11.15556655]]\n",
      "Loop  3150 :    Loss_Train:  [[ 12.73069991]]    Loss_Validation:  [[ 11.15558737]]\n",
      "Loop  3151 :    Loss_Train:  [[ 12.73068924]]    Loss_Validation:  [[ 11.15560819]]\n",
      "Loop  3152 :    Loss_Train:  [[ 12.7306786]]    Loss_Validation:  [[ 11.15562899]]\n",
      "Loop  3153 :    Loss_Train:  [[ 12.73066798]]    Loss_Validation:  [[ 11.15564978]]\n",
      "Loop  3154 :    Loss_Train:  [[ 12.73065737]]    Loss_Validation:  [[ 11.15567056]]\n",
      "Loop  3155 :    Loss_Train:  [[ 12.73064678]]    Loss_Validation:  [[ 11.15569133]]\n",
      "Loop  3156 :    Loss_Train:  [[ 12.73063621]]    Loss_Validation:  [[ 11.15571208]]\n",
      "Loop  3157 :    Loss_Train:  [[ 12.73062565]]    Loss_Validation:  [[ 11.15573283]]\n",
      "Loop  3158 :    Loss_Train:  [[ 12.73061512]]    Loss_Validation:  [[ 11.15575356]]\n",
      "Loop  3159 :    Loss_Train:  [[ 12.7306046]]    Loss_Validation:  [[ 11.15577428]]\n",
      "Loop  3160 :    Loss_Train:  [[ 12.7305941]]    Loss_Validation:  [[ 11.15579498]]\n",
      "Loop  3161 :    Loss_Train:  [[ 12.73058362]]    Loss_Validation:  [[ 11.15581567]]\n",
      "Loop  3162 :    Loss_Train:  [[ 12.73057316]]    Loss_Validation:  [[ 11.15583636]]\n",
      "Loop  3163 :    Loss_Train:  [[ 12.73056271]]    Loss_Validation:  [[ 11.15585703]]\n",
      "Loop  3164 :    Loss_Train:  [[ 12.73055228]]    Loss_Validation:  [[ 11.15587768]]\n",
      "Loop  3165 :    Loss_Train:  [[ 12.73054187]]    Loss_Validation:  [[ 11.15589833]]\n",
      "Loop  3166 :    Loss_Train:  [[ 12.73053148]]    Loss_Validation:  [[ 11.15591896]]\n",
      "Loop  3167 :    Loss_Train:  [[ 12.7305211]]    Loss_Validation:  [[ 11.15593958]]\n",
      "Loop  3168 :    Loss_Train:  [[ 12.73051074]]    Loss_Validation:  [[ 11.15596019]]\n",
      "Loop  3169 :    Loss_Train:  [[ 12.7305004]]    Loss_Validation:  [[ 11.15598079]]\n",
      "Loop  3170 :    Loss_Train:  [[ 12.73049008]]    Loss_Validation:  [[ 11.15600138]]\n",
      "Loop  3171 :    Loss_Train:  [[ 12.73047977]]    Loss_Validation:  [[ 11.15602195]]\n",
      "Loop  3172 :    Loss_Train:  [[ 12.73046949]]    Loss_Validation:  [[ 11.15604251]]\n",
      "Loop  3173 :    Loss_Train:  [[ 12.73045922]]    Loss_Validation:  [[ 11.15606306]]\n",
      "Loop  3174 :    Loss_Train:  [[ 12.73044896]]    Loss_Validation:  [[ 11.1560836]]\n",
      "Loop  3175 :    Loss_Train:  [[ 12.73043873]]    Loss_Validation:  [[ 11.15610412]]\n",
      "Loop  3176 :    Loss_Train:  [[ 12.73042851]]    Loss_Validation:  [[ 11.15612463]]\n",
      "Loop  3177 :    Loss_Train:  [[ 12.73041831]]    Loss_Validation:  [[ 11.15614513]]\n",
      "Loop  3178 :    Loss_Train:  [[ 12.73040812]]    Loss_Validation:  [[ 11.15616562]]\n",
      "Loop  3179 :    Loss_Train:  [[ 12.73039796]]    Loss_Validation:  [[ 11.1561861]]\n",
      "Loop  3180 :    Loss_Train:  [[ 12.73038781]]    Loss_Validation:  [[ 11.15620656]]\n",
      "Loop  3181 :    Loss_Train:  [[ 12.73037768]]    Loss_Validation:  [[ 11.15622702]]\n",
      "Loop  3182 :    Loss_Train:  [[ 12.73036756]]    Loss_Validation:  [[ 11.15624746]]\n",
      "Loop  3183 :    Loss_Train:  [[ 12.73035747]]    Loss_Validation:  [[ 11.15626788]]\n",
      "Loop  3184 :    Loss_Train:  [[ 12.73034738]]    Loss_Validation:  [[ 11.1562883]]\n",
      "Loop  3185 :    Loss_Train:  [[ 12.73033732]]    Loss_Validation:  [[ 11.15630871]]\n",
      "Loop  3186 :    Loss_Train:  [[ 12.73032728]]    Loss_Validation:  [[ 11.1563291]]\n",
      "Loop  3187 :    Loss_Train:  [[ 12.73031725]]    Loss_Validation:  [[ 11.15634948]]\n",
      "Loop  3188 :    Loss_Train:  [[ 12.73030723]]    Loss_Validation:  [[ 11.15636985]]\n",
      "Loop  3189 :    Loss_Train:  [[ 12.73029724]]    Loss_Validation:  [[ 11.1563902]]\n",
      "Loop  3190 :    Loss_Train:  [[ 12.73028726]]    Loss_Validation:  [[ 11.15641055]]\n",
      "Loop  3191 :    Loss_Train:  [[ 12.7302773]]    Loss_Validation:  [[ 11.15643088]]\n",
      "Loop  3192 :    Loss_Train:  [[ 12.73026735]]    Loss_Validation:  [[ 11.1564512]]\n",
      "Loop  3193 :    Loss_Train:  [[ 12.73025743]]    Loss_Validation:  [[ 11.15647151]]\n",
      "Loop  3194 :    Loss_Train:  [[ 12.73024752]]    Loss_Validation:  [[ 11.15649181]]\n",
      "Loop  3195 :    Loss_Train:  [[ 12.73023762]]    Loss_Validation:  [[ 11.15651209]]\n",
      "Loop  3196 :    Loss_Train:  [[ 12.73022774]]    Loss_Validation:  [[ 11.15653236]]\n",
      "Loop  3197 :    Loss_Train:  [[ 12.73021788]]    Loss_Validation:  [[ 11.15655263]]\n",
      "Loop  3198 :    Loss_Train:  [[ 12.73020804]]    Loss_Validation:  [[ 11.15657287]]\n",
      "Loop  3199 :    Loss_Train:  [[ 12.73019821]]    Loss_Validation:  [[ 11.15659311]]\n",
      "Loop  3200 :    Loss_Train:  [[ 12.7301884]]    Loss_Validation:  [[ 11.15661334]]\n",
      "Loop  3201 :    Loss_Train:  [[ 12.73017861]]    Loss_Validation:  [[ 11.15663355]]\n",
      "Loop  3202 :    Loss_Train:  [[ 12.73016883]]    Loss_Validation:  [[ 11.15665375]]\n",
      "Loop  3203 :    Loss_Train:  [[ 12.73015907]]    Loss_Validation:  [[ 11.15667394]]\n",
      "Loop  3204 :    Loss_Train:  [[ 12.73014933]]    Loss_Validation:  [[ 11.15669412]]\n",
      "Loop  3205 :    Loss_Train:  [[ 12.7301396]]    Loss_Validation:  [[ 11.15671428]]\n",
      "Loop  3206 :    Loss_Train:  [[ 12.73012989]]    Loss_Validation:  [[ 11.15673444]]\n",
      "Loop  3207 :    Loss_Train:  [[ 12.73012019]]    Loss_Validation:  [[ 11.15675458]]\n",
      "Loop  3208 :    Loss_Train:  [[ 12.73011052]]    Loss_Validation:  [[ 11.15677471]]\n",
      "Loop  3209 :    Loss_Train:  [[ 12.73010085]]    Loss_Validation:  [[ 11.15679483]]\n",
      "Loop  3210 :    Loss_Train:  [[ 12.73009121]]    Loss_Validation:  [[ 11.15681493]]\n",
      "Loop  3211 :    Loss_Train:  [[ 12.73008158]]    Loss_Validation:  [[ 11.15683503]]\n",
      "Loop  3212 :    Loss_Train:  [[ 12.73007197]]    Loss_Validation:  [[ 11.15685511]]\n",
      "Loop  3213 :    Loss_Train:  [[ 12.73006237]]    Loss_Validation:  [[ 11.15687518]]\n",
      "Loop  3214 :    Loss_Train:  [[ 12.73005279]]    Loss_Validation:  [[ 11.15689524]]\n",
      "Loop  3215 :    Loss_Train:  [[ 12.73004323]]    Loss_Validation:  [[ 11.15691528]]\n",
      "Loop  3216 :    Loss_Train:  [[ 12.73003368]]    Loss_Validation:  [[ 11.15693532]]\n",
      "Loop  3217 :    Loss_Train:  [[ 12.73002415]]    Loss_Validation:  [[ 11.15695534]]\n",
      "Loop  3218 :    Loss_Train:  [[ 12.73001463]]    Loss_Validation:  [[ 11.15697535]]\n",
      "Loop  3219 :    Loss_Train:  [[ 12.73000513]]    Loss_Validation:  [[ 11.15699535]]\n",
      "Loop  3220 :    Loss_Train:  [[ 12.72999565]]    Loss_Validation:  [[ 11.15701534]]\n",
      "Loop  3221 :    Loss_Train:  [[ 12.72998618]]    Loss_Validation:  [[ 11.15703532]]\n",
      "Loop  3222 :    Loss_Train:  [[ 12.72997673]]    Loss_Validation:  [[ 11.15705528]]\n",
      "Loop  3223 :    Loss_Train:  [[ 12.7299673]]    Loss_Validation:  [[ 11.15707523]]\n",
      "Loop  3224 :    Loss_Train:  [[ 12.72995788]]    Loss_Validation:  [[ 11.15709517]]\n",
      "Loop  3225 :    Loss_Train:  [[ 12.72994847]]    Loss_Validation:  [[ 11.1571151]]\n",
      "Loop  3226 :    Loss_Train:  [[ 12.72993909]]    Loss_Validation:  [[ 11.15713502]]\n",
      "Loop  3227 :    Loss_Train:  [[ 12.72992971]]    Loss_Validation:  [[ 11.15715492]]\n",
      "Loop  3228 :    Loss_Train:  [[ 12.72992036]]    Loss_Validation:  [[ 11.15717481]]\n",
      "Loop  3229 :    Loss_Train:  [[ 12.72991102]]    Loss_Validation:  [[ 11.1571947]]\n",
      "Loop  3230 :    Loss_Train:  [[ 12.7299017]]    Loss_Validation:  [[ 11.15721457]]\n",
      "Loop  3231 :    Loss_Train:  [[ 12.72989239]]    Loss_Validation:  [[ 11.15723442]]\n",
      "Loop  3232 :    Loss_Train:  [[ 12.72988309]]    Loss_Validation:  [[ 11.15725427]]\n",
      "Loop  3233 :    Loss_Train:  [[ 12.72987382]]    Loss_Validation:  [[ 11.1572741]]\n",
      "Loop  3234 :    Loss_Train:  [[ 12.72986456]]    Loss_Validation:  [[ 11.15729393]]\n",
      "Loop  3235 :    Loss_Train:  [[ 12.72985531]]    Loss_Validation:  [[ 11.15731374]]\n",
      "Loop  3236 :    Loss_Train:  [[ 12.72984608]]    Loss_Validation:  [[ 11.15733354]]\n",
      "Loop  3237 :    Loss_Train:  [[ 12.72983687]]    Loss_Validation:  [[ 11.15735332]]\n",
      "Loop  3238 :    Loss_Train:  [[ 12.72982767]]    Loss_Validation:  [[ 11.1573731]]\n",
      "Loop  3239 :    Loss_Train:  [[ 12.72981849]]    Loss_Validation:  [[ 11.15739286]]\n",
      "Loop  3240 :    Loss_Train:  [[ 12.72980932]]    Loss_Validation:  [[ 11.15741261]]\n",
      "Loop  3241 :    Loss_Train:  [[ 12.72980017]]    Loss_Validation:  [[ 11.15743235]]\n",
      "Loop  3242 :    Loss_Train:  [[ 12.72979103]]    Loss_Validation:  [[ 11.15745208]]\n",
      "Loop  3243 :    Loss_Train:  [[ 12.72978191]]    Loss_Validation:  [[ 11.1574718]]\n",
      "Loop  3244 :    Loss_Train:  [[ 12.72977281]]    Loss_Validation:  [[ 11.1574915]]\n",
      "Loop  3245 :    Loss_Train:  [[ 12.72976372]]    Loss_Validation:  [[ 11.1575112]]\n",
      "Loop  3246 :    Loss_Train:  [[ 12.72975464]]    Loss_Validation:  [[ 11.15753088]]\n",
      "Loop  3247 :    Loss_Train:  [[ 12.72974558]]    Loss_Validation:  [[ 11.15755055]]\n",
      "Loop  3248 :    Loss_Train:  [[ 12.72973654]]    Loss_Validation:  [[ 11.15757021]]\n",
      "Loop  3249 :    Loss_Train:  [[ 12.72972751]]    Loss_Validation:  [[ 11.15758985]]\n",
      "Loop  3250 :    Loss_Train:  [[ 12.7297185]]    Loss_Validation:  [[ 11.15760949]]\n",
      "Loop  3251 :    Loss_Train:  [[ 12.7297095]]    Loss_Validation:  [[ 11.15762911]]\n",
      "Loop  3252 :    Loss_Train:  [[ 12.72970052]]    Loss_Validation:  [[ 11.15764872]]\n",
      "Loop  3253 :    Loss_Train:  [[ 12.72969155]]    Loss_Validation:  [[ 11.15766832]]\n",
      "Loop  3254 :    Loss_Train:  [[ 12.7296826]]    Loss_Validation:  [[ 11.15768791]]\n",
      "Loop  3255 :    Loss_Train:  [[ 12.72967366]]    Loss_Validation:  [[ 11.15770749]]\n",
      "Loop  3256 :    Loss_Train:  [[ 12.72966474]]    Loss_Validation:  [[ 11.15772705]]\n",
      "Loop  3257 :    Loss_Train:  [[ 12.72965583]]    Loss_Validation:  [[ 11.15774661]]\n",
      "Loop  3258 :    Loss_Train:  [[ 12.72964694]]    Loss_Validation:  [[ 11.15776615]]\n",
      "Loop  3259 :    Loss_Train:  [[ 12.72963806]]    Loss_Validation:  [[ 11.15778568]]\n",
      "Loop  3260 :    Loss_Train:  [[ 12.7296292]]    Loss_Validation:  [[ 11.1578052]]\n",
      "Loop  3261 :    Loss_Train:  [[ 12.72962035]]    Loss_Validation:  [[ 11.1578247]]\n",
      "Loop  3262 :    Loss_Train:  [[ 12.72961152]]    Loss_Validation:  [[ 11.1578442]]\n",
      "Loop  3263 :    Loss_Train:  [[ 12.7296027]]    Loss_Validation:  [[ 11.15786368]]\n",
      "Loop  3264 :    Loss_Train:  [[ 12.7295939]]    Loss_Validation:  [[ 11.15788315]]\n",
      "Loop  3265 :    Loss_Train:  [[ 12.72958511]]    Loss_Validation:  [[ 11.15790261]]\n",
      "Loop  3266 :    Loss_Train:  [[ 12.72957634]]    Loss_Validation:  [[ 11.15792206]]\n",
      "Loop  3267 :    Loss_Train:  [[ 12.72956758]]    Loss_Validation:  [[ 11.1579415]]\n",
      "Loop  3268 :    Loss_Train:  [[ 12.72955884]]    Loss_Validation:  [[ 11.15796092]]\n",
      "Loop  3269 :    Loss_Train:  [[ 12.72955011]]    Loss_Validation:  [[ 11.15798034]]\n",
      "Loop  3270 :    Loss_Train:  [[ 12.7295414]]    Loss_Validation:  [[ 11.15799974]]\n",
      "Loop  3271 :    Loss_Train:  [[ 12.7295327]]    Loss_Validation:  [[ 11.15801913]]\n",
      "Loop  3272 :    Loss_Train:  [[ 12.72952402]]    Loss_Validation:  [[ 11.15803851]]\n",
      "Loop  3273 :    Loss_Train:  [[ 12.72951535]]    Loss_Validation:  [[ 11.15805788]]\n",
      "Loop  3274 :    Loss_Train:  [[ 12.72950669]]    Loss_Validation:  [[ 11.15807723]]\n",
      "Loop  3275 :    Loss_Train:  [[ 12.72949805]]    Loss_Validation:  [[ 11.15809658]]\n",
      "Loop  3276 :    Loss_Train:  [[ 12.72948943]]    Loss_Validation:  [[ 11.15811591]]\n",
      "Loop  3277 :    Loss_Train:  [[ 12.72948082]]    Loss_Validation:  [[ 11.15813523]]\n",
      "Loop  3278 :    Loss_Train:  [[ 12.72947222]]    Loss_Validation:  [[ 11.15815454]]\n",
      "Loop  3279 :    Loss_Train:  [[ 12.72946364]]    Loss_Validation:  [[ 11.15817384]]\n",
      "Loop  3280 :    Loss_Train:  [[ 12.72945507]]    Loss_Validation:  [[ 11.15819313]]\n",
      "Loop  3281 :    Loss_Train:  [[ 12.72944652]]    Loss_Validation:  [[ 11.1582124]]\n",
      "Loop  3282 :    Loss_Train:  [[ 12.72943798]]    Loss_Validation:  [[ 11.15823167]]\n",
      "Loop  3283 :    Loss_Train:  [[ 12.72942946]]    Loss_Validation:  [[ 11.15825092]]\n",
      "Loop  3284 :    Loss_Train:  [[ 12.72942095]]    Loss_Validation:  [[ 11.15827016]]\n",
      "Loop  3285 :    Loss_Train:  [[ 12.72941246]]    Loss_Validation:  [[ 11.15828939]]\n",
      "Loop  3286 :    Loss_Train:  [[ 12.72940398]]    Loss_Validation:  [[ 11.15830861]]\n",
      "Loop  3287 :    Loss_Train:  [[ 12.72939551]]    Loss_Validation:  [[ 11.15832781]]\n",
      "Loop  3288 :    Loss_Train:  [[ 12.72938706]]    Loss_Validation:  [[ 11.15834701]]\n",
      "Loop  3289 :    Loss_Train:  [[ 12.72937862]]    Loss_Validation:  [[ 11.15836619]]\n",
      "Loop  3290 :    Loss_Train:  [[ 12.7293702]]    Loss_Validation:  [[ 11.15838536]]\n",
      "Loop  3291 :    Loss_Train:  [[ 12.72936179]]    Loss_Validation:  [[ 11.15840452]]\n",
      "Loop  3292 :    Loss_Train:  [[ 12.72935339]]    Loss_Validation:  [[ 11.15842367]]\n",
      "Loop  3293 :    Loss_Train:  [[ 12.72934501]]    Loss_Validation:  [[ 11.15844281]]\n",
      "Loop  3294 :    Loss_Train:  [[ 12.72933665]]    Loss_Validation:  [[ 11.15846193]]\n",
      "Loop  3295 :    Loss_Train:  [[ 12.72932829]]    Loss_Validation:  [[ 11.15848105]]\n",
      "Loop  3296 :    Loss_Train:  [[ 12.72931995]]    Loss_Validation:  [[ 11.15850015]]\n",
      "Loop  3297 :    Loss_Train:  [[ 12.72931163]]    Loss_Validation:  [[ 11.15851924]]\n",
      "Loop  3298 :    Loss_Train:  [[ 12.72930332]]    Loss_Validation:  [[ 11.15853832]]\n",
      "Loop  3299 :    Loss_Train:  [[ 12.72929502]]    Loss_Validation:  [[ 11.15855739]]\n",
      "Loop  3300 :    Loss_Train:  [[ 12.72928674]]    Loss_Validation:  [[ 11.15857645]]\n",
      "Loop  3301 :    Loss_Train:  [[ 12.72927847]]    Loss_Validation:  [[ 11.15859549]]\n",
      "Loop  3302 :    Loss_Train:  [[ 12.72927022]]    Loss_Validation:  [[ 11.15861453]]\n",
      "Loop  3303 :    Loss_Train:  [[ 12.72926198]]    Loss_Validation:  [[ 11.15863355]]\n",
      "Loop  3304 :    Loss_Train:  [[ 12.72925375]]    Loss_Validation:  [[ 11.15865256]]\n",
      "Loop  3305 :    Loss_Train:  [[ 12.72924554]]    Loss_Validation:  [[ 11.15867156]]\n",
      "Loop  3306 :    Loss_Train:  [[ 12.72923734]]    Loss_Validation:  [[ 11.15869055]]\n",
      "Loop  3307 :    Loss_Train:  [[ 12.72922916]]    Loss_Validation:  [[ 11.15870953]]\n",
      "Loop  3308 :    Loss_Train:  [[ 12.72922099]]    Loss_Validation:  [[ 11.15872849]]\n",
      "Loop  3309 :    Loss_Train:  [[ 12.72921283]]    Loss_Validation:  [[ 11.15874745]]\n",
      "Loop  3310 :    Loss_Train:  [[ 12.72920469]]    Loss_Validation:  [[ 11.15876639]]\n",
      "Loop  3311 :    Loss_Train:  [[ 12.72919656]]    Loss_Validation:  [[ 11.15878532]]\n",
      "Loop  3312 :    Loss_Train:  [[ 12.72918844]]    Loss_Validation:  [[ 11.15880424]]\n",
      "Loop  3313 :    Loss_Train:  [[ 12.72918034]]    Loss_Validation:  [[ 11.15882315]]\n",
      "Loop  3314 :    Loss_Train:  [[ 12.72917225]]    Loss_Validation:  [[ 11.15884205]]\n",
      "Loop  3315 :    Loss_Train:  [[ 12.72916418]]    Loss_Validation:  [[ 11.15886094]]\n",
      "Loop  3316 :    Loss_Train:  [[ 12.72915612]]    Loss_Validation:  [[ 11.15887981]]\n",
      "Loop  3317 :    Loss_Train:  [[ 12.72914807]]    Loss_Validation:  [[ 11.15889868]]\n",
      "Loop  3318 :    Loss_Train:  [[ 12.72914003]]    Loss_Validation:  [[ 11.15891753]]\n",
      "Loop  3319 :    Loss_Train:  [[ 12.72913201]]    Loss_Validation:  [[ 11.15893637]]\n",
      "Loop  3320 :    Loss_Train:  [[ 12.72912401]]    Loss_Validation:  [[ 11.1589552]]\n",
      "Loop  3321 :    Loss_Train:  [[ 12.72911601]]    Loss_Validation:  [[ 11.15897402]]\n",
      "Loop  3322 :    Loss_Train:  [[ 12.72910803]]    Loss_Validation:  [[ 11.15899282]]\n",
      "Loop  3323 :    Loss_Train:  [[ 12.72910007]]    Loss_Validation:  [[ 11.15901162]]\n",
      "Loop  3324 :    Loss_Train:  [[ 12.72909211]]    Loss_Validation:  [[ 11.1590304]]\n",
      "Loop  3325 :    Loss_Train:  [[ 12.72908418]]    Loss_Validation:  [[ 11.15904918]]\n",
      "Loop  3326 :    Loss_Train:  [[ 12.72907625]]    Loss_Validation:  [[ 11.15906794]]\n",
      "Loop  3327 :    Loss_Train:  [[ 12.72906834]]    Loss_Validation:  [[ 11.15908669]]\n",
      "Loop  3328 :    Loss_Train:  [[ 12.72906044]]    Loss_Validation:  [[ 11.15910543]]\n",
      "Loop  3329 :    Loss_Train:  [[ 12.72905255]]    Loss_Validation:  [[ 11.15912416]]\n",
      "Loop  3330 :    Loss_Train:  [[ 12.72904468]]    Loss_Validation:  [[ 11.15914287]]\n",
      "Loop  3331 :    Loss_Train:  [[ 12.72903682]]    Loss_Validation:  [[ 11.15916158]]\n",
      "Loop  3332 :    Loss_Train:  [[ 12.72902897]]    Loss_Validation:  [[ 11.15918027]]\n",
      "Loop  3333 :    Loss_Train:  [[ 12.72902114]]    Loss_Validation:  [[ 11.15919896]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  3334 :    Loss_Train:  [[ 12.72901332]]    Loss_Validation:  [[ 11.15921763]]\n",
      "Loop  3335 :    Loss_Train:  [[ 12.72900551]]    Loss_Validation:  [[ 11.15923629]]\n",
      "Loop  3336 :    Loss_Train:  [[ 12.72899772]]    Loss_Validation:  [[ 11.15925494]]\n",
      "Loop  3337 :    Loss_Train:  [[ 12.72898994]]    Loss_Validation:  [[ 11.15927357]]\n",
      "Loop  3338 :    Loss_Train:  [[ 12.72898217]]    Loss_Validation:  [[ 11.1592922]]\n",
      "Loop  3339 :    Loss_Train:  [[ 12.72897442]]    Loss_Validation:  [[ 11.15931082]]\n",
      "Loop  3340 :    Loss_Train:  [[ 12.72896668]]    Loss_Validation:  [[ 11.15932942]]\n",
      "Loop  3341 :    Loss_Train:  [[ 12.72895895]]    Loss_Validation:  [[ 11.15934801]]\n",
      "Loop  3342 :    Loss_Train:  [[ 12.72895124]]    Loss_Validation:  [[ 11.1593666]]\n",
      "Loop  3343 :    Loss_Train:  [[ 12.72894353]]    Loss_Validation:  [[ 11.15938517]]\n",
      "Loop  3344 :    Loss_Train:  [[ 12.72893585]]    Loss_Validation:  [[ 11.15940373]]\n",
      "Loop  3345 :    Loss_Train:  [[ 12.72892817]]    Loss_Validation:  [[ 11.15942227]]\n",
      "Loop  3346 :    Loss_Train:  [[ 12.72892051]]    Loss_Validation:  [[ 11.15944081]]\n",
      "Loop  3347 :    Loss_Train:  [[ 12.72891286]]    Loss_Validation:  [[ 11.15945934]]\n",
      "Loop  3348 :    Loss_Train:  [[ 12.72890522]]    Loss_Validation:  [[ 11.15947785]]\n",
      "Loop  3349 :    Loss_Train:  [[ 12.7288976]]    Loss_Validation:  [[ 11.15949635]]\n",
      "Loop  3350 :    Loss_Train:  [[ 12.72888998]]    Loss_Validation:  [[ 11.15951485]]\n",
      "Loop  3351 :    Loss_Train:  [[ 12.72888239]]    Loss_Validation:  [[ 11.15953333]]\n",
      "Loop  3352 :    Loss_Train:  [[ 12.7288748]]    Loss_Validation:  [[ 11.1595518]]\n",
      "Loop  3353 :    Loss_Train:  [[ 12.72886723]]    Loss_Validation:  [[ 11.15957026]]\n",
      "Loop  3354 :    Loss_Train:  [[ 12.72885967]]    Loss_Validation:  [[ 11.1595887]]\n",
      "Loop  3355 :    Loss_Train:  [[ 12.72885212]]    Loss_Validation:  [[ 11.15960714]]\n",
      "Loop  3356 :    Loss_Train:  [[ 12.72884459]]    Loss_Validation:  [[ 11.15962556]]\n",
      "Loop  3357 :    Loss_Train:  [[ 12.72883706]]    Loss_Validation:  [[ 11.15964398]]\n",
      "Loop  3358 :    Loss_Train:  [[ 12.72882955]]    Loss_Validation:  [[ 11.15966238]]\n",
      "Loop  3359 :    Loss_Train:  [[ 12.72882206]]    Loss_Validation:  [[ 11.15968077]]\n",
      "Loop  3360 :    Loss_Train:  [[ 12.72881457]]    Loss_Validation:  [[ 11.15969915]]\n",
      "Loop  3361 :    Loss_Train:  [[ 12.7288071]]    Loss_Validation:  [[ 11.15971752]]\n",
      "Loop  3362 :    Loss_Train:  [[ 12.72879964]]    Loss_Validation:  [[ 11.15973588]]\n",
      "Loop  3363 :    Loss_Train:  [[ 12.7287922]]    Loss_Validation:  [[ 11.15975423]]\n",
      "Loop  3364 :    Loss_Train:  [[ 12.72878476]]    Loss_Validation:  [[ 11.15977257]]\n",
      "Loop  3365 :    Loss_Train:  [[ 12.72877734]]    Loss_Validation:  [[ 11.15979089]]\n",
      "Loop  3366 :    Loss_Train:  [[ 12.72876993]]    Loss_Validation:  [[ 11.15980921]]\n",
      "Loop  3367 :    Loss_Train:  [[ 12.72876254]]    Loss_Validation:  [[ 11.15982751]]\n",
      "Loop  3368 :    Loss_Train:  [[ 12.72875515]]    Loss_Validation:  [[ 11.1598458]]\n",
      "Loop  3369 :    Loss_Train:  [[ 12.72874778]]    Loss_Validation:  [[ 11.15986408]]\n",
      "Loop  3370 :    Loss_Train:  [[ 12.72874042]]    Loss_Validation:  [[ 11.15988235]]\n",
      "Loop  3371 :    Loss_Train:  [[ 12.72873308]]    Loss_Validation:  [[ 11.15990061]]\n",
      "Loop  3372 :    Loss_Train:  [[ 12.72872574]]    Loss_Validation:  [[ 11.15991886]]\n",
      "Loop  3373 :    Loss_Train:  [[ 12.72871842]]    Loss_Validation:  [[ 11.15993709]]\n",
      "Loop  3374 :    Loss_Train:  [[ 12.72871111]]    Loss_Validation:  [[ 11.15995532]]\n",
      "Loop  3375 :    Loss_Train:  [[ 12.72870382]]    Loss_Validation:  [[ 11.15997353]]\n",
      "Loop  3376 :    Loss_Train:  [[ 12.72869653]]    Loss_Validation:  [[ 11.15999174]]\n",
      "Loop  3377 :    Loss_Train:  [[ 12.72868926]]    Loss_Validation:  [[ 11.16000993]]\n",
      "Loop  3378 :    Loss_Train:  [[ 12.728682]]    Loss_Validation:  [[ 11.16002811]]\n",
      "Loop  3379 :    Loss_Train:  [[ 12.72867475]]    Loss_Validation:  [[ 11.16004628]]\n",
      "Loop  3380 :    Loss_Train:  [[ 12.72866752]]    Loss_Validation:  [[ 11.16006444]]\n",
      "Loop  3381 :    Loss_Train:  [[ 12.72866029]]    Loss_Validation:  [[ 11.16008259]]\n",
      "Loop  3382 :    Loss_Train:  [[ 12.72865308]]    Loss_Validation:  [[ 11.16010073]]\n",
      "Loop  3383 :    Loss_Train:  [[ 12.72864588]]    Loss_Validation:  [[ 11.16011885]]\n",
      "Loop  3384 :    Loss_Train:  [[ 12.7286387]]    Loss_Validation:  [[ 11.16013697]]\n",
      "Loop  3385 :    Loss_Train:  [[ 12.72863152]]    Loss_Validation:  [[ 11.16015507]]\n",
      "Loop  3386 :    Loss_Train:  [[ 12.72862436]]    Loss_Validation:  [[ 11.16017317]]\n",
      "Loop  3387 :    Loss_Train:  [[ 12.72861721]]    Loss_Validation:  [[ 11.16019125]]\n",
      "Loop  3388 :    Loss_Train:  [[ 12.72861007]]    Loss_Validation:  [[ 11.16020932]]\n",
      "Loop  3389 :    Loss_Train:  [[ 12.72860294]]    Loss_Validation:  [[ 11.16022738]]\n",
      "Loop  3390 :    Loss_Train:  [[ 12.72859583]]    Loss_Validation:  [[ 11.16024543]]\n",
      "Loop  3391 :    Loss_Train:  [[ 12.72858872]]    Loss_Validation:  [[ 11.16026347]]\n",
      "Loop  3392 :    Loss_Train:  [[ 12.72858163]]    Loss_Validation:  [[ 11.1602815]]\n",
      "Loop  3393 :    Loss_Train:  [[ 12.72857455]]    Loss_Validation:  [[ 11.16029951]]\n",
      "Loop  3394 :    Loss_Train:  [[ 12.72856749]]    Loss_Validation:  [[ 11.16031752]]\n",
      "Loop  3395 :    Loss_Train:  [[ 12.72856043]]    Loss_Validation:  [[ 11.16033551]]\n",
      "Loop  3396 :    Loss_Train:  [[ 12.72855339]]    Loss_Validation:  [[ 11.1603535]]\n",
      "Loop  3397 :    Loss_Train:  [[ 12.72854636]]    Loss_Validation:  [[ 11.16037147]]\n",
      "Loop  3398 :    Loss_Train:  [[ 12.72853934]]    Loss_Validation:  [[ 11.16038943]]\n",
      "Loop  3399 :    Loss_Train:  [[ 12.72853233]]    Loss_Validation:  [[ 11.16040738]]\n",
      "Loop  3400 :    Loss_Train:  [[ 12.72852534]]    Loss_Validation:  [[ 11.16042532]]\n",
      "Loop  3401 :    Loss_Train:  [[ 12.72851835]]    Loss_Validation:  [[ 11.16044325]]\n",
      "Loop  3402 :    Loss_Train:  [[ 12.72851138]]    Loss_Validation:  [[ 11.16046117]]\n",
      "Loop  3403 :    Loss_Train:  [[ 12.72850442]]    Loss_Validation:  [[ 11.16047908]]\n",
      "Loop  3404 :    Loss_Train:  [[ 12.72849747]]    Loss_Validation:  [[ 11.16049697]]\n",
      "Loop  3405 :    Loss_Train:  [[ 12.72849053]]    Loss_Validation:  [[ 11.16051486]]\n",
      "Loop  3406 :    Loss_Train:  [[ 12.72848361]]    Loss_Validation:  [[ 11.16053273]]\n",
      "Loop  3407 :    Loss_Train:  [[ 12.72847669]]    Loss_Validation:  [[ 11.1605506]]\n",
      "Loop  3408 :    Loss_Train:  [[ 12.72846979]]    Loss_Validation:  [[ 11.16056845]]\n",
      "Loop  3409 :    Loss_Train:  [[ 12.7284629]]    Loss_Validation:  [[ 11.16058629]]\n",
      "Loop  3410 :    Loss_Train:  [[ 12.72845602]]    Loss_Validation:  [[ 11.16060412]]\n",
      "Loop  3411 :    Loss_Train:  [[ 12.72844915]]    Loss_Validation:  [[ 11.16062194]]\n",
      "Loop  3412 :    Loss_Train:  [[ 12.7284423]]    Loss_Validation:  [[ 11.16063975]]\n",
      "Loop  3413 :    Loss_Train:  [[ 12.72843545]]    Loss_Validation:  [[ 11.16065755]]\n",
      "Loop  3414 :    Loss_Train:  [[ 12.72842862]]    Loss_Validation:  [[ 11.16067534]]\n",
      "Loop  3415 :    Loss_Train:  [[ 12.7284218]]    Loss_Validation:  [[ 11.16069311]]\n",
      "Loop  3416 :    Loss_Train:  [[ 12.72841499]]    Loss_Validation:  [[ 11.16071088]]\n",
      "Loop  3417 :    Loss_Train:  [[ 12.72840819]]    Loss_Validation:  [[ 11.16072864]]\n",
      "Loop  3418 :    Loss_Train:  [[ 12.72840141]]    Loss_Validation:  [[ 11.16074638]]\n",
      "Loop  3419 :    Loss_Train:  [[ 12.72839463]]    Loss_Validation:  [[ 11.16076411]]\n",
      "Loop  3420 :    Loss_Train:  [[ 12.72838787]]    Loss_Validation:  [[ 11.16078184]]\n",
      "Loop  3421 :    Loss_Train:  [[ 12.72838111]]    Loss_Validation:  [[ 11.16079955]]\n",
      "Loop  3422 :    Loss_Train:  [[ 12.72837437]]    Loss_Validation:  [[ 11.16081725]]\n",
      "Loop  3423 :    Loss_Train:  [[ 12.72836764]]    Loss_Validation:  [[ 11.16083494]]\n",
      "Loop  3424 :    Loss_Train:  [[ 12.72836092]]    Loss_Validation:  [[ 11.16085262]]\n",
      "Loop  3425 :    Loss_Train:  [[ 12.72835422]]    Loss_Validation:  [[ 11.16087029]]\n",
      "Loop  3426 :    Loss_Train:  [[ 12.72834752]]    Loss_Validation:  [[ 11.16088795]]\n",
      "Loop  3427 :    Loss_Train:  [[ 12.72834084]]    Loss_Validation:  [[ 11.16090559]]\n",
      "Loop  3428 :    Loss_Train:  [[ 12.72833416]]    Loss_Validation:  [[ 11.16092323]]\n",
      "Loop  3429 :    Loss_Train:  [[ 12.7283275]]    Loss_Validation:  [[ 11.16094086]]\n",
      "Loop  3430 :    Loss_Train:  [[ 12.72832085]]    Loss_Validation:  [[ 11.16095847]]\n",
      "Loop  3431 :    Loss_Train:  [[ 12.72831421]]    Loss_Validation:  [[ 11.16097607]]\n",
      "Loop  3432 :    Loss_Train:  [[ 12.72830758]]    Loss_Validation:  [[ 11.16099367]]\n",
      "Loop  3433 :    Loss_Train:  [[ 12.72830096]]    Loss_Validation:  [[ 11.16101125]]\n",
      "Loop  3434 :    Loss_Train:  [[ 12.72829436]]    Loss_Validation:  [[ 11.16102882]]\n",
      "Loop  3435 :    Loss_Train:  [[ 12.72828776]]    Loss_Validation:  [[ 11.16104638]]\n",
      "Loop  3436 :    Loss_Train:  [[ 12.72828118]]    Loss_Validation:  [[ 11.16106393]]\n",
      "Loop  3437 :    Loss_Train:  [[ 12.7282746]]    Loss_Validation:  [[ 11.16108147]]\n",
      "Loop  3438 :    Loss_Train:  [[ 12.72826804]]    Loss_Validation:  [[ 11.161099]]\n",
      "Loop  3439 :    Loss_Train:  [[ 12.72826149]]    Loss_Validation:  [[ 11.16111652]]\n",
      "Loop  3440 :    Loss_Train:  [[ 12.72825495]]    Loss_Validation:  [[ 11.16113403]]\n",
      "Loop  3441 :    Loss_Train:  [[ 12.72824842]]    Loss_Validation:  [[ 11.16115153]]\n",
      "Loop  3442 :    Loss_Train:  [[ 12.7282419]]    Loss_Validation:  [[ 11.16116901]]\n",
      "Loop  3443 :    Loss_Train:  [[ 12.7282354]]    Loss_Validation:  [[ 11.16118649]]\n",
      "Loop  3444 :    Loss_Train:  [[ 12.7282289]]    Loss_Validation:  [[ 11.16120395]]\n",
      "Loop  3445 :    Loss_Train:  [[ 12.72822241]]    Loss_Validation:  [[ 11.16122141]]\n",
      "Loop  3446 :    Loss_Train:  [[ 12.72821594]]    Loss_Validation:  [[ 11.16123885]]\n",
      "Loop  3447 :    Loss_Train:  [[ 12.72820948]]    Loss_Validation:  [[ 11.16125628]]\n",
      "Loop  3448 :    Loss_Train:  [[ 12.72820302]]    Loss_Validation:  [[ 11.1612737]]\n",
      "Loop  3449 :    Loss_Train:  [[ 12.72819658]]    Loss_Validation:  [[ 11.16129112]]\n",
      "Loop  3450 :    Loss_Train:  [[ 12.72819015]]    Loss_Validation:  [[ 11.16130852]]\n",
      "Loop  3451 :    Loss_Train:  [[ 12.72818373]]    Loss_Validation:  [[ 11.16132591]]\n",
      "Loop  3452 :    Loss_Train:  [[ 12.72817732]]    Loss_Validation:  [[ 11.16134329]]\n",
      "Loop  3453 :    Loss_Train:  [[ 12.72817092]]    Loss_Validation:  [[ 11.16136066]]\n",
      "Loop  3454 :    Loss_Train:  [[ 12.72816454]]    Loss_Validation:  [[ 11.16137801]]\n",
      "Loop  3455 :    Loss_Train:  [[ 12.72815816]]    Loss_Validation:  [[ 11.16139536]]\n",
      "Loop  3456 :    Loss_Train:  [[ 12.72815179]]    Loss_Validation:  [[ 11.1614127]]\n",
      "Loop  3457 :    Loss_Train:  [[ 12.72814544]]    Loss_Validation:  [[ 11.16143002]]\n",
      "Loop  3458 :    Loss_Train:  [[ 12.72813909]]    Loss_Validation:  [[ 11.16144734]]\n",
      "Loop  3459 :    Loss_Train:  [[ 12.72813276]]    Loss_Validation:  [[ 11.16146464]]\n",
      "Loop  3460 :    Loss_Train:  [[ 12.72812643]]    Loss_Validation:  [[ 11.16148194]]\n",
      "Loop  3461 :    Loss_Train:  [[ 12.72812012]]    Loss_Validation:  [[ 11.16149922]]\n",
      "Loop  3462 :    Loss_Train:  [[ 12.72811382]]    Loss_Validation:  [[ 11.1615165]]\n",
      "Loop  3463 :    Loss_Train:  [[ 12.72810753]]    Loss_Validation:  [[ 11.16153376]]\n",
      "Loop  3464 :    Loss_Train:  [[ 12.72810125]]    Loss_Validation:  [[ 11.16155101]]\n",
      "Loop  3465 :    Loss_Train:  [[ 12.72809497]]    Loss_Validation:  [[ 11.16156825]]\n",
      "Loop  3466 :    Loss_Train:  [[ 12.72808871]]    Loss_Validation:  [[ 11.16158548]]\n",
      "Loop  3467 :    Loss_Train:  [[ 12.72808247]]    Loss_Validation:  [[ 11.1616027]]\n",
      "Loop  3468 :    Loss_Train:  [[ 12.72807623]]    Loss_Validation:  [[ 11.16161991]]\n",
      "Loop  3469 :    Loss_Train:  [[ 12.72807]]    Loss_Validation:  [[ 11.16163711]]\n",
      "Loop  3470 :    Loss_Train:  [[ 12.72806378]]    Loss_Validation:  [[ 11.1616543]]\n",
      "Loop  3471 :    Loss_Train:  [[ 12.72805757]]    Loss_Validation:  [[ 11.16167148]]\n",
      "Loop  3472 :    Loss_Train:  [[ 12.72805137]]    Loss_Validation:  [[ 11.16168865]]\n",
      "Loop  3473 :    Loss_Train:  [[ 12.72804519]]    Loss_Validation:  [[ 11.1617058]]\n",
      "Loop  3474 :    Loss_Train:  [[ 12.72803901]]    Loss_Validation:  [[ 11.16172295]]\n",
      "Loop  3475 :    Loss_Train:  [[ 12.72803284]]    Loss_Validation:  [[ 11.16174009]]\n",
      "Loop  3476 :    Loss_Train:  [[ 12.72802669]]    Loss_Validation:  [[ 11.16175721]]\n",
      "Loop  3477 :    Loss_Train:  [[ 12.72802054]]    Loss_Validation:  [[ 11.16177433]]\n",
      "Loop  3478 :    Loss_Train:  [[ 12.72801441]]    Loss_Validation:  [[ 11.16179143]]\n",
      "Loop  3479 :    Loss_Train:  [[ 12.72800828]]    Loss_Validation:  [[ 11.16180853]]\n",
      "Loop  3480 :    Loss_Train:  [[ 12.72800217]]    Loss_Validation:  [[ 11.16182561]]\n",
      "Loop  3481 :    Loss_Train:  [[ 12.72799606]]    Loss_Validation:  [[ 11.16184268]]\n",
      "Loop  3482 :    Loss_Train:  [[ 12.72798997]]    Loss_Validation:  [[ 11.16185974]]\n",
      "Loop  3483 :    Loss_Train:  [[ 12.72798389]]    Loss_Validation:  [[ 11.1618768]]\n",
      "Loop  3484 :    Loss_Train:  [[ 12.72797781]]    Loss_Validation:  [[ 11.16189384]]\n",
      "Loop  3485 :    Loss_Train:  [[ 12.72797175]]    Loss_Validation:  [[ 11.16191087]]\n",
      "Loop  3486 :    Loss_Train:  [[ 12.7279657]]    Loss_Validation:  [[ 11.16192789]]\n",
      "Loop  3487 :    Loss_Train:  [[ 12.72795965]]    Loss_Validation:  [[ 11.1619449]]\n",
      "Loop  3488 :    Loss_Train:  [[ 12.72795362]]    Loss_Validation:  [[ 11.1619619]]\n",
      "Loop  3489 :    Loss_Train:  [[ 12.7279476]]    Loss_Validation:  [[ 11.16197889]]\n",
      "Loop  3490 :    Loss_Train:  [[ 12.72794159]]    Loss_Validation:  [[ 11.16199587]]\n",
      "Loop  3491 :    Loss_Train:  [[ 12.72793558]]    Loss_Validation:  [[ 11.16201283]]\n",
      "Loop  3492 :    Loss_Train:  [[ 12.72792959]]    Loss_Validation:  [[ 11.16202979]]\n",
      "Loop  3493 :    Loss_Train:  [[ 12.72792361]]    Loss_Validation:  [[ 11.16204674]]\n",
      "Loop  3494 :    Loss_Train:  [[ 12.72791764]]    Loss_Validation:  [[ 11.16206368]]\n",
      "Loop  3495 :    Loss_Train:  [[ 12.72791167]]    Loss_Validation:  [[ 11.1620806]]\n",
      "Loop  3496 :    Loss_Train:  [[ 12.72790572]]    Loss_Validation:  [[ 11.16209752]]\n",
      "Loop  3497 :    Loss_Train:  [[ 12.72789978]]    Loss_Validation:  [[ 11.16211442]]\n",
      "Loop  3498 :    Loss_Train:  [[ 12.72789385]]    Loss_Validation:  [[ 11.16213132]]\n",
      "Loop  3499 :    Loss_Train:  [[ 12.72788792]]    Loss_Validation:  [[ 11.1621482]]\n",
      "Loop  3500 :    Loss_Train:  [[ 12.72788201]]    Loss_Validation:  [[ 11.16216508]]\n",
      "Loop  3501 :    Loss_Train:  [[ 12.72787611]]    Loss_Validation:  [[ 11.16218194]]\n",
      "Loop  3502 :    Loss_Train:  [[ 12.72787022]]    Loss_Validation:  [[ 11.1621988]]\n",
      "Loop  3503 :    Loss_Train:  [[ 12.72786433]]    Loss_Validation:  [[ 11.16221564]]\n",
      "Loop  3504 :    Loss_Train:  [[ 12.72785846]]    Loss_Validation:  [[ 11.16223247]]\n",
      "Loop  3505 :    Loss_Train:  [[ 12.7278526]]    Loss_Validation:  [[ 11.1622493]]\n",
      "Loop  3506 :    Loss_Train:  [[ 12.72784675]]    Loss_Validation:  [[ 11.16226611]]\n",
      "Loop  3507 :    Loss_Train:  [[ 12.7278409]]    Loss_Validation:  [[ 11.16228291]]\n",
      "Loop  3508 :    Loss_Train:  [[ 12.72783507]]    Loss_Validation:  [[ 11.1622997]]\n",
      "Loop  3509 :    Loss_Train:  [[ 12.72782924]]    Loss_Validation:  [[ 11.16231648]]\n",
      "Loop  3510 :    Loss_Train:  [[ 12.72782343]]    Loss_Validation:  [[ 11.16233325]]\n",
      "Loop  3511 :    Loss_Train:  [[ 12.72781763]]    Loss_Validation:  [[ 11.16235001]]\n",
      "Loop  3512 :    Loss_Train:  [[ 12.72781183]]    Loss_Validation:  [[ 11.16236676]]\n",
      "Loop  3513 :    Loss_Train:  [[ 12.72780605]]    Loss_Validation:  [[ 11.1623835]]\n",
      "Loop  3514 :    Loss_Train:  [[ 12.72780027]]    Loss_Validation:  [[ 11.16240023]]\n",
      "Loop  3515 :    Loss_Train:  [[ 12.72779451]]    Loss_Validation:  [[ 11.16241695]]\n",
      "Loop  3516 :    Loss_Train:  [[ 12.72778875]]    Loss_Validation:  [[ 11.16243366]]\n",
      "Loop  3517 :    Loss_Train:  [[ 12.72778301]]    Loss_Validation:  [[ 11.16245036]]\n",
      "Loop  3518 :    Loss_Train:  [[ 12.72777727]]    Loss_Validation:  [[ 11.16246705]]\n",
      "Loop  3519 :    Loss_Train:  [[ 12.72777154]]    Loss_Validation:  [[ 11.16248372]]\n",
      "Loop  3520 :    Loss_Train:  [[ 12.72776582]]    Loss_Validation:  [[ 11.16250039]]\n",
      "Loop  3521 :    Loss_Train:  [[ 12.72776012]]    Loss_Validation:  [[ 11.16251705]]\n",
      "Loop  3522 :    Loss_Train:  [[ 12.72775442]]    Loss_Validation:  [[ 11.1625337]]\n",
      "Loop  3523 :    Loss_Train:  [[ 12.72774873]]    Loss_Validation:  [[ 11.16255033]]\n",
      "Loop  3524 :    Loss_Train:  [[ 12.72774305]]    Loss_Validation:  [[ 11.16256696]]\n",
      "Loop  3525 :    Loss_Train:  [[ 12.72773738]]    Loss_Validation:  [[ 11.16258357]]\n",
      "Loop  3526 :    Loss_Train:  [[ 12.72773172]]    Loss_Validation:  [[ 11.16260018]]\n",
      "Loop  3527 :    Loss_Train:  [[ 12.72772607]]    Loss_Validation:  [[ 11.16261678]]\n",
      "Loop  3528 :    Loss_Train:  [[ 12.72772043]]    Loss_Validation:  [[ 11.16263336]]\n",
      "Loop  3529 :    Loss_Train:  [[ 12.7277148]]    Loss_Validation:  [[ 11.16264994]]\n",
      "Loop  3530 :    Loss_Train:  [[ 12.72770918]]    Loss_Validation:  [[ 11.1626665]]\n",
      "Loop  3531 :    Loss_Train:  [[ 12.72770357]]    Loss_Validation:  [[ 11.16268306]]\n",
      "Loop  3532 :    Loss_Train:  [[ 12.72769796]]    Loss_Validation:  [[ 11.1626996]]\n",
      "Loop  3533 :    Loss_Train:  [[ 12.72769237]]    Loss_Validation:  [[ 11.16271613]]\n",
      "Loop  3534 :    Loss_Train:  [[ 12.72768678]]    Loss_Validation:  [[ 11.16273266]]\n",
      "Loop  3535 :    Loss_Train:  [[ 12.72768121]]    Loss_Validation:  [[ 11.16274917]]\n",
      "Loop  3536 :    Loss_Train:  [[ 12.72767564]]    Loss_Validation:  [[ 11.16276568]]\n",
      "Loop  3537 :    Loss_Train:  [[ 12.72767009]]    Loss_Validation:  [[ 11.16278217]]\n",
      "Loop  3538 :    Loss_Train:  [[ 12.72766454]]    Loss_Validation:  [[ 11.16279865]]\n",
      "Loop  3539 :    Loss_Train:  [[ 12.727659]]    Loss_Validation:  [[ 11.16281512]]\n",
      "Loop  3540 :    Loss_Train:  [[ 12.72765347]]    Loss_Validation:  [[ 11.16283159]]\n",
      "Loop  3541 :    Loss_Train:  [[ 12.72764796]]    Loss_Validation:  [[ 11.16284804]]\n",
      "Loop  3542 :    Loss_Train:  [[ 12.72764245]]    Loss_Validation:  [[ 11.16286448]]\n",
      "Loop  3543 :    Loss_Train:  [[ 12.72763694]]    Loss_Validation:  [[ 11.16288091]]\n",
      "Loop  3544 :    Loss_Train:  [[ 12.72763145]]    Loss_Validation:  [[ 11.16289734]]\n",
      "Loop  3545 :    Loss_Train:  [[ 12.72762597]]    Loss_Validation:  [[ 11.16291375]]\n",
      "Loop  3546 :    Loss_Train:  [[ 12.7276205]]    Loss_Validation:  [[ 11.16293015]]\n",
      "Loop  3547 :    Loss_Train:  [[ 12.72761503]]    Loss_Validation:  [[ 11.16294654]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  3548 :    Loss_Train:  [[ 12.72760958]]    Loss_Validation:  [[ 11.16296292]]\n",
      "Loop  3549 :    Loss_Train:  [[ 12.72760413]]    Loss_Validation:  [[ 11.16297929]]\n",
      "Loop  3550 :    Loss_Train:  [[ 12.7275987]]    Loss_Validation:  [[ 11.16299565]]\n",
      "Loop  3551 :    Loss_Train:  [[ 12.72759327]]    Loss_Validation:  [[ 11.163012]]\n",
      "Loop  3552 :    Loss_Train:  [[ 12.72758785]]    Loss_Validation:  [[ 11.16302835]]\n",
      "Loop  3553 :    Loss_Train:  [[ 12.72758244]]    Loss_Validation:  [[ 11.16304468]]\n",
      "Loop  3554 :    Loss_Train:  [[ 12.72757704]]    Loss_Validation:  [[ 11.163061]]\n",
      "Loop  3555 :    Loss_Train:  [[ 12.72757165]]    Loss_Validation:  [[ 11.16307731]]\n",
      "Loop  3556 :    Loss_Train:  [[ 12.72756627]]    Loss_Validation:  [[ 11.16309361]]\n",
      "Loop  3557 :    Loss_Train:  [[ 12.7275609]]    Loss_Validation:  [[ 11.1631099]]\n",
      "Loop  3558 :    Loss_Train:  [[ 12.72755553]]    Loss_Validation:  [[ 11.16312618]]\n",
      "Loop  3559 :    Loss_Train:  [[ 12.72755018]]    Loss_Validation:  [[ 11.16314245]]\n",
      "Loop  3560 :    Loss_Train:  [[ 12.72754483]]    Loss_Validation:  [[ 11.16315871]]\n",
      "Loop  3561 :    Loss_Train:  [[ 12.72753949]]    Loss_Validation:  [[ 11.16317496]]\n",
      "Loop  3562 :    Loss_Train:  [[ 12.72753417]]    Loss_Validation:  [[ 11.1631912]]\n",
      "Loop  3563 :    Loss_Train:  [[ 12.72752885]]    Loss_Validation:  [[ 11.16320743]]\n",
      "Loop  3564 :    Loss_Train:  [[ 12.72752354]]    Loss_Validation:  [[ 11.16322365]]\n",
      "Loop  3565 :    Loss_Train:  [[ 12.72751824]]    Loss_Validation:  [[ 11.16323985]]\n",
      "Loop  3566 :    Loss_Train:  [[ 12.72751294]]    Loss_Validation:  [[ 11.16325605]]\n",
      "Loop  3567 :    Loss_Train:  [[ 12.72750766]]    Loss_Validation:  [[ 11.16327224]]\n",
      "Loop  3568 :    Loss_Train:  [[ 12.72750238]]    Loss_Validation:  [[ 11.16328842]]\n",
      "Loop  3569 :    Loss_Train:  [[ 12.72749712]]    Loss_Validation:  [[ 11.16330459]]\n",
      "Loop  3570 :    Loss_Train:  [[ 12.72749186]]    Loss_Validation:  [[ 11.16332075]]\n",
      "Loop  3571 :    Loss_Train:  [[ 12.72748661]]    Loss_Validation:  [[ 11.1633369]]\n",
      "Loop  3572 :    Loss_Train:  [[ 12.72748137]]    Loss_Validation:  [[ 11.16335304]]\n",
      "Loop  3573 :    Loss_Train:  [[ 12.72747614]]    Loss_Validation:  [[ 11.16336917]]\n",
      "Loop  3574 :    Loss_Train:  [[ 12.72747092]]    Loss_Validation:  [[ 11.16338529]]\n",
      "Loop  3575 :    Loss_Train:  [[ 12.72746571]]    Loss_Validation:  [[ 11.1634014]]\n",
      "Loop  3576 :    Loss_Train:  [[ 12.7274605]]    Loss_Validation:  [[ 11.1634175]]\n",
      "Loop  3577 :    Loss_Train:  [[ 12.72745531]]    Loss_Validation:  [[ 11.16343359]]\n",
      "Loop  3578 :    Loss_Train:  [[ 12.72745012]]    Loss_Validation:  [[ 11.16344966]]\n",
      "Loop  3579 :    Loss_Train:  [[ 12.72744494]]    Loss_Validation:  [[ 11.16346573]]\n",
      "Loop  3580 :    Loss_Train:  [[ 12.72743977]]    Loss_Validation:  [[ 11.16348179]]\n",
      "Loop  3581 :    Loss_Train:  [[ 12.72743461]]    Loss_Validation:  [[ 11.16349784]]\n",
      "Loop  3582 :    Loss_Train:  [[ 12.72742946]]    Loss_Validation:  [[ 11.16351388]]\n",
      "Loop  3583 :    Loss_Train:  [[ 12.72742431]]    Loss_Validation:  [[ 11.16352991]]\n",
      "Loop  3584 :    Loss_Train:  [[ 12.72741918]]    Loss_Validation:  [[ 11.16354593]]\n",
      "Loop  3585 :    Loss_Train:  [[ 12.72741405]]    Loss_Validation:  [[ 11.16356194]]\n",
      "Loop  3586 :    Loss_Train:  [[ 12.72740893]]    Loss_Validation:  [[ 11.16357794]]\n",
      "Loop  3587 :    Loss_Train:  [[ 12.72740383]]    Loss_Validation:  [[ 11.16359393]]\n",
      "Loop  3588 :    Loss_Train:  [[ 12.72739872]]    Loss_Validation:  [[ 11.16360991]]\n",
      "Loop  3589 :    Loss_Train:  [[ 12.72739363]]    Loss_Validation:  [[ 11.16362588]]\n",
      "Loop  3590 :    Loss_Train:  [[ 12.72738855]]    Loss_Validation:  [[ 11.16364183]]\n",
      "Loop  3591 :    Loss_Train:  [[ 12.72738347]]    Loss_Validation:  [[ 11.16365778]]\n",
      "Loop  3592 :    Loss_Train:  [[ 12.72737841]]    Loss_Validation:  [[ 11.16367372]]\n",
      "Loop  3593 :    Loss_Train:  [[ 12.72737335]]    Loss_Validation:  [[ 11.16368965]]\n",
      "Loop  3594 :    Loss_Train:  [[ 12.7273683]]    Loss_Validation:  [[ 11.16370557]]\n",
      "Loop  3595 :    Loss_Train:  [[ 12.72736326]]    Loss_Validation:  [[ 11.16372148]]\n",
      "Loop  3596 :    Loss_Train:  [[ 12.72735822]]    Loss_Validation:  [[ 11.16373738]]\n",
      "Loop  3597 :    Loss_Train:  [[ 12.7273532]]    Loss_Validation:  [[ 11.16375327]]\n",
      "Loop  3598 :    Loss_Train:  [[ 12.72734818]]    Loss_Validation:  [[ 11.16376915]]\n",
      "Loop  3599 :    Loss_Train:  [[ 12.72734318]]    Loss_Validation:  [[ 11.16378502]]\n",
      "Loop  3600 :    Loss_Train:  [[ 12.72733818]]    Loss_Validation:  [[ 11.16380088]]\n",
      "Loop  3601 :    Loss_Train:  [[ 12.72733318]]    Loss_Validation:  [[ 11.16381673]]\n",
      "Loop  3602 :    Loss_Train:  [[ 12.7273282]]    Loss_Validation:  [[ 11.16383258]]\n",
      "Loop  3603 :    Loss_Train:  [[ 12.72732323]]    Loss_Validation:  [[ 11.16384841]]\n",
      "Loop  3604 :    Loss_Train:  [[ 12.72731826]]    Loss_Validation:  [[ 11.16386423]]\n",
      "Loop  3605 :    Loss_Train:  [[ 12.7273133]]    Loss_Validation:  [[ 11.16388004]]\n",
      "Loop  3606 :    Loss_Train:  [[ 12.72730835]]    Loss_Validation:  [[ 11.16389584]]\n",
      "Loop  3607 :    Loss_Train:  [[ 12.72730341]]    Loss_Validation:  [[ 11.16391163]]\n",
      "Loop  3608 :    Loss_Train:  [[ 12.72729848]]    Loss_Validation:  [[ 11.16392741]]\n",
      "Loop  3609 :    Loss_Train:  [[ 12.72729356]]    Loss_Validation:  [[ 11.16394318]]\n",
      "Loop  3610 :    Loss_Train:  [[ 12.72728864]]    Loss_Validation:  [[ 11.16395895]]\n",
      "Loop  3611 :    Loss_Train:  [[ 12.72728373]]    Loss_Validation:  [[ 11.1639747]]\n",
      "Loop  3612 :    Loss_Train:  [[ 12.72727883]]    Loss_Validation:  [[ 11.16399044]]\n",
      "Loop  3613 :    Loss_Train:  [[ 12.72727394]]    Loss_Validation:  [[ 11.16400617]]\n",
      "Loop  3614 :    Loss_Train:  [[ 12.72726906]]    Loss_Validation:  [[ 11.16402189]]\n",
      "Loop  3615 :    Loss_Train:  [[ 12.72726418]]    Loss_Validation:  [[ 11.16403761]]\n",
      "Loop  3616 :    Loss_Train:  [[ 12.72725931]]    Loss_Validation:  [[ 11.16405331]]\n",
      "Loop  3617 :    Loss_Train:  [[ 12.72725445]]    Loss_Validation:  [[ 11.164069]]\n",
      "Loop  3618 :    Loss_Train:  [[ 12.7272496]]    Loss_Validation:  [[ 11.16408469]]\n",
      "Loop  3619 :    Loss_Train:  [[ 12.72724476]]    Loss_Validation:  [[ 11.16410036]]\n",
      "Loop  3620 :    Loss_Train:  [[ 12.72723993]]    Loss_Validation:  [[ 11.16411602]]\n",
      "Loop  3621 :    Loss_Train:  [[ 12.7272351]]    Loss_Validation:  [[ 11.16413168]]\n",
      "Loop  3622 :    Loss_Train:  [[ 12.72723028]]    Loss_Validation:  [[ 11.16414732]]\n",
      "Loop  3623 :    Loss_Train:  [[ 12.72722547]]    Loss_Validation:  [[ 11.16416296]]\n",
      "Loop  3624 :    Loss_Train:  [[ 12.72722067]]    Loss_Validation:  [[ 11.16417858]]\n",
      "Loop  3625 :    Loss_Train:  [[ 12.72721587]]    Loss_Validation:  [[ 11.1641942]]\n",
      "Loop  3626 :    Loss_Train:  [[ 12.72721109]]    Loss_Validation:  [[ 11.1642098]]\n",
      "Loop  3627 :    Loss_Train:  [[ 12.72720631]]    Loss_Validation:  [[ 11.1642254]]\n",
      "Loop  3628 :    Loss_Train:  [[ 12.72720154]]    Loss_Validation:  [[ 11.16424098]]\n",
      "Loop  3629 :    Loss_Train:  [[ 12.72719678]]    Loss_Validation:  [[ 11.16425656]]\n",
      "Loop  3630 :    Loss_Train:  [[ 12.72719202]]    Loss_Validation:  [[ 11.16427212]]\n",
      "Loop  3631 :    Loss_Train:  [[ 12.72718728]]    Loss_Validation:  [[ 11.16428768]]\n",
      "Loop  3632 :    Loss_Train:  [[ 12.72718254]]    Loss_Validation:  [[ 11.16430323]]\n",
      "Loop  3633 :    Loss_Train:  [[ 12.72717781]]    Loss_Validation:  [[ 11.16431876]]\n",
      "Loop  3634 :    Loss_Train:  [[ 12.72717308]]    Loss_Validation:  [[ 11.16433429]]\n",
      "Loop  3635 :    Loss_Train:  [[ 12.72716837]]    Loss_Validation:  [[ 11.16434981]]\n",
      "Loop  3636 :    Loss_Train:  [[ 12.72716366]]    Loss_Validation:  [[ 11.16436532]]\n",
      "Loop  3637 :    Loss_Train:  [[ 12.72715896]]    Loss_Validation:  [[ 11.16438082]]\n",
      "Loop  3638 :    Loss_Train:  [[ 12.72715427]]    Loss_Validation:  [[ 11.1643963]]\n",
      "Loop  3639 :    Loss_Train:  [[ 12.72714959]]    Loss_Validation:  [[ 11.16441178]]\n",
      "Loop  3640 :    Loss_Train:  [[ 12.72714491]]    Loss_Validation:  [[ 11.16442725]]\n",
      "Loop  3641 :    Loss_Train:  [[ 12.72714024]]    Loss_Validation:  [[ 11.16444271]]\n",
      "Loop  3642 :    Loss_Train:  [[ 12.72713558]]    Loss_Validation:  [[ 11.16445816]]\n",
      "Loop  3643 :    Loss_Train:  [[ 12.72713093]]    Loss_Validation:  [[ 11.1644736]]\n",
      "Loop  3644 :    Loss_Train:  [[ 12.72712629]]    Loss_Validation:  [[ 11.16448903]]\n",
      "Loop  3645 :    Loss_Train:  [[ 12.72712165]]    Loss_Validation:  [[ 11.16450445]]\n",
      "Loop  3646 :    Loss_Train:  [[ 12.72711702]]    Loss_Validation:  [[ 11.16451986]]\n",
      "Loop  3647 :    Loss_Train:  [[ 12.7271124]]    Loss_Validation:  [[ 11.16453526]]\n",
      "Loop  3648 :    Loss_Train:  [[ 12.72710779]]    Loss_Validation:  [[ 11.16455066]]\n",
      "Loop  3649 :    Loss_Train:  [[ 12.72710318]]    Loss_Validation:  [[ 11.16456604]]\n",
      "Loop  3650 :    Loss_Train:  [[ 12.72709858]]    Loss_Validation:  [[ 11.16458141]]\n",
      "Loop  3651 :    Loss_Train:  [[ 12.72709399]]    Loss_Validation:  [[ 11.16459677]]\n",
      "Loop  3652 :    Loss_Train:  [[ 12.72708941]]    Loss_Validation:  [[ 11.16461213]]\n",
      "Loop  3653 :    Loss_Train:  [[ 12.72708484]]    Loss_Validation:  [[ 11.16462747]]\n",
      "Loop  3654 :    Loss_Train:  [[ 12.72708027]]    Loss_Validation:  [[ 11.16464281]]\n",
      "Loop  3655 :    Loss_Train:  [[ 12.72707571]]    Loss_Validation:  [[ 11.16465813]]\n",
      "Loop  3656 :    Loss_Train:  [[ 12.72707116]]    Loss_Validation:  [[ 11.16467344]]\n",
      "Loop  3657 :    Loss_Train:  [[ 12.72706661]]    Loss_Validation:  [[ 11.16468875]]\n",
      "Loop  3658 :    Loss_Train:  [[ 12.72706208]]    Loss_Validation:  [[ 11.16470405]]\n",
      "Loop  3659 :    Loss_Train:  [[ 12.72705755]]    Loss_Validation:  [[ 11.16471933]]\n",
      "Loop  3660 :    Loss_Train:  [[ 12.72705302]]    Loss_Validation:  [[ 11.16473461]]\n",
      "Loop  3661 :    Loss_Train:  [[ 12.72704851]]    Loss_Validation:  [[ 11.16474988]]\n",
      "Loop  3662 :    Loss_Train:  [[ 12.727044]]    Loss_Validation:  [[ 11.16476513]]\n",
      "Loop  3663 :    Loss_Train:  [[ 12.7270395]]    Loss_Validation:  [[ 11.16478038]]\n",
      "Loop  3664 :    Loss_Train:  [[ 12.72703501]]    Loss_Validation:  [[ 11.16479562]]\n",
      "Loop  3665 :    Loss_Train:  [[ 12.72703053]]    Loss_Validation:  [[ 11.16481085]]\n",
      "Loop  3666 :    Loss_Train:  [[ 12.72702605]]    Loss_Validation:  [[ 11.16482607]]\n",
      "Loop  3667 :    Loss_Train:  [[ 12.72702158]]    Loss_Validation:  [[ 11.16484128]]\n",
      "Loop  3668 :    Loss_Train:  [[ 12.72701712]]    Loss_Validation:  [[ 11.16485648]]\n",
      "Loop  3669 :    Loss_Train:  [[ 12.72701267]]    Loss_Validation:  [[ 11.16487167]]\n",
      "Loop  3670 :    Loss_Train:  [[ 12.72700822]]    Loss_Validation:  [[ 11.16488685]]\n",
      "Loop  3671 :    Loss_Train:  [[ 12.72700378]]    Loss_Validation:  [[ 11.16490202]]\n",
      "Loop  3672 :    Loss_Train:  [[ 12.72699935]]    Loss_Validation:  [[ 11.16491718]]\n",
      "Loop  3673 :    Loss_Train:  [[ 12.72699492]]    Loss_Validation:  [[ 11.16493233]]\n",
      "Loop  3674 :    Loss_Train:  [[ 12.72699051]]    Loss_Validation:  [[ 11.16494748]]\n",
      "Loop  3675 :    Loss_Train:  [[ 12.7269861]]    Loss_Validation:  [[ 11.16496261]]\n",
      "Loop  3676 :    Loss_Train:  [[ 12.72698169]]    Loss_Validation:  [[ 11.16497773]]\n",
      "Loop  3677 :    Loss_Train:  [[ 12.7269773]]    Loss_Validation:  [[ 11.16499285]]\n",
      "Loop  3678 :    Loss_Train:  [[ 12.72697291]]    Loss_Validation:  [[ 11.16500795]]\n",
      "Loop  3679 :    Loss_Train:  [[ 12.72696853]]    Loss_Validation:  [[ 11.16502305]]\n",
      "Loop  3680 :    Loss_Train:  [[ 12.72696416]]    Loss_Validation:  [[ 11.16503813]]\n",
      "Loop  3681 :    Loss_Train:  [[ 12.72695979]]    Loss_Validation:  [[ 11.16505321]]\n",
      "Loop  3682 :    Loss_Train:  [[ 12.72695543]]    Loss_Validation:  [[ 11.16506828]]\n",
      "Loop  3683 :    Loss_Train:  [[ 12.72695108]]    Loss_Validation:  [[ 11.16508334]]\n",
      "Loop  3684 :    Loss_Train:  [[ 12.72694674]]    Loss_Validation:  [[ 11.16509838]]\n",
      "Loop  3685 :    Loss_Train:  [[ 12.7269424]]    Loss_Validation:  [[ 11.16511342]]\n",
      "Loop  3686 :    Loss_Train:  [[ 12.72693807]]    Loss_Validation:  [[ 11.16512845]]\n",
      "Loop  3687 :    Loss_Train:  [[ 12.72693375]]    Loss_Validation:  [[ 11.16514347]]\n",
      "Loop  3688 :    Loss_Train:  [[ 12.72692944]]    Loss_Validation:  [[ 11.16515848]]\n",
      "Loop  3689 :    Loss_Train:  [[ 12.72692513]]    Loss_Validation:  [[ 11.16517348]]\n",
      "Loop  3690 :    Loss_Train:  [[ 12.72692083]]    Loss_Validation:  [[ 11.16518847]]\n",
      "Loop  3691 :    Loss_Train:  [[ 12.72691653]]    Loss_Validation:  [[ 11.16520346]]\n",
      "Loop  3692 :    Loss_Train:  [[ 12.72691225]]    Loss_Validation:  [[ 11.16521843]]\n",
      "Loop  3693 :    Loss_Train:  [[ 12.72690797]]    Loss_Validation:  [[ 11.16523339]]\n",
      "Loop  3694 :    Loss_Train:  [[ 12.7269037]]    Loss_Validation:  [[ 11.16524834]]\n",
      "Loop  3695 :    Loss_Train:  [[ 12.72689943]]    Loss_Validation:  [[ 11.16526329]]\n",
      "Loop  3696 :    Loss_Train:  [[ 12.72689517]]    Loss_Validation:  [[ 11.16527822]]\n",
      "Loop  3697 :    Loss_Train:  [[ 12.72689092]]    Loss_Validation:  [[ 11.16529315]]\n",
      "Loop  3698 :    Loss_Train:  [[ 12.72688668]]    Loss_Validation:  [[ 11.16530807]]\n",
      "Loop  3699 :    Loss_Train:  [[ 12.72688244]]    Loss_Validation:  [[ 11.16532297]]\n",
      "Loop  3700 :    Loss_Train:  [[ 12.72687821]]    Loss_Validation:  [[ 11.16533787]]\n",
      "Loop  3701 :    Loss_Train:  [[ 12.72687399]]    Loss_Validation:  [[ 11.16535276]]\n",
      "Loop  3702 :    Loss_Train:  [[ 12.72686978]]    Loss_Validation:  [[ 11.16536764]]\n",
      "Loop  3703 :    Loss_Train:  [[ 12.72686557]]    Loss_Validation:  [[ 11.16538251]]\n",
      "Loop  3704 :    Loss_Train:  [[ 12.72686137]]    Loss_Validation:  [[ 11.16539737]]\n",
      "Loop  3705 :    Loss_Train:  [[ 12.72685717]]    Loss_Validation:  [[ 11.16541222]]\n",
      "Loop  3706 :    Loss_Train:  [[ 12.72685299]]    Loss_Validation:  [[ 11.16542706]]\n",
      "Loop  3707 :    Loss_Train:  [[ 12.7268488]]    Loss_Validation:  [[ 11.16544189]]\n",
      "Loop  3708 :    Loss_Train:  [[ 12.72684463]]    Loss_Validation:  [[ 11.16545671]]\n",
      "Loop  3709 :    Loss_Train:  [[ 12.72684047]]    Loss_Validation:  [[ 11.16547153]]\n",
      "Loop  3710 :    Loss_Train:  [[ 12.72683631]]    Loss_Validation:  [[ 11.16548633]]\n",
      "Loop  3711 :    Loss_Train:  [[ 12.72683215]]    Loss_Validation:  [[ 11.16550112]]\n",
      "Loop  3712 :    Loss_Train:  [[ 12.72682801]]    Loss_Validation:  [[ 11.16551591]]\n",
      "Loop  3713 :    Loss_Train:  [[ 12.72682387]]    Loss_Validation:  [[ 11.16553068]]\n",
      "Loop  3714 :    Loss_Train:  [[ 12.72681974]]    Loss_Validation:  [[ 11.16554545]]\n",
      "Loop  3715 :    Loss_Train:  [[ 12.72681561]]    Loss_Validation:  [[ 11.16556021]]\n",
      "Loop  3716 :    Loss_Train:  [[ 12.7268115]]    Loss_Validation:  [[ 11.16557496]]\n",
      "Loop  3717 :    Loss_Train:  [[ 12.72680738]]    Loss_Validation:  [[ 11.1655897]]\n",
      "Loop  3718 :    Loss_Train:  [[ 12.72680328]]    Loss_Validation:  [[ 11.16560442]]\n",
      "Loop  3719 :    Loss_Train:  [[ 12.72679918]]    Loss_Validation:  [[ 11.16561914]]\n",
      "Loop  3720 :    Loss_Train:  [[ 12.72679509]]    Loss_Validation:  [[ 11.16563386]]\n",
      "Loop  3721 :    Loss_Train:  [[ 12.72679101]]    Loss_Validation:  [[ 11.16564856]]\n",
      "Loop  3722 :    Loss_Train:  [[ 12.72678693]]    Loss_Validation:  [[ 11.16566325]]\n",
      "Loop  3723 :    Loss_Train:  [[ 12.72678286]]    Loss_Validation:  [[ 11.16567793]]\n",
      "Loop  3724 :    Loss_Train:  [[ 12.7267788]]    Loss_Validation:  [[ 11.16569261]]\n",
      "Loop  3725 :    Loss_Train:  [[ 12.72677474]]    Loss_Validation:  [[ 11.16570727]]\n",
      "Loop  3726 :    Loss_Train:  [[ 12.72677069]]    Loss_Validation:  [[ 11.16572193]]\n",
      "Loop  3727 :    Loss_Train:  [[ 12.72676665]]    Loss_Validation:  [[ 11.16573657]]\n",
      "Loop  3728 :    Loss_Train:  [[ 12.72676261]]    Loss_Validation:  [[ 11.16575121]]\n",
      "Loop  3729 :    Loss_Train:  [[ 12.72675858]]    Loss_Validation:  [[ 11.16576584]]\n",
      "Loop  3730 :    Loss_Train:  [[ 12.72675456]]    Loss_Validation:  [[ 11.16578045]]\n",
      "Loop  3731 :    Loss_Train:  [[ 12.72675055]]    Loss_Validation:  [[ 11.16579506]]\n",
      "Loop  3732 :    Loss_Train:  [[ 12.72674654]]    Loss_Validation:  [[ 11.16580966]]\n",
      "Loop  3733 :    Loss_Train:  [[ 12.72674253]]    Loss_Validation:  [[ 11.16582425]]\n",
      "Loop  3734 :    Loss_Train:  [[ 12.72673854]]    Loss_Validation:  [[ 11.16583884]]\n",
      "Loop  3735 :    Loss_Train:  [[ 12.72673455]]    Loss_Validation:  [[ 11.16585341]]\n",
      "Loop  3736 :    Loss_Train:  [[ 12.72673057]]    Loss_Validation:  [[ 11.16586797]]\n",
      "Loop  3737 :    Loss_Train:  [[ 12.72672659]]    Loss_Validation:  [[ 11.16588252]]\n",
      "Loop  3738 :    Loss_Train:  [[ 12.72672262]]    Loss_Validation:  [[ 11.16589707]]\n",
      "Loop  3739 :    Loss_Train:  [[ 12.72671866]]    Loss_Validation:  [[ 11.1659116]]\n",
      "Loop  3740 :    Loss_Train:  [[ 12.7267147]]    Loss_Validation:  [[ 11.16592613]]\n",
      "Loop  3741 :    Loss_Train:  [[ 12.72671075]]    Loss_Validation:  [[ 11.16594065]]\n",
      "Loop  3742 :    Loss_Train:  [[ 12.72670681]]    Loss_Validation:  [[ 11.16595516]]\n",
      "Loop  3743 :    Loss_Train:  [[ 12.72670287]]    Loss_Validation:  [[ 11.16596965]]\n",
      "Loop  3744 :    Loss_Train:  [[ 12.72669894]]    Loss_Validation:  [[ 11.16598414]]\n",
      "Loop  3745 :    Loss_Train:  [[ 12.72669502]]    Loss_Validation:  [[ 11.16599862]]\n",
      "Loop  3746 :    Loss_Train:  [[ 12.7266911]]    Loss_Validation:  [[ 11.1660131]]\n",
      "Loop  3747 :    Loss_Train:  [[ 12.72668719]]    Loss_Validation:  [[ 11.16602756]]\n",
      "Loop  3748 :    Loss_Train:  [[ 12.72668329]]    Loss_Validation:  [[ 11.16604201]]\n",
      "Loop  3749 :    Loss_Train:  [[ 12.72667939]]    Loss_Validation:  [[ 11.16605645]]\n",
      "Loop  3750 :    Loss_Train:  [[ 12.7266755]]    Loss_Validation:  [[ 11.16607089]]\n",
      "Loop  3751 :    Loss_Train:  [[ 12.72667162]]    Loss_Validation:  [[ 11.16608531]]\n",
      "Loop  3752 :    Loss_Train:  [[ 12.72666774]]    Loss_Validation:  [[ 11.16609973]]\n",
      "Loop  3753 :    Loss_Train:  [[ 12.72666387]]    Loss_Validation:  [[ 11.16611414]]\n",
      "Loop  3754 :    Loss_Train:  [[ 12.72666]]    Loss_Validation:  [[ 11.16612854]]\n",
      "Loop  3755 :    Loss_Train:  [[ 12.72665614]]    Loss_Validation:  [[ 11.16614293]]\n",
      "Loop  3756 :    Loss_Train:  [[ 12.72665229]]    Loss_Validation:  [[ 11.16615731]]\n",
      "Loop  3757 :    Loss_Train:  [[ 12.72664845]]    Loss_Validation:  [[ 11.16617168]]\n",
      "Loop  3758 :    Loss_Train:  [[ 12.72664461]]    Loss_Validation:  [[ 11.16618604]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  3759 :    Loss_Train:  [[ 12.72664077]]    Loss_Validation:  [[ 11.16620039]]\n",
      "Loop  3760 :    Loss_Train:  [[ 12.72663695]]    Loss_Validation:  [[ 11.16621473]]\n",
      "Loop  3761 :    Loss_Train:  [[ 12.72663313]]    Loss_Validation:  [[ 11.16622907]]\n",
      "Loop  3762 :    Loss_Train:  [[ 12.72662931]]    Loss_Validation:  [[ 11.16624339]]\n",
      "Loop  3763 :    Loss_Train:  [[ 12.72662551]]    Loss_Validation:  [[ 11.16625771]]\n",
      "Loop  3764 :    Loss_Train:  [[ 12.72662171]]    Loss_Validation:  [[ 11.16627202]]\n",
      "Loop  3765 :    Loss_Train:  [[ 12.72661791]]    Loss_Validation:  [[ 11.16628632]]\n",
      "Loop  3766 :    Loss_Train:  [[ 12.72661412]]    Loss_Validation:  [[ 11.16630061]]\n",
      "Loop  3767 :    Loss_Train:  [[ 12.72661034]]    Loss_Validation:  [[ 11.16631489]]\n",
      "Loop  3768 :    Loss_Train:  [[ 12.72660656]]    Loss_Validation:  [[ 11.16632916]]\n",
      "Loop  3769 :    Loss_Train:  [[ 12.7266028]]    Loss_Validation:  [[ 11.16634342]]\n",
      "Loop  3770 :    Loss_Train:  [[ 12.72659903]]    Loss_Validation:  [[ 11.16635767]]\n",
      "Loop  3771 :    Loss_Train:  [[ 12.72659528]]    Loss_Validation:  [[ 11.16637192]]\n",
      "Loop  3772 :    Loss_Train:  [[ 12.72659152]]    Loss_Validation:  [[ 11.16638615]]\n",
      "Loop  3773 :    Loss_Train:  [[ 12.72658778]]    Loss_Validation:  [[ 11.16640038]]\n",
      "Loop  3774 :    Loss_Train:  [[ 12.72658404]]    Loss_Validation:  [[ 11.1664146]]\n",
      "Loop  3775 :    Loss_Train:  [[ 12.72658031]]    Loss_Validation:  [[ 11.1664288]]\n",
      "Loop  3776 :    Loss_Train:  [[ 12.72657658]]    Loss_Validation:  [[ 11.166443]]\n",
      "Loop  3777 :    Loss_Train:  [[ 12.72657287]]    Loss_Validation:  [[ 11.16645719]]\n",
      "Loop  3778 :    Loss_Train:  [[ 12.72656915]]    Loss_Validation:  [[ 11.16647137]]\n",
      "Loop  3779 :    Loss_Train:  [[ 12.72656544]]    Loss_Validation:  [[ 11.16648555]]\n",
      "Loop  3780 :    Loss_Train:  [[ 12.72656174]]    Loss_Validation:  [[ 11.16649971]]\n",
      "Loop  3781 :    Loss_Train:  [[ 12.72655805]]    Loss_Validation:  [[ 11.16651386]]\n",
      "Loop  3782 :    Loss_Train:  [[ 12.72655436]]    Loss_Validation:  [[ 11.16652801]]\n",
      "Loop  3783 :    Loss_Train:  [[ 12.72655068]]    Loss_Validation:  [[ 11.16654214]]\n",
      "Loop  3784 :    Loss_Train:  [[ 12.726547]]    Loss_Validation:  [[ 11.16655627]]\n",
      "Loop  3785 :    Loss_Train:  [[ 12.72654333]]    Loss_Validation:  [[ 11.16657039]]\n",
      "Loop  3786 :    Loss_Train:  [[ 12.72653967]]    Loss_Validation:  [[ 11.1665845]]\n",
      "Loop  3787 :    Loss_Train:  [[ 12.72653601]]    Loss_Validation:  [[ 11.1665986]]\n",
      "Loop  3788 :    Loss_Train:  [[ 12.72653236]]    Loss_Validation:  [[ 11.16661269]]\n",
      "Loop  3789 :    Loss_Train:  [[ 12.72652871]]    Loss_Validation:  [[ 11.16662677]]\n",
      "Loop  3790 :    Loss_Train:  [[ 12.72652507]]    Loss_Validation:  [[ 11.16664085]]\n",
      "Loop  3791 :    Loss_Train:  [[ 12.72652144]]    Loss_Validation:  [[ 11.16665491]]\n",
      "Loop  3792 :    Loss_Train:  [[ 12.72651781]]    Loss_Validation:  [[ 11.16666897]]\n",
      "Loop  3793 :    Loss_Train:  [[ 12.72651419]]    Loss_Validation:  [[ 11.16668301]]\n",
      "Loop  3794 :    Loss_Train:  [[ 12.72651057]]    Loss_Validation:  [[ 11.16669705]]\n",
      "Loop  3795 :    Loss_Train:  [[ 12.72650696]]    Loss_Validation:  [[ 11.16671108]]\n",
      "Loop  3796 :    Loss_Train:  [[ 12.72650336]]    Loss_Validation:  [[ 11.1667251]]\n",
      "Loop  3797 :    Loss_Train:  [[ 12.72649976]]    Loss_Validation:  [[ 11.16673911]]\n",
      "Loop  3798 :    Loss_Train:  [[ 12.72649617]]    Loss_Validation:  [[ 11.16675311]]\n",
      "Loop  3799 :    Loss_Train:  [[ 12.72649259]]    Loss_Validation:  [[ 11.16676711]]\n",
      "Loop  3800 :    Loss_Train:  [[ 12.72648901]]    Loss_Validation:  [[ 11.16678109]]\n",
      "Loop  3801 :    Loss_Train:  [[ 12.72648543]]    Loss_Validation:  [[ 11.16679507]]\n",
      "Loop  3802 :    Loss_Train:  [[ 12.72648186]]    Loss_Validation:  [[ 11.16680903]]\n",
      "Loop  3803 :    Loss_Train:  [[ 12.7264783]]    Loss_Validation:  [[ 11.16682299]]\n",
      "Loop  3804 :    Loss_Train:  [[ 12.72647475]]    Loss_Validation:  [[ 11.16683694]]\n",
      "Loop  3805 :    Loss_Train:  [[ 12.7264712]]    Loss_Validation:  [[ 11.16685088]]\n",
      "Loop  3806 :    Loss_Train:  [[ 12.72646765]]    Loss_Validation:  [[ 11.16686481]]\n",
      "Loop  3807 :    Loss_Train:  [[ 12.72646412]]    Loss_Validation:  [[ 11.16687873]]\n",
      "Loop  3808 :    Loss_Train:  [[ 12.72646058]]    Loss_Validation:  [[ 11.16689265]]\n",
      "Loop  3809 :    Loss_Train:  [[ 12.72645706]]    Loss_Validation:  [[ 11.16690655]]\n",
      "Loop  3810 :    Loss_Train:  [[ 12.72645354]]    Loss_Validation:  [[ 11.16692045]]\n",
      "Loop  3811 :    Loss_Train:  [[ 12.72645002]]    Loss_Validation:  [[ 11.16693433]]\n",
      "Loop  3812 :    Loss_Train:  [[ 12.72644651]]    Loss_Validation:  [[ 11.16694821]]\n",
      "Loop  3813 :    Loss_Train:  [[ 12.72644301]]    Loss_Validation:  [[ 11.16696208]]\n",
      "Loop  3814 :    Loss_Train:  [[ 12.72643951]]    Loss_Validation:  [[ 11.16697594]]\n",
      "Loop  3815 :    Loss_Train:  [[ 12.72643602]]    Loss_Validation:  [[ 11.16698979]]\n",
      "Loop  3816 :    Loss_Train:  [[ 12.72643254]]    Loss_Validation:  [[ 11.16700364]]\n",
      "Loop  3817 :    Loss_Train:  [[ 12.72642906]]    Loss_Validation:  [[ 11.16701747]]\n",
      "Loop  3818 :    Loss_Train:  [[ 12.72642558]]    Loss_Validation:  [[ 11.16703129]]\n",
      "Loop  3819 :    Loss_Train:  [[ 12.72642211]]    Loss_Validation:  [[ 11.16704511]]\n",
      "Loop  3820 :    Loss_Train:  [[ 12.72641865]]    Loss_Validation:  [[ 11.16705892]]\n",
      "Loop  3821 :    Loss_Train:  [[ 12.7264152]]    Loss_Validation:  [[ 11.16707272]]\n",
      "Loop  3822 :    Loss_Train:  [[ 12.72641174]]    Loss_Validation:  [[ 11.16708651]]\n",
      "Loop  3823 :    Loss_Train:  [[ 12.7264083]]    Loss_Validation:  [[ 11.16710029]]\n",
      "Loop  3824 :    Loss_Train:  [[ 12.72640486]]    Loss_Validation:  [[ 11.16711406]]\n",
      "Loop  3825 :    Loss_Train:  [[ 12.72640143]]    Loss_Validation:  [[ 11.16712783]]\n",
      "Loop  3826 :    Loss_Train:  [[ 12.726398]]    Loss_Validation:  [[ 11.16714158]]\n",
      "Loop  3827 :    Loss_Train:  [[ 12.72639458]]    Loss_Validation:  [[ 11.16715533]]\n",
      "Loop  3828 :    Loss_Train:  [[ 12.72639116]]    Loss_Validation:  [[ 11.16716906]]\n",
      "Loop  3829 :    Loss_Train:  [[ 12.72638775]]    Loss_Validation:  [[ 11.16718279]]\n",
      "Loop  3830 :    Loss_Train:  [[ 12.72638434]]    Loss_Validation:  [[ 11.16719651]]\n",
      "Loop  3831 :    Loss_Train:  [[ 12.72638094]]    Loss_Validation:  [[ 11.16721022]]\n",
      "Loop  3832 :    Loss_Train:  [[ 12.72637755]]    Loss_Validation:  [[ 11.16722393]]\n",
      "Loop  3833 :    Loss_Train:  [[ 12.72637416]]    Loss_Validation:  [[ 11.16723762]]\n",
      "Loop  3834 :    Loss_Train:  [[ 12.72637078]]    Loss_Validation:  [[ 11.16725131]]\n",
      "Loop  3835 :    Loss_Train:  [[ 12.7263674]]    Loss_Validation:  [[ 11.16726498]]\n",
      "Loop  3836 :    Loss_Train:  [[ 12.72636403]]    Loss_Validation:  [[ 11.16727865]]\n",
      "Loop  3837 :    Loss_Train:  [[ 12.72636066]]    Loss_Validation:  [[ 11.16729231]]\n",
      "Loop  3838 :    Loss_Train:  [[ 12.7263573]]    Loss_Validation:  [[ 11.16730596]]\n",
      "Loop  3839 :    Loss_Train:  [[ 12.72635395]]    Loss_Validation:  [[ 11.1673196]]\n",
      "Loop  3840 :    Loss_Train:  [[ 12.7263506]]    Loss_Validation:  [[ 11.16733323]]\n",
      "Loop  3841 :    Loss_Train:  [[ 12.72634726]]    Loss_Validation:  [[ 11.16734686]]\n",
      "Loop  3842 :    Loss_Train:  [[ 12.72634392]]    Loss_Validation:  [[ 11.16736047]]\n",
      "Loop  3843 :    Loss_Train:  [[ 12.72634059]]    Loss_Validation:  [[ 11.16737408]]\n",
      "Loop  3844 :    Loss_Train:  [[ 12.72633726]]    Loss_Validation:  [[ 11.16738768]]\n",
      "Loop  3845 :    Loss_Train:  [[ 12.72633394]]    Loss_Validation:  [[ 11.16740127]]\n",
      "Loop  3846 :    Loss_Train:  [[ 12.72633062]]    Loss_Validation:  [[ 11.16741485]]\n",
      "Loop  3847 :    Loss_Train:  [[ 12.72632731]]    Loss_Validation:  [[ 11.16742842]]\n",
      "Loop  3848 :    Loss_Train:  [[ 12.72632401]]    Loss_Validation:  [[ 11.16744198]]\n",
      "Loop  3849 :    Loss_Train:  [[ 12.72632071]]    Loss_Validation:  [[ 11.16745554]]\n",
      "Loop  3850 :    Loss_Train:  [[ 12.72631742]]    Loss_Validation:  [[ 11.16746909]]\n",
      "Loop  3851 :    Loss_Train:  [[ 12.72631413]]    Loss_Validation:  [[ 11.16748262]]\n",
      "Loop  3852 :    Loss_Train:  [[ 12.72631084]]    Loss_Validation:  [[ 11.16749615]]\n",
      "Loop  3853 :    Loss_Train:  [[ 12.72630757]]    Loss_Validation:  [[ 11.16750967]]\n",
      "Loop  3854 :    Loss_Train:  [[ 12.7263043]]    Loss_Validation:  [[ 11.16752318]]\n",
      "Loop  3855 :    Loss_Train:  [[ 12.72630103]]    Loss_Validation:  [[ 11.16753669]]\n",
      "Loop  3856 :    Loss_Train:  [[ 12.72629777]]    Loss_Validation:  [[ 11.16755018]]\n",
      "Loop  3857 :    Loss_Train:  [[ 12.72629451]]    Loss_Validation:  [[ 11.16756367]]\n",
      "Loop  3858 :    Loss_Train:  [[ 12.72629126]]    Loss_Validation:  [[ 11.16757715]]\n",
      "Loop  3859 :    Loss_Train:  [[ 12.72628802]]    Loss_Validation:  [[ 11.16759061]]\n",
      "Loop  3860 :    Loss_Train:  [[ 12.72628478]]    Loss_Validation:  [[ 11.16760407]]\n",
      "Loop  3861 :    Loss_Train:  [[ 12.72628154]]    Loss_Validation:  [[ 11.16761753]]\n",
      "Loop  3862 :    Loss_Train:  [[ 12.72627832]]    Loss_Validation:  [[ 11.16763097]]\n",
      "Loop  3863 :    Loss_Train:  [[ 12.72627509]]    Loss_Validation:  [[ 11.1676444]]\n",
      "Loop  3864 :    Loss_Train:  [[ 12.72627187]]    Loss_Validation:  [[ 11.16765783]]\n",
      "Loop  3865 :    Loss_Train:  [[ 12.72626866]]    Loss_Validation:  [[ 11.16767125]]\n",
      "Loop  3866 :    Loss_Train:  [[ 12.72626545]]    Loss_Validation:  [[ 11.16768465]]\n",
      "Loop  3867 :    Loss_Train:  [[ 12.72626225]]    Loss_Validation:  [[ 11.16769805]]\n",
      "Loop  3868 :    Loss_Train:  [[ 12.72625906]]    Loss_Validation:  [[ 11.16771145]]\n",
      "Loop  3869 :    Loss_Train:  [[ 12.72625586]]    Loss_Validation:  [[ 11.16772483]]\n",
      "Loop  3870 :    Loss_Train:  [[ 12.72625268]]    Loss_Validation:  [[ 11.1677382]]\n",
      "Loop  3871 :    Loss_Train:  [[ 12.7262495]]    Loss_Validation:  [[ 11.16775157]]\n",
      "Loop  3872 :    Loss_Train:  [[ 12.72624632]]    Loss_Validation:  [[ 11.16776493]]\n",
      "Loop  3873 :    Loss_Train:  [[ 12.72624315]]    Loss_Validation:  [[ 11.16777827]]\n",
      "Loop  3874 :    Loss_Train:  [[ 12.72623999]]    Loss_Validation:  [[ 11.16779161]]\n",
      "Loop  3875 :    Loss_Train:  [[ 12.72623683]]    Loss_Validation:  [[ 11.16780495]]\n",
      "Loop  3876 :    Loss_Train:  [[ 12.72623367]]    Loss_Validation:  [[ 11.16781827]]\n",
      "Loop  3877 :    Loss_Train:  [[ 12.72623052]]    Loss_Validation:  [[ 11.16783158]]\n",
      "Loop  3878 :    Loss_Train:  [[ 12.72622738]]    Loss_Validation:  [[ 11.16784489]]\n",
      "Loop  3879 :    Loss_Train:  [[ 12.72622424]]    Loss_Validation:  [[ 11.16785819]]\n",
      "Loop  3880 :    Loss_Train:  [[ 12.72622111]]    Loss_Validation:  [[ 11.16787148]]\n",
      "Loop  3881 :    Loss_Train:  [[ 12.72621798]]    Loss_Validation:  [[ 11.16788476]]\n",
      "Loop  3882 :    Loss_Train:  [[ 12.72621486]]    Loss_Validation:  [[ 11.16789803]]\n",
      "Loop  3883 :    Loss_Train:  [[ 12.72621174]]    Loss_Validation:  [[ 11.16791129]]\n",
      "Loop  3884 :    Loss_Train:  [[ 12.72620863]]    Loss_Validation:  [[ 11.16792455]]\n",
      "Loop  3885 :    Loss_Train:  [[ 12.72620552]]    Loss_Validation:  [[ 11.1679378]]\n",
      "Loop  3886 :    Loss_Train:  [[ 12.72620242]]    Loss_Validation:  [[ 11.16795103]]\n",
      "Loop  3887 :    Loss_Train:  [[ 12.72619932]]    Loss_Validation:  [[ 11.16796426]]\n",
      "Loop  3888 :    Loss_Train:  [[ 12.72619623]]    Loss_Validation:  [[ 11.16797748]]\n",
      "Loop  3889 :    Loss_Train:  [[ 12.72619314]]    Loss_Validation:  [[ 11.1679907]]\n",
      "Loop  3890 :    Loss_Train:  [[ 12.72619006]]    Loss_Validation:  [[ 11.1680039]]\n",
      "Loop  3891 :    Loss_Train:  [[ 12.72618698]]    Loss_Validation:  [[ 11.1680171]]\n",
      "Loop  3892 :    Loss_Train:  [[ 12.72618391]]    Loss_Validation:  [[ 11.16803028]]\n",
      "Loop  3893 :    Loss_Train:  [[ 12.72618084]]    Loss_Validation:  [[ 11.16804346]]\n",
      "Loop  3894 :    Loss_Train:  [[ 12.72617778]]    Loss_Validation:  [[ 11.16805663]]\n",
      "Loop  3895 :    Loss_Train:  [[ 12.72617473]]    Loss_Validation:  [[ 11.1680698]]\n",
      "Loop  3896 :    Loss_Train:  [[ 12.72617167]]    Loss_Validation:  [[ 11.16808295]]\n",
      "Loop  3897 :    Loss_Train:  [[ 12.72616863]]    Loss_Validation:  [[ 11.1680961]]\n",
      "Loop  3898 :    Loss_Train:  [[ 12.72616559]]    Loss_Validation:  [[ 11.16810923]]\n",
      "Loop  3899 :    Loss_Train:  [[ 12.72616255]]    Loss_Validation:  [[ 11.16812236]]\n",
      "Loop  3900 :    Loss_Train:  [[ 12.72615952]]    Loss_Validation:  [[ 11.16813548]]\n",
      "Loop  3901 :    Loss_Train:  [[ 12.72615649]]    Loss_Validation:  [[ 11.16814859]]\n",
      "Loop  3902 :    Loss_Train:  [[ 12.72615347]]    Loss_Validation:  [[ 11.1681617]]\n",
      "Loop  3903 :    Loss_Train:  [[ 12.72615046]]    Loss_Validation:  [[ 11.16817479]]\n",
      "Loop  3904 :    Loss_Train:  [[ 12.72614745]]    Loss_Validation:  [[ 11.16818788]]\n",
      "Loop  3905 :    Loss_Train:  [[ 12.72614444]]    Loss_Validation:  [[ 11.16820096]]\n",
      "Loop  3906 :    Loss_Train:  [[ 12.72614144]]    Loss_Validation:  [[ 11.16821403]]\n",
      "Loop  3907 :    Loss_Train:  [[ 12.72613844]]    Loss_Validation:  [[ 11.16822709]]\n",
      "Loop  3908 :    Loss_Train:  [[ 12.72613545]]    Loss_Validation:  [[ 11.16824014]]\n",
      "Loop  3909 :    Loss_Train:  [[ 12.72613247]]    Loss_Validation:  [[ 11.16825318]]\n",
      "Loop  3910 :    Loss_Train:  [[ 12.72612949]]    Loss_Validation:  [[ 11.16826622]]\n",
      "Loop  3911 :    Loss_Train:  [[ 12.72612651]]    Loss_Validation:  [[ 11.16827925]]\n",
      "Loop  3912 :    Loss_Train:  [[ 12.72612354]]    Loss_Validation:  [[ 11.16829227]]\n",
      "Loop  3913 :    Loss_Train:  [[ 12.72612057]]    Loss_Validation:  [[ 11.16830528]]\n",
      "Loop  3914 :    Loss_Train:  [[ 12.72611761]]    Loss_Validation:  [[ 11.16831828]]\n",
      "Loop  3915 :    Loss_Train:  [[ 12.72611465]]    Loss_Validation:  [[ 11.16833128]]\n",
      "Loop  3916 :    Loss_Train:  [[ 12.7261117]]    Loss_Validation:  [[ 11.16834426]]\n",
      "Loop  3917 :    Loss_Train:  [[ 12.72610876]]    Loss_Validation:  [[ 11.16835724]]\n",
      "Loop  3918 :    Loss_Train:  [[ 12.72610581]]    Loss_Validation:  [[ 11.16837021]]\n",
      "Loop  3919 :    Loss_Train:  [[ 12.72610288]]    Loss_Validation:  [[ 11.16838317]]\n",
      "Loop  3920 :    Loss_Train:  [[ 12.72609995]]    Loss_Validation:  [[ 11.16839613]]\n",
      "Loop  3921 :    Loss_Train:  [[ 12.72609702]]    Loss_Validation:  [[ 11.16840907]]\n",
      "Loop  3922 :    Loss_Train:  [[ 12.7260941]]    Loss_Validation:  [[ 11.16842201]]\n",
      "Loop  3923 :    Loss_Train:  [[ 12.72609118]]    Loss_Validation:  [[ 11.16843493]]\n",
      "Loop  3924 :    Loss_Train:  [[ 12.72608827]]    Loss_Validation:  [[ 11.16844785]]\n",
      "Loop  3925 :    Loss_Train:  [[ 12.72608536]]    Loss_Validation:  [[ 11.16846077]]\n",
      "Loop  3926 :    Loss_Train:  [[ 12.72608246]]    Loss_Validation:  [[ 11.16847367]]\n",
      "Loop  3927 :    Loss_Train:  [[ 12.72607956]]    Loss_Validation:  [[ 11.16848656]]\n",
      "Loop  3928 :    Loss_Train:  [[ 12.72607666]]    Loss_Validation:  [[ 11.16849945]]\n",
      "Loop  3929 :    Loss_Train:  [[ 12.72607378]]    Loss_Validation:  [[ 11.16851233]]\n",
      "Loop  3930 :    Loss_Train:  [[ 12.72607089]]    Loss_Validation:  [[ 11.1685252]]\n",
      "Loop  3931 :    Loss_Train:  [[ 12.72606801]]    Loss_Validation:  [[ 11.16853806]]\n",
      "Loop  3932 :    Loss_Train:  [[ 12.72606514]]    Loss_Validation:  [[ 11.16855091]]\n",
      "Loop  3933 :    Loss_Train:  [[ 12.72606227]]    Loss_Validation:  [[ 11.16856376]]\n",
      "Loop  3934 :    Loss_Train:  [[ 12.7260594]]    Loss_Validation:  [[ 11.1685766]]\n",
      "Loop  3935 :    Loss_Train:  [[ 12.72605655]]    Loss_Validation:  [[ 11.16858942]]\n",
      "Loop  3936 :    Loss_Train:  [[ 12.72605369]]    Loss_Validation:  [[ 11.16860225]]\n",
      "Loop  3937 :    Loss_Train:  [[ 12.72605084]]    Loss_Validation:  [[ 11.16861506]]\n",
      "Loop  3938 :    Loss_Train:  [[ 12.72604799]]    Loss_Validation:  [[ 11.16862786]]\n",
      "Loop  3939 :    Loss_Train:  [[ 12.72604515]]    Loss_Validation:  [[ 11.16864066]]\n",
      "Loop  3940 :    Loss_Train:  [[ 12.72604232]]    Loss_Validation:  [[ 11.16865345]]\n",
      "Loop  3941 :    Loss_Train:  [[ 12.72603949]]    Loss_Validation:  [[ 11.16866623]]\n",
      "Loop  3942 :    Loss_Train:  [[ 12.72603666]]    Loss_Validation:  [[ 11.168679]]\n",
      "Loop  3943 :    Loss_Train:  [[ 12.72603384]]    Loss_Validation:  [[ 11.16869176]]\n",
      "Loop  3944 :    Loss_Train:  [[ 12.72603102]]    Loss_Validation:  [[ 11.16870451]]\n",
      "Loop  3945 :    Loss_Train:  [[ 12.72602821]]    Loss_Validation:  [[ 11.16871726]]\n",
      "Loop  3946 :    Loss_Train:  [[ 12.7260254]]    Loss_Validation:  [[ 11.16873]]\n",
      "Loop  3947 :    Loss_Train:  [[ 12.72602259]]    Loss_Validation:  [[ 11.16874273]]\n",
      "Loop  3948 :    Loss_Train:  [[ 12.7260198]]    Loss_Validation:  [[ 11.16875545]]\n",
      "Loop  3949 :    Loss_Train:  [[ 12.726017]]    Loss_Validation:  [[ 11.16876817]]\n",
      "Loop  3950 :    Loss_Train:  [[ 12.72601421]]    Loss_Validation:  [[ 11.16878087]]\n",
      "Loop  3951 :    Loss_Train:  [[ 12.72601143]]    Loss_Validation:  [[ 11.16879357]]\n",
      "Loop  3952 :    Loss_Train:  [[ 12.72600865]]    Loss_Validation:  [[ 11.16880626]]\n",
      "Loop  3953 :    Loss_Train:  [[ 12.72600587]]    Loss_Validation:  [[ 11.16881894]]\n",
      "Loop  3954 :    Loss_Train:  [[ 12.7260031]]    Loss_Validation:  [[ 11.16883161]]\n",
      "Loop  3955 :    Loss_Train:  [[ 12.72600033]]    Loss_Validation:  [[ 11.16884428]]\n",
      "Loop  3956 :    Loss_Train:  [[ 12.72599757]]    Loss_Validation:  [[ 11.16885693]]\n",
      "Loop  3957 :    Loss_Train:  [[ 12.72599481]]    Loss_Validation:  [[ 11.16886958]]\n",
      "Loop  3958 :    Loss_Train:  [[ 12.72599206]]    Loss_Validation:  [[ 11.16888222]]\n",
      "Loop  3959 :    Loss_Train:  [[ 12.72598931]]    Loss_Validation:  [[ 11.16889486]]\n",
      "Loop  3960 :    Loss_Train:  [[ 12.72598657]]    Loss_Validation:  [[ 11.16890748]]\n",
      "Loop  3961 :    Loss_Train:  [[ 12.72598383]]    Loss_Validation:  [[ 11.1689201]]\n",
      "Loop  3962 :    Loss_Train:  [[ 12.7259811]]    Loss_Validation:  [[ 11.1689327]]\n",
      "Loop  3963 :    Loss_Train:  [[ 12.72597837]]    Loss_Validation:  [[ 11.1689453]]\n",
      "Loop  3964 :    Loss_Train:  [[ 12.72597564]]    Loss_Validation:  [[ 11.1689579]]\n",
      "Loop  3965 :    Loss_Train:  [[ 12.72597292]]    Loss_Validation:  [[ 11.16897048]]\n",
      "Loop  3966 :    Loss_Train:  [[ 12.7259702]]    Loss_Validation:  [[ 11.16898306]]\n",
      "Loop  3967 :    Loss_Train:  [[ 12.72596749]]    Loss_Validation:  [[ 11.16899562]]\n",
      "Loop  3968 :    Loss_Train:  [[ 12.72596478]]    Loss_Validation:  [[ 11.16900818]]\n",
      "Loop  3969 :    Loss_Train:  [[ 12.72596208]]    Loss_Validation:  [[ 11.16902073]]\n",
      "Loop  3970 :    Loss_Train:  [[ 12.72595938]]    Loss_Validation:  [[ 11.16903328]]\n",
      "Loop  3971 :    Loss_Train:  [[ 12.72595669]]    Loss_Validation:  [[ 11.16904581]]\n",
      "Loop  3972 :    Loss_Train:  [[ 12.725954]]    Loss_Validation:  [[ 11.16905834]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  3973 :    Loss_Train:  [[ 12.72595131]]    Loss_Validation:  [[ 11.16907086]]\n",
      "Loop  3974 :    Loss_Train:  [[ 12.72594863]]    Loss_Validation:  [[ 11.16908337]]\n",
      "Loop  3975 :    Loss_Train:  [[ 12.72594596]]    Loss_Validation:  [[ 11.16909587]]\n",
      "Loop  3976 :    Loss_Train:  [[ 12.72594328]]    Loss_Validation:  [[ 11.16910837]]\n",
      "Loop  3977 :    Loss_Train:  [[ 12.72594062]]    Loss_Validation:  [[ 11.16912085]]\n",
      "Loop  3978 :    Loss_Train:  [[ 12.72593795]]    Loss_Validation:  [[ 11.16913333]]\n",
      "Loop  3979 :    Loss_Train:  [[ 12.7259353]]    Loss_Validation:  [[ 11.1691458]]\n",
      "Loop  3980 :    Loss_Train:  [[ 12.72593264]]    Loss_Validation:  [[ 11.16915827]]\n",
      "Loop  3981 :    Loss_Train:  [[ 12.72592999]]    Loss_Validation:  [[ 11.16917072]]\n",
      "Loop  3982 :    Loss_Train:  [[ 12.72592735]]    Loss_Validation:  [[ 11.16918317]]\n",
      "Loop  3983 :    Loss_Train:  [[ 12.72592471]]    Loss_Validation:  [[ 11.16919561]]\n",
      "Loop  3984 :    Loss_Train:  [[ 12.72592207]]    Loss_Validation:  [[ 11.16920804]]\n",
      "Loop  3985 :    Loss_Train:  [[ 12.72591944]]    Loss_Validation:  [[ 11.16922046]]\n",
      "Loop  3986 :    Loss_Train:  [[ 12.72591681]]    Loss_Validation:  [[ 11.16923287]]\n",
      "Loop  3987 :    Loss_Train:  [[ 12.72591419]]    Loss_Validation:  [[ 11.16924528]]\n",
      "Loop  3988 :    Loss_Train:  [[ 12.72591157]]    Loss_Validation:  [[ 11.16925768]]\n",
      "Loop  3989 :    Loss_Train:  [[ 12.72590895]]    Loss_Validation:  [[ 11.16927007]]\n",
      "Loop  3990 :    Loss_Train:  [[ 12.72590634]]    Loss_Validation:  [[ 11.16928245]]\n",
      "Loop  3991 :    Loss_Train:  [[ 12.72590373]]    Loss_Validation:  [[ 11.16929483]]\n",
      "Loop  3992 :    Loss_Train:  [[ 12.72590113]]    Loss_Validation:  [[ 11.16930719]]\n",
      "Loop  3993 :    Loss_Train:  [[ 12.72589854]]    Loss_Validation:  [[ 11.16931955]]\n",
      "Loop  3994 :    Loss_Train:  [[ 12.72589594]]    Loss_Validation:  [[ 11.1693319]]\n",
      "Loop  3995 :    Loss_Train:  [[ 12.72589335]]    Loss_Validation:  [[ 11.16934425]]\n",
      "Loop  3996 :    Loss_Train:  [[ 12.72589077]]    Loss_Validation:  [[ 11.16935658]]\n",
      "Loop  3997 :    Loss_Train:  [[ 12.72588819]]    Loss_Validation:  [[ 11.16936891]]\n",
      "Loop  3998 :    Loss_Train:  [[ 12.72588561]]    Loss_Validation:  [[ 11.16938123]]\n",
      "Loop  3999 :    Loss_Train:  [[ 12.72588304]]    Loss_Validation:  [[ 11.16939354]]\n",
      "Loop  4000 :    Loss_Train:  [[ 12.72588047]]    Loss_Validation:  [[ 11.16940584]]\n",
      "Loop  4001 :    Loss_Train:  [[ 12.72587791]]    Loss_Validation:  [[ 11.16941814]]\n",
      "Loop  4002 :    Loss_Train:  [[ 12.72587535]]    Loss_Validation:  [[ 11.16943042]]\n",
      "Loop  4003 :    Loss_Train:  [[ 12.7258728]]    Loss_Validation:  [[ 11.1694427]]\n",
      "Loop  4004 :    Loss_Train:  [[ 12.72587024]]    Loss_Validation:  [[ 11.16945497]]\n",
      "Loop  4005 :    Loss_Train:  [[ 12.7258677]]    Loss_Validation:  [[ 11.16946724]]\n",
      "Loop  4006 :    Loss_Train:  [[ 12.72586516]]    Loss_Validation:  [[ 11.16947949]]\n",
      "Loop  4007 :    Loss_Train:  [[ 12.72586262]]    Loss_Validation:  [[ 11.16949174]]\n",
      "Loop  4008 :    Loss_Train:  [[ 12.72586008]]    Loss_Validation:  [[ 11.16950398]]\n",
      "Loop  4009 :    Loss_Train:  [[ 12.72585755]]    Loss_Validation:  [[ 11.16951621]]\n",
      "Loop  4010 :    Loss_Train:  [[ 12.72585503]]    Loss_Validation:  [[ 11.16952843]]\n",
      "Loop  4011 :    Loss_Train:  [[ 12.72585251]]    Loss_Validation:  [[ 11.16954065]]\n",
      "Loop  4012 :    Loss_Train:  [[ 12.72584999]]    Loss_Validation:  [[ 11.16955286]]\n",
      "Loop  4013 :    Loss_Train:  [[ 12.72584748]]    Loss_Validation:  [[ 11.16956506]]\n",
      "Loop  4014 :    Loss_Train:  [[ 12.72584497]]    Loss_Validation:  [[ 11.16957725]]\n",
      "Loop  4015 :    Loss_Train:  [[ 12.72584246]]    Loss_Validation:  [[ 11.16958944]]\n",
      "Loop  4016 :    Loss_Train:  [[ 12.72583996]]    Loss_Validation:  [[ 11.16960161]]\n",
      "Loop  4017 :    Loss_Train:  [[ 12.72583747]]    Loss_Validation:  [[ 11.16961378]]\n",
      "Loop  4018 :    Loss_Train:  [[ 12.72583498]]    Loss_Validation:  [[ 11.16962594]]\n",
      "Loop  4019 :    Loss_Train:  [[ 12.72583249]]    Loss_Validation:  [[ 11.16963809]]\n",
      "Loop  4020 :    Loss_Train:  [[ 12.72583]]    Loss_Validation:  [[ 11.16965024]]\n",
      "Loop  4021 :    Loss_Train:  [[ 12.72582752]]    Loss_Validation:  [[ 11.16966238]]\n",
      "Loop  4022 :    Loss_Train:  [[ 12.72582505]]    Loss_Validation:  [[ 11.16967451]]\n",
      "Loop  4023 :    Loss_Train:  [[ 12.72582258]]    Loss_Validation:  [[ 11.16968663]]\n",
      "Loop  4024 :    Loss_Train:  [[ 12.72582011]]    Loss_Validation:  [[ 11.16969874]]\n",
      "Loop  4025 :    Loss_Train:  [[ 12.72581765]]    Loss_Validation:  [[ 11.16971085]]\n",
      "Loop  4026 :    Loss_Train:  [[ 12.72581519]]    Loss_Validation:  [[ 11.16972295]]\n",
      "Loop  4027 :    Loss_Train:  [[ 12.72581273]]    Loss_Validation:  [[ 11.16973504]]\n",
      "Loop  4028 :    Loss_Train:  [[ 12.72581028]]    Loss_Validation:  [[ 11.16974712]]\n",
      "Loop  4029 :    Loss_Train:  [[ 12.72580783]]    Loss_Validation:  [[ 11.16975919]]\n",
      "Loop  4030 :    Loss_Train:  [[ 12.72580539]]    Loss_Validation:  [[ 11.16977126]]\n",
      "Loop  4031 :    Loss_Train:  [[ 12.72580295]]    Loss_Validation:  [[ 11.16978332]]\n",
      "Loop  4032 :    Loss_Train:  [[ 12.72580052]]    Loss_Validation:  [[ 11.16979537]]\n",
      "Loop  4033 :    Loss_Train:  [[ 12.72579808]]    Loss_Validation:  [[ 11.16980741]]\n",
      "Loop  4034 :    Loss_Train:  [[ 12.72579566]]    Loss_Validation:  [[ 11.16981945]]\n",
      "Loop  4035 :    Loss_Train:  [[ 12.72579324]]    Loss_Validation:  [[ 11.16983148]]\n",
      "Loop  4036 :    Loss_Train:  [[ 12.72579082]]    Loss_Validation:  [[ 11.1698435]]\n",
      "Loop  4037 :    Loss_Train:  [[ 12.7257884]]    Loss_Validation:  [[ 11.16985551]]\n",
      "Loop  4038 :    Loss_Train:  [[ 12.72578599]]    Loss_Validation:  [[ 11.16986751]]\n",
      "Loop  4039 :    Loss_Train:  [[ 12.72578358]]    Loss_Validation:  [[ 11.16987951]]\n",
      "Loop  4040 :    Loss_Train:  [[ 12.72578118]]    Loss_Validation:  [[ 11.1698915]]\n",
      "Loop  4041 :    Loss_Train:  [[ 12.72577878]]    Loss_Validation:  [[ 11.16990348]]\n",
      "Loop  4042 :    Loss_Train:  [[ 12.72577639]]    Loss_Validation:  [[ 11.16991545]]\n",
      "Loop  4043 :    Loss_Train:  [[ 12.725774]]    Loss_Validation:  [[ 11.16992742]]\n",
      "Loop  4044 :    Loss_Train:  [[ 12.72577161]]    Loss_Validation:  [[ 11.16993938]]\n",
      "Loop  4045 :    Loss_Train:  [[ 12.72576922]]    Loss_Validation:  [[ 11.16995133]]\n",
      "Loop  4046 :    Loss_Train:  [[ 12.72576685]]    Loss_Validation:  [[ 11.16996327]]\n",
      "Loop  4047 :    Loss_Train:  [[ 12.72576447]]    Loss_Validation:  [[ 11.16997521]]\n",
      "Loop  4048 :    Loss_Train:  [[ 12.7257621]]    Loss_Validation:  [[ 11.16998713]]\n",
      "Loop  4049 :    Loss_Train:  [[ 12.72575973]]    Loss_Validation:  [[ 11.16999905]]\n",
      "Loop  4050 :    Loss_Train:  [[ 12.72575737]]    Loss_Validation:  [[ 11.17001096]]\n",
      "Loop  4051 :    Loss_Train:  [[ 12.72575501]]    Loss_Validation:  [[ 11.17002287]]\n",
      "Loop  4052 :    Loss_Train:  [[ 12.72575265]]    Loss_Validation:  [[ 11.17003476]]\n",
      "Loop  4053 :    Loss_Train:  [[ 12.7257503]]    Loss_Validation:  [[ 11.17004665]]\n",
      "Loop  4054 :    Loss_Train:  [[ 12.72574795]]    Loss_Validation:  [[ 11.17005853]]\n",
      "Loop  4055 :    Loss_Train:  [[ 12.72574561]]    Loss_Validation:  [[ 11.17007041]]\n",
      "Loop  4056 :    Loss_Train:  [[ 12.72574327]]    Loss_Validation:  [[ 11.17008227]]\n",
      "Loop  4057 :    Loss_Train:  [[ 12.72574093]]    Loss_Validation:  [[ 11.17009413]]\n",
      "Loop  4058 :    Loss_Train:  [[ 12.7257386]]    Loss_Validation:  [[ 11.17010598]]\n",
      "Loop  4059 :    Loss_Train:  [[ 12.72573627]]    Loss_Validation:  [[ 11.17011782]]\n",
      "Loop  4060 :    Loss_Train:  [[ 12.72573395]]    Loss_Validation:  [[ 11.17012966]]\n",
      "Loop  4061 :    Loss_Train:  [[ 12.72573163]]    Loss_Validation:  [[ 11.17014148]]\n",
      "Loop  4062 :    Loss_Train:  [[ 12.72572931]]    Loss_Validation:  [[ 11.1701533]]\n",
      "Loop  4063 :    Loss_Train:  [[ 12.725727]]    Loss_Validation:  [[ 11.17016511]]\n",
      "Loop  4064 :    Loss_Train:  [[ 12.72572469]]    Loss_Validation:  [[ 11.17017692]]\n",
      "Loop  4065 :    Loss_Train:  [[ 12.72572238]]    Loss_Validation:  [[ 11.17018871]]\n",
      "Loop  4066 :    Loss_Train:  [[ 12.72572008]]    Loss_Validation:  [[ 11.1702005]]\n",
      "Loop  4067 :    Loss_Train:  [[ 12.72571778]]    Loss_Validation:  [[ 11.17021228]]\n",
      "Loop  4068 :    Loss_Train:  [[ 12.72571549]]    Loss_Validation:  [[ 11.17022406]]\n",
      "Loop  4069 :    Loss_Train:  [[ 12.7257132]]    Loss_Validation:  [[ 11.17023582]]\n",
      "Loop  4070 :    Loss_Train:  [[ 12.72571091]]    Loss_Validation:  [[ 11.17024758]]\n",
      "Loop  4071 :    Loss_Train:  [[ 12.72570863]]    Loss_Validation:  [[ 11.17025933]]\n",
      "Loop  4072 :    Loss_Train:  [[ 12.72570635]]    Loss_Validation:  [[ 11.17027108]]\n",
      "Loop  4073 :    Loss_Train:  [[ 12.72570407]]    Loss_Validation:  [[ 11.17028281]]\n",
      "Loop  4074 :    Loss_Train:  [[ 12.7257018]]    Loss_Validation:  [[ 11.17029454]]\n",
      "Loop  4075 :    Loss_Train:  [[ 12.72569953]]    Loss_Validation:  [[ 11.17030626]]\n",
      "Loop  4076 :    Loss_Train:  [[ 12.72569727]]    Loss_Validation:  [[ 11.17031797]]\n",
      "Loop  4077 :    Loss_Train:  [[ 12.72569501]]    Loss_Validation:  [[ 11.17032968]]\n",
      "Loop  4078 :    Loss_Train:  [[ 12.72569275]]    Loss_Validation:  [[ 11.17034137]]\n",
      "Loop  4079 :    Loss_Train:  [[ 12.7256905]]    Loss_Validation:  [[ 11.17035306]]\n",
      "Loop  4080 :    Loss_Train:  [[ 12.72568825]]    Loss_Validation:  [[ 11.17036475]]\n",
      "Loop  4081 :    Loss_Train:  [[ 12.72568601]]    Loss_Validation:  [[ 11.17037642]]\n",
      "Loop  4082 :    Loss_Train:  [[ 12.72568376]]    Loss_Validation:  [[ 11.17038809]]\n",
      "Loop  4083 :    Loss_Train:  [[ 12.72568153]]    Loss_Validation:  [[ 11.17039975]]\n",
      "Loop  4084 :    Loss_Train:  [[ 12.72567929]]    Loss_Validation:  [[ 11.1704114]]\n",
      "Loop  4085 :    Loss_Train:  [[ 12.72567706]]    Loss_Validation:  [[ 11.17042304]]\n",
      "Loop  4086 :    Loss_Train:  [[ 12.72567483]]    Loss_Validation:  [[ 11.17043468]]\n",
      "Loop  4087 :    Loss_Train:  [[ 12.72567261]]    Loss_Validation:  [[ 11.17044631]]\n",
      "Loop  4088 :    Loss_Train:  [[ 12.72567039]]    Loss_Validation:  [[ 11.17045793]]\n",
      "Loop  4089 :    Loss_Train:  [[ 12.72566818]]    Loss_Validation:  [[ 11.17046954]]\n",
      "Loop  4090 :    Loss_Train:  [[ 12.72566596]]    Loss_Validation:  [[ 11.17048115]]\n",
      "Loop  4091 :    Loss_Train:  [[ 12.72566376]]    Loss_Validation:  [[ 11.17049275]]\n",
      "Loop  4092 :    Loss_Train:  [[ 12.72566155]]    Loss_Validation:  [[ 11.17050434]]\n",
      "Loop  4093 :    Loss_Train:  [[ 12.72565935]]    Loss_Validation:  [[ 11.17051593]]\n",
      "Loop  4094 :    Loss_Train:  [[ 12.72565715]]    Loss_Validation:  [[ 11.1705275]]\n",
      "Loop  4095 :    Loss_Train:  [[ 12.72565496]]    Loss_Validation:  [[ 11.17053907]]\n",
      "Loop  4096 :    Loss_Train:  [[ 12.72565277]]    Loss_Validation:  [[ 11.17055063]]\n",
      "Loop  4097 :    Loss_Train:  [[ 12.72565058]]    Loss_Validation:  [[ 11.17056219]]\n",
      "Loop  4098 :    Loss_Train:  [[ 12.7256484]]    Loss_Validation:  [[ 11.17057373]]\n",
      "Loop  4099 :    Loss_Train:  [[ 12.72564622]]    Loss_Validation:  [[ 11.17058527]]\n",
      "Loop  4100 :    Loss_Train:  [[ 12.72564404]]    Loss_Validation:  [[ 11.1705968]]\n",
      "Loop  4101 :    Loss_Train:  [[ 12.72564187]]    Loss_Validation:  [[ 11.17060833]]\n",
      "Loop  4102 :    Loss_Train:  [[ 12.7256397]]    Loss_Validation:  [[ 11.17061984]]\n",
      "Loop  4103 :    Loss_Train:  [[ 12.72563754]]    Loss_Validation:  [[ 11.17063135]]\n",
      "Loop  4104 :    Loss_Train:  [[ 12.72563537]]    Loss_Validation:  [[ 11.17064285]]\n",
      "Loop  4105 :    Loss_Train:  [[ 12.72563322]]    Loss_Validation:  [[ 11.17065435]]\n",
      "Loop  4106 :    Loss_Train:  [[ 12.72563106]]    Loss_Validation:  [[ 11.17066583]]\n",
      "Loop  4107 :    Loss_Train:  [[ 12.72562891]]    Loss_Validation:  [[ 11.17067731]]\n",
      "Loop  4108 :    Loss_Train:  [[ 12.72562676]]    Loss_Validation:  [[ 11.17068878]]\n",
      "Loop  4109 :    Loss_Train:  [[ 12.72562462]]    Loss_Validation:  [[ 11.17070025]]\n",
      "Loop  4110 :    Loss_Train:  [[ 12.72562248]]    Loss_Validation:  [[ 11.17071171]]\n",
      "Loop  4111 :    Loss_Train:  [[ 12.72562034]]    Loss_Validation:  [[ 11.17072315]]\n",
      "Loop  4112 :    Loss_Train:  [[ 12.72561821]]    Loss_Validation:  [[ 11.1707346]]\n",
      "Loop  4113 :    Loss_Train:  [[ 12.72561608]]    Loss_Validation:  [[ 11.17074603]]\n",
      "Loop  4114 :    Loss_Train:  [[ 12.72561395]]    Loss_Validation:  [[ 11.17075746]]\n",
      "Loop  4115 :    Loss_Train:  [[ 12.72561183]]    Loss_Validation:  [[ 11.17076888]]\n",
      "Loop  4116 :    Loss_Train:  [[ 12.72560971]]    Loss_Validation:  [[ 11.17078029]]\n",
      "Loop  4117 :    Loss_Train:  [[ 12.7256076]]    Loss_Validation:  [[ 11.17079169]]\n",
      "Loop  4118 :    Loss_Train:  [[ 12.72560549]]    Loss_Validation:  [[ 11.17080309]]\n",
      "Loop  4119 :    Loss_Train:  [[ 12.72560338]]    Loss_Validation:  [[ 11.17081448]]\n",
      "Loop  4120 :    Loss_Train:  [[ 12.72560127]]    Loss_Validation:  [[ 11.17082586]]\n",
      "Loop  4121 :    Loss_Train:  [[ 12.72559917]]    Loss_Validation:  [[ 11.17083724]]\n",
      "Loop  4122 :    Loss_Train:  [[ 12.72559707]]    Loss_Validation:  [[ 11.17084861]]\n",
      "Loop  4123 :    Loss_Train:  [[ 12.72559498]]    Loss_Validation:  [[ 11.17085997]]\n",
      "Loop  4124 :    Loss_Train:  [[ 12.72559289]]    Loss_Validation:  [[ 11.17087132]]\n",
      "Loop  4125 :    Loss_Train:  [[ 12.7255908]]    Loss_Validation:  [[ 11.17088266]]\n",
      "Loop  4126 :    Loss_Train:  [[ 12.72558871]]    Loss_Validation:  [[ 11.170894]]\n",
      "Loop  4127 :    Loss_Train:  [[ 12.72558663]]    Loss_Validation:  [[ 11.17090533]]\n",
      "Loop  4128 :    Loss_Train:  [[ 12.72558455]]    Loss_Validation:  [[ 11.17091666]]\n",
      "Loop  4129 :    Loss_Train:  [[ 12.72558248]]    Loss_Validation:  [[ 11.17092797]]\n",
      "Loop  4130 :    Loss_Train:  [[ 12.72558041]]    Loss_Validation:  [[ 11.17093928]]\n",
      "Loop  4131 :    Loss_Train:  [[ 12.72557834]]    Loss_Validation:  [[ 11.17095058]]\n",
      "Loop  4132 :    Loss_Train:  [[ 12.72557628]]    Loss_Validation:  [[ 11.17096188]]\n",
      "Loop  4133 :    Loss_Train:  [[ 12.72557422]]    Loss_Validation:  [[ 11.17097316]]\n",
      "Loop  4134 :    Loss_Train:  [[ 12.72557216]]    Loss_Validation:  [[ 11.17098444]]\n",
      "Loop  4135 :    Loss_Train:  [[ 12.72557011]]    Loss_Validation:  [[ 11.17099571]]\n",
      "Loop  4136 :    Loss_Train:  [[ 12.72556806]]    Loss_Validation:  [[ 11.17100698]]\n",
      "Loop  4137 :    Loss_Train:  [[ 12.72556601]]    Loss_Validation:  [[ 11.17101823]]\n",
      "Loop  4138 :    Loss_Train:  [[ 12.72556397]]    Loss_Validation:  [[ 11.17102948]]\n",
      "Loop  4139 :    Loss_Train:  [[ 12.72556193]]    Loss_Validation:  [[ 11.17104073]]\n",
      "Loop  4140 :    Loss_Train:  [[ 12.72555989]]    Loss_Validation:  [[ 11.17105196]]\n",
      "Loop  4141 :    Loss_Train:  [[ 12.72555786]]    Loss_Validation:  [[ 11.17106319]]\n",
      "Loop  4142 :    Loss_Train:  [[ 12.72555583]]    Loss_Validation:  [[ 11.17107441]]\n",
      "Loop  4143 :    Loss_Train:  [[ 12.7255538]]    Loss_Validation:  [[ 11.17108562]]\n",
      "Loop  4144 :    Loss_Train:  [[ 12.72555178]]    Loss_Validation:  [[ 11.17109683]]\n",
      "Loop  4145 :    Loss_Train:  [[ 12.72554976]]    Loss_Validation:  [[ 11.17110803]]\n",
      "Loop  4146 :    Loss_Train:  [[ 12.72554774]]    Loss_Validation:  [[ 11.17111922]]\n",
      "Loop  4147 :    Loss_Train:  [[ 12.72554573]]    Loss_Validation:  [[ 11.1711304]]\n",
      "Loop  4148 :    Loss_Train:  [[ 12.72554372]]    Loss_Validation:  [[ 11.17114158]]\n",
      "Loop  4149 :    Loss_Train:  [[ 12.72554171]]    Loss_Validation:  [[ 11.17115275]]\n",
      "Loop  4150 :    Loss_Train:  [[ 12.72553971]]    Loss_Validation:  [[ 11.17116391]]\n",
      "Loop  4151 :    Loss_Train:  [[ 12.72553771]]    Loss_Validation:  [[ 11.17117507]]\n",
      "Loop  4152 :    Loss_Train:  [[ 12.72553571]]    Loss_Validation:  [[ 11.17118621]]\n",
      "Loop  4153 :    Loss_Train:  [[ 12.72553372]]    Loss_Validation:  [[ 11.17119735]]\n",
      "Loop  4154 :    Loss_Train:  [[ 12.72553173]]    Loss_Validation:  [[ 11.17120849]]\n",
      "Loop  4155 :    Loss_Train:  [[ 12.72552974]]    Loss_Validation:  [[ 11.17121961]]\n",
      "Loop  4156 :    Loss_Train:  [[ 12.72552776]]    Loss_Validation:  [[ 11.17123073]]\n",
      "Loop  4157 :    Loss_Train:  [[ 12.72552578]]    Loss_Validation:  [[ 11.17124184]]\n",
      "Loop  4158 :    Loss_Train:  [[ 12.7255238]]    Loss_Validation:  [[ 11.17125295]]\n",
      "Loop  4159 :    Loss_Train:  [[ 12.72552183]]    Loss_Validation:  [[ 11.17126404]]\n",
      "Loop  4160 :    Loss_Train:  [[ 12.72551986]]    Loss_Validation:  [[ 11.17127513]]\n",
      "Loop  4161 :    Loss_Train:  [[ 12.72551789]]    Loss_Validation:  [[ 11.17128622]]\n",
      "Loop  4162 :    Loss_Train:  [[ 12.72551592]]    Loss_Validation:  [[ 11.17129729]]\n",
      "Loop  4163 :    Loss_Train:  [[ 12.72551396]]    Loss_Validation:  [[ 11.17130836]]\n",
      "Loop  4164 :    Loss_Train:  [[ 12.72551201]]    Loss_Validation:  [[ 11.17131942]]\n",
      "Loop  4165 :    Loss_Train:  [[ 12.72551005]]    Loss_Validation:  [[ 11.17133047]]\n",
      "Loop  4166 :    Loss_Train:  [[ 12.7255081]]    Loss_Validation:  [[ 11.17134152]]\n",
      "Loop  4167 :    Loss_Train:  [[ 12.72550615]]    Loss_Validation:  [[ 11.17135256]]\n",
      "Loop  4168 :    Loss_Train:  [[ 12.72550421]]    Loss_Validation:  [[ 11.17136359]]\n",
      "Loop  4169 :    Loss_Train:  [[ 12.72550227]]    Loss_Validation:  [[ 11.17137461]]\n",
      "Loop  4170 :    Loss_Train:  [[ 12.72550033]]    Loss_Validation:  [[ 11.17138563]]\n",
      "Loop  4171 :    Loss_Train:  [[ 12.72549839]]    Loss_Validation:  [[ 11.17139664]]\n",
      "Loop  4172 :    Loss_Train:  [[ 12.72549646]]    Loss_Validation:  [[ 11.17140765]]\n",
      "Loop  4173 :    Loss_Train:  [[ 12.72549453]]    Loss_Validation:  [[ 11.17141864]]\n",
      "Loop  4174 :    Loss_Train:  [[ 12.72549261]]    Loss_Validation:  [[ 11.17142963]]\n",
      "Loop  4175 :    Loss_Train:  [[ 12.72549068]]    Loss_Validation:  [[ 11.17144061]]\n",
      "Loop  4176 :    Loss_Train:  [[ 12.72548876]]    Loss_Validation:  [[ 11.17145159]]\n",
      "Loop  4177 :    Loss_Train:  [[ 12.72548685]]    Loss_Validation:  [[ 11.17146255]]\n",
      "Loop  4178 :    Loss_Train:  [[ 12.72548494]]    Loss_Validation:  [[ 11.17147351]]\n",
      "Loop  4179 :    Loss_Train:  [[ 12.72548303]]    Loss_Validation:  [[ 11.17148447]]\n",
      "Loop  4180 :    Loss_Train:  [[ 12.72548112]]    Loss_Validation:  [[ 11.17149541]]\n",
      "Loop  4181 :    Loss_Train:  [[ 12.72547922]]    Loss_Validation:  [[ 11.17150635]]\n",
      "Loop  4182 :    Loss_Train:  [[ 12.72547732]]    Loss_Validation:  [[ 11.17151728]]\n",
      "Loop  4183 :    Loss_Train:  [[ 12.72547542]]    Loss_Validation:  [[ 11.17152821]]\n",
      "Loop  4184 :    Loss_Train:  [[ 12.72547352]]    Loss_Validation:  [[ 11.17153912]]\n",
      "Loop  4185 :    Loss_Train:  [[ 12.72547163]]    Loss_Validation:  [[ 11.17155003]]\n",
      "Loop  4186 :    Loss_Train:  [[ 12.72546975]]    Loss_Validation:  [[ 11.17156094]]\n",
      "Loop  4187 :    Loss_Train:  [[ 12.72546786]]    Loss_Validation:  [[ 11.17157183]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  4188 :    Loss_Train:  [[ 12.72546598]]    Loss_Validation:  [[ 11.17158272]]\n",
      "Loop  4189 :    Loss_Train:  [[ 12.7254641]]    Loss_Validation:  [[ 11.1715936]]\n",
      "Loop  4190 :    Loss_Train:  [[ 12.72546223]]    Loss_Validation:  [[ 11.17160448]]\n",
      "Loop  4191 :    Loss_Train:  [[ 12.72546035]]    Loss_Validation:  [[ 11.17161534]]\n",
      "Loop  4192 :    Loss_Train:  [[ 12.72545848]]    Loss_Validation:  [[ 11.1716262]]\n",
      "Loop  4193 :    Loss_Train:  [[ 12.72545662]]    Loss_Validation:  [[ 11.17163706]]\n",
      "Loop  4194 :    Loss_Train:  [[ 12.72545476]]    Loss_Validation:  [[ 11.1716479]]\n",
      "Loop  4195 :    Loss_Train:  [[ 12.7254529]]    Loss_Validation:  [[ 11.17165874]]\n",
      "Loop  4196 :    Loss_Train:  [[ 12.72545104]]    Loss_Validation:  [[ 11.17166957]]\n",
      "Loop  4197 :    Loss_Train:  [[ 12.72544918]]    Loss_Validation:  [[ 11.1716804]]\n",
      "Loop  4198 :    Loss_Train:  [[ 12.72544733]]    Loss_Validation:  [[ 11.17169122]]\n",
      "Loop  4199 :    Loss_Train:  [[ 12.72544549]]    Loss_Validation:  [[ 11.17170203]]\n",
      "Loop  4200 :    Loss_Train:  [[ 12.72544364]]    Loss_Validation:  [[ 11.17171283]]\n",
      "Loop  4201 :    Loss_Train:  [[ 12.7254418]]    Loss_Validation:  [[ 11.17172363]]\n",
      "Loop  4202 :    Loss_Train:  [[ 12.72543996]]    Loss_Validation:  [[ 11.17173442]]\n",
      "Loop  4203 :    Loss_Train:  [[ 12.72543813]]    Loss_Validation:  [[ 11.1717452]]\n",
      "Loop  4204 :    Loss_Train:  [[ 12.72543629]]    Loss_Validation:  [[ 11.17175597]]\n",
      "Loop  4205 :    Loss_Train:  [[ 12.72543446]]    Loss_Validation:  [[ 11.17176674]]\n",
      "Loop  4206 :    Loss_Train:  [[ 12.72543264]]    Loss_Validation:  [[ 11.1717775]]\n",
      "Loop  4207 :    Loss_Train:  [[ 12.72543081]]    Loss_Validation:  [[ 11.17178826]]\n",
      "Loop  4208 :    Loss_Train:  [[ 12.72542899]]    Loss_Validation:  [[ 11.17179901]]\n",
      "Loop  4209 :    Loss_Train:  [[ 12.72542718]]    Loss_Validation:  [[ 11.17180975]]\n",
      "Loop  4210 :    Loss_Train:  [[ 12.72542536]]    Loss_Validation:  [[ 11.17182048]]\n",
      "Loop  4211 :    Loss_Train:  [[ 12.72542355]]    Loss_Validation:  [[ 11.17183121]]\n",
      "Loop  4212 :    Loss_Train:  [[ 12.72542174]]    Loss_Validation:  [[ 11.17184192]]\n",
      "Loop  4213 :    Loss_Train:  [[ 12.72541994]]    Loss_Validation:  [[ 11.17185264]]\n",
      "Loop  4214 :    Loss_Train:  [[ 12.72541813]]    Loss_Validation:  [[ 11.17186334]]\n",
      "Loop  4215 :    Loss_Train:  [[ 12.72541633]]    Loss_Validation:  [[ 11.17187404]]\n",
      "Loop  4216 :    Loss_Train:  [[ 12.72541454]]    Loss_Validation:  [[ 11.17188473]]\n",
      "Loop  4217 :    Loss_Train:  [[ 12.72541274]]    Loss_Validation:  [[ 11.17189541]]\n",
      "Loop  4218 :    Loss_Train:  [[ 12.72541095]]    Loss_Validation:  [[ 11.17190609]]\n",
      "Loop  4219 :    Loss_Train:  [[ 12.72540916]]    Loss_Validation:  [[ 11.17191676]]\n",
      "Loop  4220 :    Loss_Train:  [[ 12.72540738]]    Loss_Validation:  [[ 11.17192742]]\n",
      "Loop  4221 :    Loss_Train:  [[ 12.7254056]]    Loss_Validation:  [[ 11.17193808]]\n",
      "Loop  4222 :    Loss_Train:  [[ 12.72540382]]    Loss_Validation:  [[ 11.17194873]]\n",
      "Loop  4223 :    Loss_Train:  [[ 12.72540204]]    Loss_Validation:  [[ 11.17195937]]\n",
      "Loop  4224 :    Loss_Train:  [[ 12.72540027]]    Loss_Validation:  [[ 11.17197001]]\n",
      "Loop  4225 :    Loss_Train:  [[ 12.7253985]]    Loss_Validation:  [[ 11.17198064]]\n",
      "Loop  4226 :    Loss_Train:  [[ 12.72539673]]    Loss_Validation:  [[ 11.17199126]]\n",
      "Loop  4227 :    Loss_Train:  [[ 12.72539497]]    Loss_Validation:  [[ 11.17200187]]\n",
      "Loop  4228 :    Loss_Train:  [[ 12.72539321]]    Loss_Validation:  [[ 11.17201248]]\n",
      "Loop  4229 :    Loss_Train:  [[ 12.72539145]]    Loss_Validation:  [[ 11.17202308]]\n",
      "Loop  4230 :    Loss_Train:  [[ 12.72538969]]    Loss_Validation:  [[ 11.17203367]]\n",
      "Loop  4231 :    Loss_Train:  [[ 12.72538794]]    Loss_Validation:  [[ 11.17204426]]\n",
      "Loop  4232 :    Loss_Train:  [[ 12.72538619]]    Loss_Validation:  [[ 11.17205484]]\n",
      "Loop  4233 :    Loss_Train:  [[ 12.72538444]]    Loss_Validation:  [[ 11.17206541]]\n",
      "Loop  4234 :    Loss_Train:  [[ 12.7253827]]    Loss_Validation:  [[ 11.17207598]]\n",
      "Loop  4235 :    Loss_Train:  [[ 12.72538096]]    Loss_Validation:  [[ 11.17208654]]\n",
      "Loop  4236 :    Loss_Train:  [[ 12.72537922]]    Loss_Validation:  [[ 11.17209709]]\n",
      "Loop  4237 :    Loss_Train:  [[ 12.72537748]]    Loss_Validation:  [[ 11.17210763]]\n",
      "Loop  4238 :    Loss_Train:  [[ 12.72537575]]    Loss_Validation:  [[ 11.17211817]]\n",
      "Loop  4239 :    Loss_Train:  [[ 12.72537402]]    Loss_Validation:  [[ 11.1721287]]\n",
      "Loop  4240 :    Loss_Train:  [[ 12.72537229]]    Loss_Validation:  [[ 11.17213923]]\n",
      "Loop  4241 :    Loss_Train:  [[ 12.72537057]]    Loss_Validation:  [[ 11.17214974]]\n",
      "Loop  4242 :    Loss_Train:  [[ 12.72536885]]    Loss_Validation:  [[ 11.17216025]]\n",
      "Loop  4243 :    Loss_Train:  [[ 12.72536713]]    Loss_Validation:  [[ 11.17217076]]\n",
      "Loop  4244 :    Loss_Train:  [[ 12.72536542]]    Loss_Validation:  [[ 11.17218125]]\n",
      "Loop  4245 :    Loss_Train:  [[ 12.7253637]]    Loss_Validation:  [[ 11.17219174]]\n",
      "Loop  4246 :    Loss_Train:  [[ 12.72536199]]    Loss_Validation:  [[ 11.17220223]]\n",
      "Loop  4247 :    Loss_Train:  [[ 12.72536029]]    Loss_Validation:  [[ 11.1722127]]\n",
      "Loop  4248 :    Loss_Train:  [[ 12.72535858]]    Loss_Validation:  [[ 11.17222317]]\n",
      "Loop  4249 :    Loss_Train:  [[ 12.72535688]]    Loss_Validation:  [[ 11.17223363]]\n",
      "Loop  4250 :    Loss_Train:  [[ 12.72535518]]    Loss_Validation:  [[ 11.17224409]]\n",
      "Loop  4251 :    Loss_Train:  [[ 12.72535348]]    Loss_Validation:  [[ 11.17225454]]\n",
      "Loop  4252 :    Loss_Train:  [[ 12.72535179]]    Loss_Validation:  [[ 11.17226498]]\n",
      "Loop  4253 :    Loss_Train:  [[ 12.7253501]]    Loss_Validation:  [[ 11.17227542]]\n",
      "Loop  4254 :    Loss_Train:  [[ 12.72534841]]    Loss_Validation:  [[ 11.17228584]]\n",
      "Loop  4255 :    Loss_Train:  [[ 12.72534673]]    Loss_Validation:  [[ 11.17229627]]\n",
      "Loop  4256 :    Loss_Train:  [[ 12.72534505]]    Loss_Validation:  [[ 11.17230668]]\n",
      "Loop  4257 :    Loss_Train:  [[ 12.72534337]]    Loss_Validation:  [[ 11.17231709]]\n",
      "Loop  4258 :    Loss_Train:  [[ 12.72534169]]    Loss_Validation:  [[ 11.17232749]]\n",
      "Loop  4259 :    Loss_Train:  [[ 12.72534002]]    Loss_Validation:  [[ 11.17233788]]\n",
      "Loop  4260 :    Loss_Train:  [[ 12.72533835]]    Loss_Validation:  [[ 11.17234827]]\n",
      "Loop  4261 :    Loss_Train:  [[ 12.72533668]]    Loss_Validation:  [[ 11.17235865]]\n",
      "Loop  4262 :    Loss_Train:  [[ 12.72533501]]    Loss_Validation:  [[ 11.17236902]]\n",
      "Loop  4263 :    Loss_Train:  [[ 12.72533335]]    Loss_Validation:  [[ 11.17237939]]\n",
      "Loop  4264 :    Loss_Train:  [[ 12.72533169]]    Loss_Validation:  [[ 11.17238975]]\n",
      "Loop  4265 :    Loss_Train:  [[ 12.72533003]]    Loss_Validation:  [[ 11.1724001]]\n",
      "Loop  4266 :    Loss_Train:  [[ 12.72532838]]    Loss_Validation:  [[ 11.17241045]]\n",
      "Loop  4267 :    Loss_Train:  [[ 12.72532673]]    Loss_Validation:  [[ 11.17242079]]\n",
      "Loop  4268 :    Loss_Train:  [[ 12.72532508]]    Loss_Validation:  [[ 11.17243112]]\n",
      "Loop  4269 :    Loss_Train:  [[ 12.72532343]]    Loss_Validation:  [[ 11.17244145]]\n",
      "Loop  4270 :    Loss_Train:  [[ 12.72532179]]    Loss_Validation:  [[ 11.17245177]]\n",
      "Loop  4271 :    Loss_Train:  [[ 12.72532015]]    Loss_Validation:  [[ 11.17246208]]\n",
      "Loop  4272 :    Loss_Train:  [[ 12.72531851]]    Loss_Validation:  [[ 11.17247238]]\n",
      "Loop  4273 :    Loss_Train:  [[ 12.72531687]]    Loss_Validation:  [[ 11.17248268]]\n",
      "Loop  4274 :    Loss_Train:  [[ 12.72531524]]    Loss_Validation:  [[ 11.17249297]]\n",
      "Loop  4275 :    Loss_Train:  [[ 12.72531361]]    Loss_Validation:  [[ 11.17250326]]\n",
      "Loop  4276 :    Loss_Train:  [[ 12.72531198]]    Loss_Validation:  [[ 11.17251354]]\n",
      "Loop  4277 :    Loss_Train:  [[ 12.72531036]]    Loss_Validation:  [[ 11.17252381]]\n",
      "Loop  4278 :    Loss_Train:  [[ 12.72530873]]    Loss_Validation:  [[ 11.17253408]]\n",
      "Loop  4279 :    Loss_Train:  [[ 12.72530712]]    Loss_Validation:  [[ 11.17254433]]\n",
      "Loop  4280 :    Loss_Train:  [[ 12.7253055]]    Loss_Validation:  [[ 11.17255459]]\n",
      "Loop  4281 :    Loss_Train:  [[ 12.72530388]]    Loss_Validation:  [[ 11.17256483]]\n",
      "Loop  4282 :    Loss_Train:  [[ 12.72530227]]    Loss_Validation:  [[ 11.17257507]]\n",
      "Loop  4283 :    Loss_Train:  [[ 12.72530066]]    Loss_Validation:  [[ 11.1725853]]\n",
      "Loop  4284 :    Loss_Train:  [[ 12.72529906]]    Loss_Validation:  [[ 11.17259552]]\n",
      "Loop  4285 :    Loss_Train:  [[ 12.72529745]]    Loss_Validation:  [[ 11.17260574]]\n",
      "Loop  4286 :    Loss_Train:  [[ 12.72529585]]    Loss_Validation:  [[ 11.17261595]]\n",
      "Loop  4287 :    Loss_Train:  [[ 12.72529426]]    Loss_Validation:  [[ 11.17262616]]\n",
      "Loop  4288 :    Loss_Train:  [[ 12.72529266]]    Loss_Validation:  [[ 11.17263636]]\n",
      "Loop  4289 :    Loss_Train:  [[ 12.72529107]]    Loss_Validation:  [[ 11.17264655]]\n",
      "Loop  4290 :    Loss_Train:  [[ 12.72528948]]    Loss_Validation:  [[ 11.17265673]]\n",
      "Loop  4291 :    Loss_Train:  [[ 12.72528789]]    Loss_Validation:  [[ 11.17266691]]\n",
      "Loop  4292 :    Loss_Train:  [[ 12.7252863]]    Loss_Validation:  [[ 11.17267708]]\n",
      "Loop  4293 :    Loss_Train:  [[ 12.72528472]]    Loss_Validation:  [[ 11.17268724]]\n",
      "Loop  4294 :    Loss_Train:  [[ 12.72528314]]    Loss_Validation:  [[ 11.1726974]]\n",
      "Loop  4295 :    Loss_Train:  [[ 12.72528156]]    Loss_Validation:  [[ 11.17270755]]\n",
      "Loop  4296 :    Loss_Train:  [[ 12.72527999]]    Loss_Validation:  [[ 11.1727177]]\n",
      "Loop  4297 :    Loss_Train:  [[ 12.72527842]]    Loss_Validation:  [[ 11.17272783]]\n",
      "Loop  4298 :    Loss_Train:  [[ 12.72527685]]    Loss_Validation:  [[ 11.17273797]]\n",
      "Loop  4299 :    Loss_Train:  [[ 12.72527528]]    Loss_Validation:  [[ 11.17274809]]\n",
      "Loop  4300 :    Loss_Train:  [[ 12.72527371]]    Loss_Validation:  [[ 11.17275821]]\n",
      "Loop  4301 :    Loss_Train:  [[ 12.72527215]]    Loss_Validation:  [[ 11.17276832]]\n",
      "Loop  4302 :    Loss_Train:  [[ 12.72527059]]    Loss_Validation:  [[ 11.17277842]]\n",
      "Loop  4303 :    Loss_Train:  [[ 12.72526904]]    Loss_Validation:  [[ 11.17278852]]\n",
      "Loop  4304 :    Loss_Train:  [[ 12.72526748]]    Loss_Validation:  [[ 11.17279861]]\n",
      "Loop  4305 :    Loss_Train:  [[ 12.72526593]]    Loss_Validation:  [[ 11.1728087]]\n",
      "Loop  4306 :    Loss_Train:  [[ 12.72526438]]    Loss_Validation:  [[ 11.17281877]]\n",
      "Loop  4307 :    Loss_Train:  [[ 12.72526284]]    Loss_Validation:  [[ 11.17282885]]\n",
      "Loop  4308 :    Loss_Train:  [[ 12.72526129]]    Loss_Validation:  [[ 11.17283891]]\n",
      "Loop  4309 :    Loss_Train:  [[ 12.72525975]]    Loss_Validation:  [[ 11.17284897]]\n",
      "Loop  4310 :    Loss_Train:  [[ 12.72525821]]    Loss_Validation:  [[ 11.17285902]]\n",
      "Loop  4311 :    Loss_Train:  [[ 12.72525667]]    Loss_Validation:  [[ 11.17286906]]\n",
      "Loop  4312 :    Loss_Train:  [[ 12.72525514]]    Loss_Validation:  [[ 11.1728791]]\n",
      "Loop  4313 :    Loss_Train:  [[ 12.72525361]]    Loss_Validation:  [[ 11.17288913]]\n",
      "Loop  4314 :    Loss_Train:  [[ 12.72525208]]    Loss_Validation:  [[ 11.17289916]]\n",
      "Loop  4315 :    Loss_Train:  [[ 12.72525055]]    Loss_Validation:  [[ 11.17290918]]\n",
      "Loop  4316 :    Loss_Train:  [[ 12.72524903]]    Loss_Validation:  [[ 11.17291919]]\n",
      "Loop  4317 :    Loss_Train:  [[ 12.72524751]]    Loss_Validation:  [[ 11.17292919]]\n",
      "Loop  4318 :    Loss_Train:  [[ 12.72524599]]    Loss_Validation:  [[ 11.17293919]]\n",
      "Loop  4319 :    Loss_Train:  [[ 12.72524447]]    Loss_Validation:  [[ 11.17294918]]\n",
      "Loop  4320 :    Loss_Train:  [[ 12.72524296]]    Loss_Validation:  [[ 11.17295917]]\n",
      "Loop  4321 :    Loss_Train:  [[ 12.72524145]]    Loss_Validation:  [[ 11.17296915]]\n",
      "Loop  4322 :    Loss_Train:  [[ 12.72523994]]    Loss_Validation:  [[ 11.17297912]]\n",
      "Loop  4323 :    Loss_Train:  [[ 12.72523843]]    Loss_Validation:  [[ 11.17298909]]\n",
      "Loop  4324 :    Loss_Train:  [[ 12.72523693]]    Loss_Validation:  [[ 11.17299904]]\n",
      "Loop  4325 :    Loss_Train:  [[ 12.72523543]]    Loss_Validation:  [[ 11.173009]]\n",
      "Loop  4326 :    Loss_Train:  [[ 12.72523393]]    Loss_Validation:  [[ 11.17301894]]\n",
      "Loop  4327 :    Loss_Train:  [[ 12.72523243]]    Loss_Validation:  [[ 11.17302888]]\n",
      "Loop  4328 :    Loss_Train:  [[ 12.72523094]]    Loss_Validation:  [[ 11.17303882]]\n",
      "Loop  4329 :    Loss_Train:  [[ 12.72522945]]    Loss_Validation:  [[ 11.17304874]]\n",
      "Loop  4330 :    Loss_Train:  [[ 12.72522796]]    Loss_Validation:  [[ 11.17305866]]\n",
      "Loop  4331 :    Loss_Train:  [[ 12.72522647]]    Loss_Validation:  [[ 11.17306857]]\n",
      "Loop  4332 :    Loss_Train:  [[ 12.72522499]]    Loss_Validation:  [[ 11.17307848]]\n",
      "Loop  4333 :    Loss_Train:  [[ 12.72522351]]    Loss_Validation:  [[ 11.17308838]]\n",
      "Loop  4334 :    Loss_Train:  [[ 12.72522203]]    Loss_Validation:  [[ 11.17309827]]\n",
      "Loop  4335 :    Loss_Train:  [[ 12.72522055]]    Loss_Validation:  [[ 11.17310816]]\n",
      "Loop  4336 :    Loss_Train:  [[ 12.72521907]]    Loss_Validation:  [[ 11.17311804]]\n",
      "Loop  4337 :    Loss_Train:  [[ 12.7252176]]    Loss_Validation:  [[ 11.17312792]]\n",
      "Loop  4338 :    Loss_Train:  [[ 12.72521613]]    Loss_Validation:  [[ 11.17313778]]\n",
      "Loop  4339 :    Loss_Train:  [[ 12.72521467]]    Loss_Validation:  [[ 11.17314765]]\n",
      "Loop  4340 :    Loss_Train:  [[ 12.7252132]]    Loss_Validation:  [[ 11.1731575]]\n",
      "Loop  4341 :    Loss_Train:  [[ 12.72521174]]    Loss_Validation:  [[ 11.17316735]]\n",
      "Loop  4342 :    Loss_Train:  [[ 12.72521028]]    Loss_Validation:  [[ 11.17317719]]\n",
      "Loop  4343 :    Loss_Train:  [[ 12.72520882]]    Loss_Validation:  [[ 11.17318702]]\n",
      "Loop  4344 :    Loss_Train:  [[ 12.72520737]]    Loss_Validation:  [[ 11.17319685]]\n",
      "Loop  4345 :    Loss_Train:  [[ 12.72520591]]    Loss_Validation:  [[ 11.17320668]]\n",
      "Loop  4346 :    Loss_Train:  [[ 12.72520446]]    Loss_Validation:  [[ 11.17321649]]\n",
      "Loop  4347 :    Loss_Train:  [[ 12.72520301]]    Loss_Validation:  [[ 11.1732263]]\n",
      "Loop  4348 :    Loss_Train:  [[ 12.72520157]]    Loss_Validation:  [[ 11.1732361]]\n",
      "Loop  4349 :    Loss_Train:  [[ 12.72520013]]    Loss_Validation:  [[ 11.1732459]]\n",
      "Loop  4350 :    Loss_Train:  [[ 12.72519868]]    Loss_Validation:  [[ 11.17325569]]\n",
      "Loop  4351 :    Loss_Train:  [[ 12.72519725]]    Loss_Validation:  [[ 11.17326547]]\n",
      "Loop  4352 :    Loss_Train:  [[ 12.72519581]]    Loss_Validation:  [[ 11.17327525]]\n",
      "Loop  4353 :    Loss_Train:  [[ 12.72519438]]    Loss_Validation:  [[ 11.17328502]]\n",
      "Loop  4354 :    Loss_Train:  [[ 12.72519294]]    Loss_Validation:  [[ 11.17329478]]\n",
      "Loop  4355 :    Loss_Train:  [[ 12.72519152]]    Loss_Validation:  [[ 11.17330454]]\n",
      "Loop  4356 :    Loss_Train:  [[ 12.72519009]]    Loss_Validation:  [[ 11.17331429]]\n",
      "Loop  4357 :    Loss_Train:  [[ 12.72518866]]    Loss_Validation:  [[ 11.17332404]]\n",
      "Loop  4358 :    Loss_Train:  [[ 12.72518724]]    Loss_Validation:  [[ 11.17333378]]\n",
      "Loop  4359 :    Loss_Train:  [[ 12.72518582]]    Loss_Validation:  [[ 11.17334351]]\n",
      "Loop  4360 :    Loss_Train:  [[ 12.7251844]]    Loss_Validation:  [[ 11.17335323]]\n",
      "Loop  4361 :    Loss_Train:  [[ 12.72518299]]    Loss_Validation:  [[ 11.17336295]]\n",
      "Loop  4362 :    Loss_Train:  [[ 12.72518158]]    Loss_Validation:  [[ 11.17337266]]\n",
      "Loop  4363 :    Loss_Train:  [[ 12.72518017]]    Loss_Validation:  [[ 11.17338237]]\n",
      "Loop  4364 :    Loss_Train:  [[ 12.72517876]]    Loss_Validation:  [[ 11.17339207]]\n",
      "Loop  4365 :    Loss_Train:  [[ 12.72517735]]    Loss_Validation:  [[ 11.17340176]]\n",
      "Loop  4366 :    Loss_Train:  [[ 12.72517595]]    Loss_Validation:  [[ 11.17341145]]\n",
      "Loop  4367 :    Loss_Train:  [[ 12.72517455]]    Loss_Validation:  [[ 11.17342113]]\n",
      "Loop  4368 :    Loss_Train:  [[ 12.72517315]]    Loss_Validation:  [[ 11.17343081]]\n",
      "Loop  4369 :    Loss_Train:  [[ 12.72517175]]    Loss_Validation:  [[ 11.17344047]]\n",
      "Loop  4370 :    Loss_Train:  [[ 12.72517036]]    Loss_Validation:  [[ 11.17345014]]\n",
      "Loop  4371 :    Loss_Train:  [[ 12.72516897]]    Loss_Validation:  [[ 11.17345979]]\n",
      "Loop  4372 :    Loss_Train:  [[ 12.72516758]]    Loss_Validation:  [[ 11.17346944]]\n",
      "Loop  4373 :    Loss_Train:  [[ 12.72516619]]    Loss_Validation:  [[ 11.17347908]]\n",
      "Loop  4374 :    Loss_Train:  [[ 12.7251648]]    Loss_Validation:  [[ 11.17348872]]\n",
      "Loop  4375 :    Loss_Train:  [[ 12.72516342]]    Loss_Validation:  [[ 11.17349835]]\n",
      "Loop  4376 :    Loss_Train:  [[ 12.72516204]]    Loss_Validation:  [[ 11.17350797]]\n",
      "Loop  4377 :    Loss_Train:  [[ 12.72516066]]    Loss_Validation:  [[ 11.17351759]]\n",
      "Loop  4378 :    Loss_Train:  [[ 12.72515928]]    Loss_Validation:  [[ 11.1735272]]\n",
      "Loop  4379 :    Loss_Train:  [[ 12.72515791]]    Loss_Validation:  [[ 11.1735368]]\n",
      "Loop  4380 :    Loss_Train:  [[ 12.72515654]]    Loss_Validation:  [[ 11.1735464]]\n",
      "Loop  4381 :    Loss_Train:  [[ 12.72515517]]    Loss_Validation:  [[ 11.17355599]]\n",
      "Loop  4382 :    Loss_Train:  [[ 12.7251538]]    Loss_Validation:  [[ 11.17356558]]\n",
      "Loop  4383 :    Loss_Train:  [[ 12.72515244]]    Loss_Validation:  [[ 11.17357516]]\n",
      "Loop  4384 :    Loss_Train:  [[ 12.72515108]]    Loss_Validation:  [[ 11.17358473]]\n",
      "Loop  4385 :    Loss_Train:  [[ 12.72514971]]    Loss_Validation:  [[ 11.17359429]]\n",
      "Loop  4386 :    Loss_Train:  [[ 12.72514836]]    Loss_Validation:  [[ 11.17360385]]\n",
      "Loop  4387 :    Loss_Train:  [[ 12.725147]]    Loss_Validation:  [[ 11.17361341]]\n",
      "Loop  4388 :    Loss_Train:  [[ 12.72514565]]    Loss_Validation:  [[ 11.17362295]]\n",
      "Loop  4389 :    Loss_Train:  [[ 12.7251443]]    Loss_Validation:  [[ 11.1736325]]\n",
      "Loop  4390 :    Loss_Train:  [[ 12.72514295]]    Loss_Validation:  [[ 11.17364203]]\n",
      "Loop  4391 :    Loss_Train:  [[ 12.7251416]]    Loss_Validation:  [[ 11.17365156]]\n",
      "Loop  4392 :    Loss_Train:  [[ 12.72514025]]    Loss_Validation:  [[ 11.17366108]]\n",
      "Loop  4393 :    Loss_Train:  [[ 12.72513891]]    Loss_Validation:  [[ 11.1736706]]\n",
      "Loop  4394 :    Loss_Train:  [[ 12.72513757]]    Loss_Validation:  [[ 11.17368011]]\n",
      "Loop  4395 :    Loss_Train:  [[ 12.72513623]]    Loss_Validation:  [[ 11.17368961]]\n",
      "Loop  4396 :    Loss_Train:  [[ 12.7251349]]    Loss_Validation:  [[ 11.17369911]]\n",
      "Loop  4397 :    Loss_Train:  [[ 12.72513356]]    Loss_Validation:  [[ 11.1737086]]\n",
      "Loop  4398 :    Loss_Train:  [[ 12.72513223]]    Loss_Validation:  [[ 11.17371808]]\n",
      "Loop  4399 :    Loss_Train:  [[ 12.7251309]]    Loss_Validation:  [[ 11.17372756]]\n",
      "Loop  4400 :    Loss_Train:  [[ 12.72512957]]    Loss_Validation:  [[ 11.17373703]]\n",
      "Loop  4401 :    Loss_Train:  [[ 12.72512825]]    Loss_Validation:  [[ 11.1737465]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  4402 :    Loss_Train:  [[ 12.72512693]]    Loss_Validation:  [[ 11.17375596]]\n",
      "Loop  4403 :    Loss_Train:  [[ 12.72512561]]    Loss_Validation:  [[ 11.17376541]]\n",
      "Loop  4404 :    Loss_Train:  [[ 12.72512429]]    Loss_Validation:  [[ 11.17377486]]\n",
      "Loop  4405 :    Loss_Train:  [[ 12.72512297]]    Loss_Validation:  [[ 11.1737843]]\n",
      "Loop  4406 :    Loss_Train:  [[ 12.72512166]]    Loss_Validation:  [[ 11.17379373]]\n",
      "Loop  4407 :    Loss_Train:  [[ 12.72512034]]    Loss_Validation:  [[ 11.17380316]]\n",
      "Loop  4408 :    Loss_Train:  [[ 12.72511903]]    Loss_Validation:  [[ 11.17381258]]\n",
      "Loop  4409 :    Loss_Train:  [[ 12.72511773]]    Loss_Validation:  [[ 11.173822]]\n",
      "Loop  4410 :    Loss_Train:  [[ 12.72511642]]    Loss_Validation:  [[ 11.1738314]]\n",
      "Loop  4411 :    Loss_Train:  [[ 12.72511512]]    Loss_Validation:  [[ 11.17384081]]\n",
      "Loop  4412 :    Loss_Train:  [[ 12.72511382]]    Loss_Validation:  [[ 11.1738502]]\n",
      "Loop  4413 :    Loss_Train:  [[ 12.72511252]]    Loss_Validation:  [[ 11.1738596]]\n",
      "Loop  4414 :    Loss_Train:  [[ 12.72511122]]    Loss_Validation:  [[ 11.17386898]]\n",
      "Loop  4415 :    Loss_Train:  [[ 12.72510992]]    Loss_Validation:  [[ 11.17387836]]\n",
      "Loop  4416 :    Loss_Train:  [[ 12.72510863]]    Loss_Validation:  [[ 11.17388773]]\n",
      "Loop  4417 :    Loss_Train:  [[ 12.72510734]]    Loss_Validation:  [[ 11.1738971]]\n",
      "Loop  4418 :    Loss_Train:  [[ 12.72510605]]    Loss_Validation:  [[ 11.17390646]]\n",
      "Loop  4419 :    Loss_Train:  [[ 12.72510477]]    Loss_Validation:  [[ 11.17391581]]\n",
      "Loop  4420 :    Loss_Train:  [[ 12.72510348]]    Loss_Validation:  [[ 11.17392516]]\n",
      "Loop  4421 :    Loss_Train:  [[ 12.7251022]]    Loss_Validation:  [[ 11.1739345]]\n",
      "Loop  4422 :    Loss_Train:  [[ 12.72510092]]    Loss_Validation:  [[ 11.17394383]]\n",
      "Loop  4423 :    Loss_Train:  [[ 12.72509964]]    Loss_Validation:  [[ 11.17395316]]\n",
      "Loop  4424 :    Loss_Train:  [[ 12.72509836]]    Loss_Validation:  [[ 11.17396248]]\n",
      "Loop  4425 :    Loss_Train:  [[ 12.72509709]]    Loss_Validation:  [[ 11.1739718]]\n",
      "Loop  4426 :    Loss_Train:  [[ 12.72509582]]    Loss_Validation:  [[ 11.17398111]]\n",
      "Loop  4427 :    Loss_Train:  [[ 12.72509455]]    Loss_Validation:  [[ 11.17399041]]\n",
      "Loop  4428 :    Loss_Train:  [[ 12.72509328]]    Loss_Validation:  [[ 11.17399971]]\n",
      "Loop  4429 :    Loss_Train:  [[ 12.72509202]]    Loss_Validation:  [[ 11.174009]]\n",
      "Loop  4430 :    Loss_Train:  [[ 12.72509075]]    Loss_Validation:  [[ 11.17401829]]\n",
      "Loop  4431 :    Loss_Train:  [[ 12.72508949]]    Loss_Validation:  [[ 11.17402757]]\n",
      "Loop  4432 :    Loss_Train:  [[ 12.72508823]]    Loss_Validation:  [[ 11.17403684]]\n",
      "Loop  4433 :    Loss_Train:  [[ 12.72508697]]    Loss_Validation:  [[ 11.17404611]]\n",
      "Loop  4434 :    Loss_Train:  [[ 12.72508572]]    Loss_Validation:  [[ 11.17405537]]\n",
      "Loop  4435 :    Loss_Train:  [[ 12.72508447]]    Loss_Validation:  [[ 11.17406462]]\n",
      "Loop  4436 :    Loss_Train:  [[ 12.72508321]]    Loss_Validation:  [[ 11.17407387]]\n",
      "Loop  4437 :    Loss_Train:  [[ 12.72508197]]    Loss_Validation:  [[ 11.17408312]]\n",
      "Loop  4438 :    Loss_Train:  [[ 12.72508072]]    Loss_Validation:  [[ 11.17409235]]\n",
      "Loop  4439 :    Loss_Train:  [[ 12.72507947]]    Loss_Validation:  [[ 11.17410158]]\n",
      "Loop  4440 :    Loss_Train:  [[ 12.72507823]]    Loss_Validation:  [[ 11.17411081]]\n",
      "Loop  4441 :    Loss_Train:  [[ 12.72507699]]    Loss_Validation:  [[ 11.17412003]]\n",
      "Loop  4442 :    Loss_Train:  [[ 12.72507575]]    Loss_Validation:  [[ 11.17412924]]\n",
      "Loop  4443 :    Loss_Train:  [[ 12.72507451]]    Loss_Validation:  [[ 11.17413844]]\n",
      "Loop  4444 :    Loss_Train:  [[ 12.72507328]]    Loss_Validation:  [[ 11.17414764]]\n",
      "Loop  4445 :    Loss_Train:  [[ 12.72507205]]    Loss_Validation:  [[ 11.17415684]]\n",
      "Loop  4446 :    Loss_Train:  [[ 12.72507082]]    Loss_Validation:  [[ 11.17416602]]\n",
      "Loop  4447 :    Loss_Train:  [[ 12.72506959]]    Loss_Validation:  [[ 11.17417521]]\n",
      "Loop  4448 :    Loss_Train:  [[ 12.72506836]]    Loss_Validation:  [[ 11.17418438]]\n",
      "Loop  4449 :    Loss_Train:  [[ 12.72506714]]    Loss_Validation:  [[ 11.17419355]]\n",
      "Loop  4450 :    Loss_Train:  [[ 12.72506591]]    Loss_Validation:  [[ 11.17420271]]\n",
      "Loop  4451 :    Loss_Train:  [[ 12.72506469]]    Loss_Validation:  [[ 11.17421187]]\n",
      "Loop  4452 :    Loss_Train:  [[ 12.72506347]]    Loss_Validation:  [[ 11.17422102]]\n",
      "Loop  4453 :    Loss_Train:  [[ 12.72506226]]    Loss_Validation:  [[ 11.17423017]]\n",
      "Loop  4454 :    Loss_Train:  [[ 12.72506104]]    Loss_Validation:  [[ 11.17423931]]\n",
      "Loop  4455 :    Loss_Train:  [[ 12.72505983]]    Loss_Validation:  [[ 11.17424844]]\n",
      "Loop  4456 :    Loss_Train:  [[ 12.72505862]]    Loss_Validation:  [[ 11.17425757]]\n",
      "Loop  4457 :    Loss_Train:  [[ 12.72505741]]    Loss_Validation:  [[ 11.17426669]]\n",
      "Loop  4458 :    Loss_Train:  [[ 12.7250562]]    Loss_Validation:  [[ 11.1742758]]\n",
      "Loop  4459 :    Loss_Train:  [[ 12.725055]]    Loss_Validation:  [[ 11.17428491]]\n",
      "Loop  4460 :    Loss_Train:  [[ 12.7250538]]    Loss_Validation:  [[ 11.17429401]]\n",
      "Loop  4461 :    Loss_Train:  [[ 12.72505259]]    Loss_Validation:  [[ 11.17430311]]\n",
      "Loop  4462 :    Loss_Train:  [[ 12.7250514]]    Loss_Validation:  [[ 11.1743122]]\n",
      "Loop  4463 :    Loss_Train:  [[ 12.7250502]]    Loss_Validation:  [[ 11.17432129]]\n",
      "Loop  4464 :    Loss_Train:  [[ 12.725049]]    Loss_Validation:  [[ 11.17433036]]\n",
      "Loop  4465 :    Loss_Train:  [[ 12.72504781]]    Loss_Validation:  [[ 11.17433944]]\n",
      "Loop  4466 :    Loss_Train:  [[ 12.72504662]]    Loss_Validation:  [[ 11.1743485]]\n",
      "Loop  4467 :    Loss_Train:  [[ 12.72504543]]    Loss_Validation:  [[ 11.17435756]]\n",
      "Loop  4468 :    Loss_Train:  [[ 12.72504424]]    Loss_Validation:  [[ 11.17436662]]\n",
      "Loop  4469 :    Loss_Train:  [[ 12.72504306]]    Loss_Validation:  [[ 11.17437567]]\n",
      "Loop  4470 :    Loss_Train:  [[ 12.72504187]]    Loss_Validation:  [[ 11.17438471]]\n",
      "Loop  4471 :    Loss_Train:  [[ 12.72504069]]    Loss_Validation:  [[ 11.17439375]]\n",
      "Loop  4472 :    Loss_Train:  [[ 12.72503951]]    Loss_Validation:  [[ 11.17440278]]\n",
      "Loop  4473 :    Loss_Train:  [[ 12.72503834]]    Loss_Validation:  [[ 11.1744118]]\n",
      "Loop  4474 :    Loss_Train:  [[ 12.72503716]]    Loss_Validation:  [[ 11.17442082]]\n",
      "Loop  4475 :    Loss_Train:  [[ 12.72503599]]    Loss_Validation:  [[ 11.17442983]]\n",
      "Loop  4476 :    Loss_Train:  [[ 12.72503482]]    Loss_Validation:  [[ 11.17443884]]\n",
      "Loop  4477 :    Loss_Train:  [[ 12.72503365]]    Loss_Validation:  [[ 11.17444784]]\n",
      "Loop  4478 :    Loss_Train:  [[ 12.72503248]]    Loss_Validation:  [[ 11.17445683]]\n",
      "Loop  4479 :    Loss_Train:  [[ 12.72503131]]    Loss_Validation:  [[ 11.17446582]]\n",
      "Loop  4480 :    Loss_Train:  [[ 12.72503015]]    Loss_Validation:  [[ 11.1744748]]\n",
      "Loop  4481 :    Loss_Train:  [[ 12.72502899]]    Loss_Validation:  [[ 11.17448378]]\n",
      "Loop  4482 :    Loss_Train:  [[ 12.72502783]]    Loss_Validation:  [[ 11.17449275]]\n",
      "Loop  4483 :    Loss_Train:  [[ 12.72502667]]    Loss_Validation:  [[ 11.17450172]]\n",
      "Loop  4484 :    Loss_Train:  [[ 12.72502551]]    Loss_Validation:  [[ 11.17451068]]\n",
      "Loop  4485 :    Loss_Train:  [[ 12.72502436]]    Loss_Validation:  [[ 11.17451963]]\n",
      "Loop  4486 :    Loss_Train:  [[ 12.7250232]]    Loss_Validation:  [[ 11.17452858]]\n",
      "Loop  4487 :    Loss_Train:  [[ 12.72502205]]    Loss_Validation:  [[ 11.17453752]]\n",
      "Loop  4488 :    Loss_Train:  [[ 12.7250209]]    Loss_Validation:  [[ 11.17454645]]\n",
      "Loop  4489 :    Loss_Train:  [[ 12.72501976]]    Loss_Validation:  [[ 11.17455538]]\n",
      "Loop  4490 :    Loss_Train:  [[ 12.72501861]]    Loss_Validation:  [[ 11.1745643]]\n",
      "Loop  4491 :    Loss_Train:  [[ 12.72501747]]    Loss_Validation:  [[ 11.17457322]]\n",
      "Loop  4492 :    Loss_Train:  [[ 12.72501633]]    Loss_Validation:  [[ 11.17458213]]\n",
      "Loop  4493 :    Loss_Train:  [[ 12.72501519]]    Loss_Validation:  [[ 11.17459104]]\n",
      "Loop  4494 :    Loss_Train:  [[ 12.72501405]]    Loss_Validation:  [[ 11.17459994]]\n",
      "Loop  4495 :    Loss_Train:  [[ 12.72501291]]    Loss_Validation:  [[ 11.17460883]]\n",
      "Loop  4496 :    Loss_Train:  [[ 12.72501178]]    Loss_Validation:  [[ 11.17461772]]\n",
      "Loop  4497 :    Loss_Train:  [[ 12.72501065]]    Loss_Validation:  [[ 11.1746266]]\n",
      "Loop  4498 :    Loss_Train:  [[ 12.72500952]]    Loss_Validation:  [[ 11.17463548]]\n",
      "Loop  4499 :    Loss_Train:  [[ 12.72500839]]    Loss_Validation:  [[ 11.17464435]]\n",
      "Loop  4500 :    Loss_Train:  [[ 12.72500726]]    Loss_Validation:  [[ 11.17465321]]\n",
      "Loop  4501 :    Loss_Train:  [[ 12.72500614]]    Loss_Validation:  [[ 11.17466207]]\n",
      "Loop  4502 :    Loss_Train:  [[ 12.72500502]]    Loss_Validation:  [[ 11.17467092]]\n",
      "Loop  4503 :    Loss_Train:  [[ 12.7250039]]    Loss_Validation:  [[ 11.17467977]]\n",
      "Loop  4504 :    Loss_Train:  [[ 12.72500278]]    Loss_Validation:  [[ 11.17468861]]\n",
      "Loop  4505 :    Loss_Train:  [[ 12.72500166]]    Loss_Validation:  [[ 11.17469744]]\n",
      "Loop  4506 :    Loss_Train:  [[ 12.72500054]]    Loss_Validation:  [[ 11.17470627]]\n",
      "Loop  4507 :    Loss_Train:  [[ 12.72499943]]    Loss_Validation:  [[ 11.17471509]]\n",
      "Loop  4508 :    Loss_Train:  [[ 12.72499832]]    Loss_Validation:  [[ 11.17472391]]\n",
      "Loop  4509 :    Loss_Train:  [[ 12.72499721]]    Loss_Validation:  [[ 11.17473272]]\n",
      "Loop  4510 :    Loss_Train:  [[ 12.7249961]]    Loss_Validation:  [[ 11.17474153]]\n",
      "Loop  4511 :    Loss_Train:  [[ 12.72499499]]    Loss_Validation:  [[ 11.17475033]]\n",
      "Loop  4512 :    Loss_Train:  [[ 12.72499389]]    Loss_Validation:  [[ 11.17475912]]\n",
      "Loop  4513 :    Loss_Train:  [[ 12.72499279]]    Loss_Validation:  [[ 11.17476791]]\n",
      "Loop  4514 :    Loss_Train:  [[ 12.72499169]]    Loss_Validation:  [[ 11.17477669]]\n",
      "Loop  4515 :    Loss_Train:  [[ 12.72499059]]    Loss_Validation:  [[ 11.17478547]]\n",
      "Loop  4516 :    Loss_Train:  [[ 12.72498949]]    Loss_Validation:  [[ 11.17479424]]\n",
      "Loop  4517 :    Loss_Train:  [[ 12.72498839]]    Loss_Validation:  [[ 11.174803]]\n",
      "Loop  4518 :    Loss_Train:  [[ 12.7249873]]    Loss_Validation:  [[ 11.17481176]]\n",
      "Loop  4519 :    Loss_Train:  [[ 12.72498621]]    Loss_Validation:  [[ 11.17482051]]\n",
      "Loop  4520 :    Loss_Train:  [[ 12.72498512]]    Loss_Validation:  [[ 11.17482926]]\n",
      "Loop  4521 :    Loss_Train:  [[ 12.72498403]]    Loss_Validation:  [[ 11.174838]]\n",
      "Loop  4522 :    Loss_Train:  [[ 12.72498294]]    Loss_Validation:  [[ 11.17484674]]\n",
      "Loop  4523 :    Loss_Train:  [[ 12.72498186]]    Loss_Validation:  [[ 11.17485547]]\n",
      "Loop  4524 :    Loss_Train:  [[ 12.72498077]]    Loss_Validation:  [[ 11.17486419]]\n",
      "Loop  4525 :    Loss_Train:  [[ 12.72497969]]    Loss_Validation:  [[ 11.17487291]]\n",
      "Loop  4526 :    Loss_Train:  [[ 12.72497861]]    Loss_Validation:  [[ 11.17488162]]\n",
      "Loop  4527 :    Loss_Train:  [[ 12.72497754]]    Loss_Validation:  [[ 11.17489033]]\n",
      "Loop  4528 :    Loss_Train:  [[ 12.72497646]]    Loss_Validation:  [[ 11.17489903]]\n",
      "Loop  4529 :    Loss_Train:  [[ 12.72497539]]    Loss_Validation:  [[ 11.17490772]]\n",
      "Loop  4530 :    Loss_Train:  [[ 12.72497431]]    Loss_Validation:  [[ 11.17491641]]\n",
      "Loop  4531 :    Loss_Train:  [[ 12.72497324]]    Loss_Validation:  [[ 11.17492509]]\n",
      "Loop  4532 :    Loss_Train:  [[ 12.72497217]]    Loss_Validation:  [[ 11.17493377]]\n",
      "Loop  4533 :    Loss_Train:  [[ 12.72497111]]    Loss_Validation:  [[ 11.17494244]]\n",
      "Loop  4534 :    Loss_Train:  [[ 12.72497004]]    Loss_Validation:  [[ 11.17495111]]\n",
      "Loop  4535 :    Loss_Train:  [[ 12.72496898]]    Loss_Validation:  [[ 11.17495977]]\n",
      "Loop  4536 :    Loss_Train:  [[ 12.72496792]]    Loss_Validation:  [[ 11.17496842]]\n",
      "Loop  4537 :    Loss_Train:  [[ 12.72496686]]    Loss_Validation:  [[ 11.17497707]]\n",
      "Loop  4538 :    Loss_Train:  [[ 12.7249658]]    Loss_Validation:  [[ 11.17498572]]\n",
      "Loop  4539 :    Loss_Train:  [[ 12.72496474]]    Loss_Validation:  [[ 11.17499435]]\n",
      "Loop  4540 :    Loss_Train:  [[ 12.72496369]]    Loss_Validation:  [[ 11.17500298]]\n",
      "Loop  4541 :    Loss_Train:  [[ 12.72496263]]    Loss_Validation:  [[ 11.17501161]]\n",
      "Loop  4542 :    Loss_Train:  [[ 12.72496158]]    Loss_Validation:  [[ 11.17502023]]\n",
      "Loop  4543 :    Loss_Train:  [[ 12.72496053]]    Loss_Validation:  [[ 11.17502884]]\n",
      "Loop  4544 :    Loss_Train:  [[ 12.72495948]]    Loss_Validation:  [[ 11.17503745]]\n",
      "Loop  4545 :    Loss_Train:  [[ 12.72495844]]    Loss_Validation:  [[ 11.17504605]]\n",
      "Loop  4546 :    Loss_Train:  [[ 12.72495739]]    Loss_Validation:  [[ 11.17505465]]\n",
      "Loop  4547 :    Loss_Train:  [[ 12.72495635]]    Loss_Validation:  [[ 11.17506324]]\n",
      "Loop  4548 :    Loss_Train:  [[ 12.72495531]]    Loss_Validation:  [[ 11.17507183]]\n",
      "Loop  4549 :    Loss_Train:  [[ 12.72495427]]    Loss_Validation:  [[ 11.17508041]]\n",
      "Loop  4550 :    Loss_Train:  [[ 12.72495323]]    Loss_Validation:  [[ 11.17508898]]\n",
      "Loop  4551 :    Loss_Train:  [[ 12.72495219]]    Loss_Validation:  [[ 11.17509755]]\n",
      "Loop  4552 :    Loss_Train:  [[ 12.72495116]]    Loss_Validation:  [[ 11.17510611]]\n",
      "Loop  4553 :    Loss_Train:  [[ 12.72495013]]    Loss_Validation:  [[ 11.17511467]]\n",
      "Loop  4554 :    Loss_Train:  [[ 12.72494909]]    Loss_Validation:  [[ 11.17512322]]\n",
      "Loop  4555 :    Loss_Train:  [[ 12.72494807]]    Loss_Validation:  [[ 11.17513177]]\n",
      "Loop  4556 :    Loss_Train:  [[ 12.72494704]]    Loss_Validation:  [[ 11.17514031]]\n",
      "Loop  4557 :    Loss_Train:  [[ 12.72494601]]    Loss_Validation:  [[ 11.17514884]]\n",
      "Loop  4558 :    Loss_Train:  [[ 12.72494499]]    Loss_Validation:  [[ 11.17515737]]\n",
      "Loop  4559 :    Loss_Train:  [[ 12.72494396]]    Loss_Validation:  [[ 11.17516589]]\n",
      "Loop  4560 :    Loss_Train:  [[ 12.72494294]]    Loss_Validation:  [[ 11.17517441]]\n",
      "Loop  4561 :    Loss_Train:  [[ 12.72494192]]    Loss_Validation:  [[ 11.17518292]]\n",
      "Loop  4562 :    Loss_Train:  [[ 12.72494091]]    Loss_Validation:  [[ 11.17519143]]\n",
      "Loop  4563 :    Loss_Train:  [[ 12.72493989]]    Loss_Validation:  [[ 11.17519993]]\n",
      "Loop  4564 :    Loss_Train:  [[ 12.72493888]]    Loss_Validation:  [[ 11.17520842]]\n",
      "Loop  4565 :    Loss_Train:  [[ 12.72493786]]    Loss_Validation:  [[ 11.17521691]]\n",
      "Loop  4566 :    Loss_Train:  [[ 12.72493685]]    Loss_Validation:  [[ 11.17522539]]\n",
      "Loop  4567 :    Loss_Train:  [[ 12.72493584]]    Loss_Validation:  [[ 11.17523387]]\n",
      "Loop  4568 :    Loss_Train:  [[ 12.72493483]]    Loss_Validation:  [[ 11.17524234]]\n",
      "Loop  4569 :    Loss_Train:  [[ 12.72493383]]    Loss_Validation:  [[ 11.17525081]]\n",
      "Loop  4570 :    Loss_Train:  [[ 12.72493282]]    Loss_Validation:  [[ 11.17525927]]\n",
      "Loop  4571 :    Loss_Train:  [[ 12.72493182]]    Loss_Validation:  [[ 11.17526772]]\n",
      "Loop  4572 :    Loss_Train:  [[ 12.72493082]]    Loss_Validation:  [[ 11.17527617]]\n",
      "Loop  4573 :    Loss_Train:  [[ 12.72492982]]    Loss_Validation:  [[ 11.17528461]]\n",
      "Loop  4574 :    Loss_Train:  [[ 12.72492882]]    Loss_Validation:  [[ 11.17529305]]\n",
      "Loop  4575 :    Loss_Train:  [[ 12.72492783]]    Loss_Validation:  [[ 11.17530148]]\n",
      "Loop  4576 :    Loss_Train:  [[ 12.72492683]]    Loss_Validation:  [[ 11.17530991]]\n",
      "Loop  4577 :    Loss_Train:  [[ 12.72492584]]    Loss_Validation:  [[ 11.17531833]]\n",
      "Loop  4578 :    Loss_Train:  [[ 12.72492485]]    Loss_Validation:  [[ 11.17532675]]\n",
      "Loop  4579 :    Loss_Train:  [[ 12.72492386]]    Loss_Validation:  [[ 11.17533516]]\n",
      "Loop  4580 :    Loss_Train:  [[ 12.72492287]]    Loss_Validation:  [[ 11.17534356]]\n",
      "Loop  4581 :    Loss_Train:  [[ 12.72492188]]    Loss_Validation:  [[ 11.17535196]]\n",
      "Loop  4582 :    Loss_Train:  [[ 12.7249209]]    Loss_Validation:  [[ 11.17536035]]\n",
      "Loop  4583 :    Loss_Train:  [[ 12.72491991]]    Loss_Validation:  [[ 11.17536874]]\n",
      "Loop  4584 :    Loss_Train:  [[ 12.72491893]]    Loss_Validation:  [[ 11.17537712]]\n",
      "Loop  4585 :    Loss_Train:  [[ 12.72491795]]    Loss_Validation:  [[ 11.1753855]]\n",
      "Loop  4586 :    Loss_Train:  [[ 12.72491697]]    Loss_Validation:  [[ 11.17539387]]\n",
      "Loop  4587 :    Loss_Train:  [[ 12.724916]]    Loss_Validation:  [[ 11.17540224]]\n",
      "Loop  4588 :    Loss_Train:  [[ 12.72491502]]    Loss_Validation:  [[ 11.17541059]]\n",
      "Loop  4589 :    Loss_Train:  [[ 12.72491405]]    Loss_Validation:  [[ 11.17541895]]\n",
      "Loop  4590 :    Loss_Train:  [[ 12.72491308]]    Loss_Validation:  [[ 11.1754273]]\n",
      "Loop  4591 :    Loss_Train:  [[ 12.72491211]]    Loss_Validation:  [[ 11.17543564]]\n",
      "Loop  4592 :    Loss_Train:  [[ 12.72491114]]    Loss_Validation:  [[ 11.17544398]]\n",
      "Loop  4593 :    Loss_Train:  [[ 12.72491017]]    Loss_Validation:  [[ 11.17545231]]\n",
      "Loop  4594 :    Loss_Train:  [[ 12.7249092]]    Loss_Validation:  [[ 11.17546063]]\n",
      "Loop  4595 :    Loss_Train:  [[ 12.72490824]]    Loss_Validation:  [[ 11.17546895]]\n",
      "Loop  4596 :    Loss_Train:  [[ 12.72490728]]    Loss_Validation:  [[ 11.17547727]]\n",
      "Loop  4597 :    Loss_Train:  [[ 12.72490632]]    Loss_Validation:  [[ 11.17548558]]\n",
      "Loop  4598 :    Loss_Train:  [[ 12.72490536]]    Loss_Validation:  [[ 11.17549388]]\n",
      "Loop  4599 :    Loss_Train:  [[ 12.7249044]]    Loss_Validation:  [[ 11.17550218]]\n",
      "Loop  4600 :    Loss_Train:  [[ 12.72490344]]    Loss_Validation:  [[ 11.17551047]]\n",
      "Loop  4601 :    Loss_Train:  [[ 12.72490249]]    Loss_Validation:  [[ 11.17551876]]\n",
      "Loop  4602 :    Loss_Train:  [[ 12.72490153]]    Loss_Validation:  [[ 11.17552704]]\n",
      "Loop  4603 :    Loss_Train:  [[ 12.72490058]]    Loss_Validation:  [[ 11.17553532]]\n",
      "Loop  4604 :    Loss_Train:  [[ 12.72489963]]    Loss_Validation:  [[ 11.17554359]]\n",
      "Loop  4605 :    Loss_Train:  [[ 12.72489868]]    Loss_Validation:  [[ 11.17555186]]\n",
      "Loop  4606 :    Loss_Train:  [[ 12.72489774]]    Loss_Validation:  [[ 11.17556011]]\n",
      "Loop  4607 :    Loss_Train:  [[ 12.72489679]]    Loss_Validation:  [[ 11.17556837]]\n",
      "Loop  4608 :    Loss_Train:  [[ 12.72489585]]    Loss_Validation:  [[ 11.17557662]]\n",
      "Loop  4609 :    Loss_Train:  [[ 12.7248949]]    Loss_Validation:  [[ 11.17558486]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  4610 :    Loss_Train:  [[ 12.72489396]]    Loss_Validation:  [[ 11.1755931]]\n",
      "Loop  4611 :    Loss_Train:  [[ 12.72489302]]    Loss_Validation:  [[ 11.17560133]]\n",
      "Loop  4612 :    Loss_Train:  [[ 12.72489209]]    Loss_Validation:  [[ 11.17560956]]\n",
      "Loop  4613 :    Loss_Train:  [[ 12.72489115]]    Loss_Validation:  [[ 11.17561778]]\n",
      "Loop  4614 :    Loss_Train:  [[ 12.72489022]]    Loss_Validation:  [[ 11.17562599]]\n",
      "Loop  4615 :    Loss_Train:  [[ 12.72488928]]    Loss_Validation:  [[ 11.1756342]]\n",
      "Loop  4616 :    Loss_Train:  [[ 12.72488835]]    Loss_Validation:  [[ 11.17564241]]\n",
      "Loop  4617 :    Loss_Train:  [[ 12.72488742]]    Loss_Validation:  [[ 11.17565061]]\n",
      "Loop  4618 :    Loss_Train:  [[ 12.72488649]]    Loss_Validation:  [[ 11.1756588]]\n",
      "Loop  4619 :    Loss_Train:  [[ 12.72488556]]    Loss_Validation:  [[ 11.17566699]]\n",
      "Loop  4620 :    Loss_Train:  [[ 12.72488464]]    Loss_Validation:  [[ 11.17567517]]\n",
      "Loop  4621 :    Loss_Train:  [[ 12.72488372]]    Loss_Validation:  [[ 11.17568335]]\n",
      "Loop  4622 :    Loss_Train:  [[ 12.72488279]]    Loss_Validation:  [[ 11.17569152]]\n",
      "Loop  4623 :    Loss_Train:  [[ 12.72488187]]    Loss_Validation:  [[ 11.17569969]]\n",
      "Loop  4624 :    Loss_Train:  [[ 12.72488095]]    Loss_Validation:  [[ 11.17570785]]\n",
      "Loop  4625 :    Loss_Train:  [[ 12.72488003]]    Loss_Validation:  [[ 11.175716]]\n",
      "Loop  4626 :    Loss_Train:  [[ 12.72487912]]    Loss_Validation:  [[ 11.17572415]]\n",
      "Loop  4627 :    Loss_Train:  [[ 12.7248782]]    Loss_Validation:  [[ 11.1757323]]\n",
      "Loop  4628 :    Loss_Train:  [[ 12.72487729]]    Loss_Validation:  [[ 11.17574044]]\n",
      "Loop  4629 :    Loss_Train:  [[ 12.72487638]]    Loss_Validation:  [[ 11.17574857]]\n",
      "Loop  4630 :    Loss_Train:  [[ 12.72487547]]    Loss_Validation:  [[ 11.1757567]]\n",
      "Loop  4631 :    Loss_Train:  [[ 12.72487456]]    Loss_Validation:  [[ 11.17576482]]\n",
      "Loop  4632 :    Loss_Train:  [[ 12.72487365]]    Loss_Validation:  [[ 11.17577294]]\n",
      "Loop  4633 :    Loss_Train:  [[ 12.72487274]]    Loss_Validation:  [[ 11.17578105]]\n",
      "Loop  4634 :    Loss_Train:  [[ 12.72487184]]    Loss_Validation:  [[ 11.17578916]]\n",
      "Loop  4635 :    Loss_Train:  [[ 12.72487094]]    Loss_Validation:  [[ 11.17579726]]\n",
      "Loop  4636 :    Loss_Train:  [[ 12.72487003]]    Loss_Validation:  [[ 11.17580535]]\n",
      "Loop  4637 :    Loss_Train:  [[ 12.72486913]]    Loss_Validation:  [[ 11.17581344]]\n",
      "Loop  4638 :    Loss_Train:  [[ 12.72486823]]    Loss_Validation:  [[ 11.17582153]]\n",
      "Loop  4639 :    Loss_Train:  [[ 12.72486734]]    Loss_Validation:  [[ 11.17582961]]\n",
      "Loop  4640 :    Loss_Train:  [[ 12.72486644]]    Loss_Validation:  [[ 11.17583768]]\n",
      "Loop  4641 :    Loss_Train:  [[ 12.72486555]]    Loss_Validation:  [[ 11.17584575]]\n",
      "Loop  4642 :    Loss_Train:  [[ 12.72486465]]    Loss_Validation:  [[ 11.17585381]]\n",
      "Loop  4643 :    Loss_Train:  [[ 12.72486376]]    Loss_Validation:  [[ 11.17586187]]\n",
      "Loop  4644 :    Loss_Train:  [[ 12.72486287]]    Loss_Validation:  [[ 11.17586992]]\n",
      "Loop  4645 :    Loss_Train:  [[ 12.72486198]]    Loss_Validation:  [[ 11.17587797]]\n",
      "Loop  4646 :    Loss_Train:  [[ 12.7248611]]    Loss_Validation:  [[ 11.17588601]]\n",
      "Loop  4647 :    Loss_Train:  [[ 12.72486021]]    Loss_Validation:  [[ 11.17589405]]\n",
      "Loop  4648 :    Loss_Train:  [[ 12.72485933]]    Loss_Validation:  [[ 11.17590208]]\n",
      "Loop  4649 :    Loss_Train:  [[ 12.72485844]]    Loss_Validation:  [[ 11.17591011]]\n",
      "Loop  4650 :    Loss_Train:  [[ 12.72485756]]    Loss_Validation:  [[ 11.17591813]]\n",
      "Loop  4651 :    Loss_Train:  [[ 12.72485668]]    Loss_Validation:  [[ 11.17592614]]\n",
      "Loop  4652 :    Loss_Train:  [[ 12.7248558]]    Loss_Validation:  [[ 11.17593415]]\n",
      "Loop  4653 :    Loss_Train:  [[ 12.72485493]]    Loss_Validation:  [[ 11.17594215]]\n",
      "Loop  4654 :    Loss_Train:  [[ 12.72485405]]    Loss_Validation:  [[ 11.17595015]]\n",
      "Loop  4655 :    Loss_Train:  [[ 12.72485318]]    Loss_Validation:  [[ 11.17595815]]\n",
      "Loop  4656 :    Loss_Train:  [[ 12.72485231]]    Loss_Validation:  [[ 11.17596613]]\n",
      "Loop  4657 :    Loss_Train:  [[ 12.72485143]]    Loss_Validation:  [[ 11.17597412]]\n",
      "Loop  4658 :    Loss_Train:  [[ 12.72485056]]    Loss_Validation:  [[ 11.17598209]]\n",
      "Loop  4659 :    Loss_Train:  [[ 12.7248497]]    Loss_Validation:  [[ 11.17599007]]\n",
      "Loop  4660 :    Loss_Train:  [[ 12.72484883]]    Loss_Validation:  [[ 11.17599803]]\n",
      "Loop  4661 :    Loss_Train:  [[ 12.72484796]]    Loss_Validation:  [[ 11.17600599]]\n",
      "Loop  4662 :    Loss_Train:  [[ 12.7248471]]    Loss_Validation:  [[ 11.17601395]]\n",
      "Loop  4663 :    Loss_Train:  [[ 12.72484624]]    Loss_Validation:  [[ 11.1760219]]\n",
      "Loop  4664 :    Loss_Train:  [[ 12.72484537]]    Loss_Validation:  [[ 11.17602985]]\n",
      "Loop  4665 :    Loss_Train:  [[ 12.72484451]]    Loss_Validation:  [[ 11.17603779]]\n",
      "Loop  4666 :    Loss_Train:  [[ 12.72484366]]    Loss_Validation:  [[ 11.17604572]]\n",
      "Loop  4667 :    Loss_Train:  [[ 12.7248428]]    Loss_Validation:  [[ 11.17605365]]\n",
      "Loop  4668 :    Loss_Train:  [[ 12.72484194]]    Loss_Validation:  [[ 11.17606157]]\n",
      "Loop  4669 :    Loss_Train:  [[ 12.72484109]]    Loss_Validation:  [[ 11.17606949]]\n",
      "Loop  4670 :    Loss_Train:  [[ 12.72484024]]    Loss_Validation:  [[ 11.17607741]]\n",
      "Loop  4671 :    Loss_Train:  [[ 12.72483938]]    Loss_Validation:  [[ 11.17608531]]\n",
      "Loop  4672 :    Loss_Train:  [[ 12.72483853]]    Loss_Validation:  [[ 11.17609322]]\n",
      "Loop  4673 :    Loss_Train:  [[ 12.72483769]]    Loss_Validation:  [[ 11.17610112]]\n",
      "Loop  4674 :    Loss_Train:  [[ 12.72483684]]    Loss_Validation:  [[ 11.17610901]]\n",
      "Loop  4675 :    Loss_Train:  [[ 12.72483599]]    Loss_Validation:  [[ 11.17611689]]\n",
      "Loop  4676 :    Loss_Train:  [[ 12.72483515]]    Loss_Validation:  [[ 11.17612478]]\n",
      "Loop  4677 :    Loss_Train:  [[ 12.7248343]]    Loss_Validation:  [[ 11.17613265]]\n",
      "Loop  4678 :    Loss_Train:  [[ 12.72483346]]    Loss_Validation:  [[ 11.17614052]]\n",
      "Loop  4679 :    Loss_Train:  [[ 12.72483262]]    Loss_Validation:  [[ 11.17614839]]\n",
      "Loop  4680 :    Loss_Train:  [[ 12.72483178]]    Loss_Validation:  [[ 11.17615625]]\n",
      "Loop  4681 :    Loss_Train:  [[ 12.72483094]]    Loss_Validation:  [[ 11.17616411]]\n",
      "Loop  4682 :    Loss_Train:  [[ 12.72483011]]    Loss_Validation:  [[ 11.17617196]]\n",
      "Loop  4683 :    Loss_Train:  [[ 12.72482927]]    Loss_Validation:  [[ 11.1761798]]\n",
      "Loop  4684 :    Loss_Train:  [[ 12.72482844]]    Loss_Validation:  [[ 11.17618764]]\n",
      "Loop  4685 :    Loss_Train:  [[ 12.72482761]]    Loss_Validation:  [[ 11.17619548]]\n",
      "Loop  4686 :    Loss_Train:  [[ 12.72482678]]    Loss_Validation:  [[ 11.1762033]]\n",
      "Loop  4687 :    Loss_Train:  [[ 12.72482595]]    Loss_Validation:  [[ 11.17621113]]\n",
      "Loop  4688 :    Loss_Train:  [[ 12.72482512]]    Loss_Validation:  [[ 11.17621895]]\n",
      "Loop  4689 :    Loss_Train:  [[ 12.72482429]]    Loss_Validation:  [[ 11.17622676]]\n",
      "Loop  4690 :    Loss_Train:  [[ 12.72482347]]    Loss_Validation:  [[ 11.17623457]]\n",
      "Loop  4691 :    Loss_Train:  [[ 12.72482264]]    Loss_Validation:  [[ 11.17624237]]\n",
      "Loop  4692 :    Loss_Train:  [[ 12.72482182]]    Loss_Validation:  [[ 11.17625017]]\n",
      "Loop  4693 :    Loss_Train:  [[ 12.724821]]    Loss_Validation:  [[ 11.17625796]]\n",
      "Loop  4694 :    Loss_Train:  [[ 12.72482018]]    Loss_Validation:  [[ 11.17626575]]\n",
      "Loop  4695 :    Loss_Train:  [[ 12.72481936]]    Loss_Validation:  [[ 11.17627353]]\n",
      "Loop  4696 :    Loss_Train:  [[ 12.72481854]]    Loss_Validation:  [[ 11.17628131]]\n",
      "Loop  4697 :    Loss_Train:  [[ 12.72481772]]    Loss_Validation:  [[ 11.17628908]]\n",
      "Loop  4698 :    Loss_Train:  [[ 12.72481691]]    Loss_Validation:  [[ 11.17629685]]\n",
      "Loop  4699 :    Loss_Train:  [[ 12.7248161]]    Loss_Validation:  [[ 11.17630461]]\n",
      "Loop  4700 :    Loss_Train:  [[ 12.72481528]]    Loss_Validation:  [[ 11.17631236]]\n",
      "Loop  4701 :    Loss_Train:  [[ 12.72481447]]    Loss_Validation:  [[ 11.17632011]]\n",
      "Loop  4702 :    Loss_Train:  [[ 12.72481366]]    Loss_Validation:  [[ 11.17632786]]\n",
      "Loop  4703 :    Loss_Train:  [[ 12.72481285]]    Loss_Validation:  [[ 11.1763356]]\n",
      "Loop  4704 :    Loss_Train:  [[ 12.72481205]]    Loss_Validation:  [[ 11.17634333]]\n",
      "Loop  4705 :    Loss_Train:  [[ 12.72481124]]    Loss_Validation:  [[ 11.17635106]]\n",
      "Loop  4706 :    Loss_Train:  [[ 12.72481044]]    Loss_Validation:  [[ 11.17635879]]\n",
      "Loop  4707 :    Loss_Train:  [[ 12.72480964]]    Loss_Validation:  [[ 11.17636651]]\n",
      "Loop  4708 :    Loss_Train:  [[ 12.72480883]]    Loss_Validation:  [[ 11.17637422]]\n",
      "Loop  4709 :    Loss_Train:  [[ 12.72480803]]    Loss_Validation:  [[ 11.17638193]]\n",
      "Loop  4710 :    Loss_Train:  [[ 12.72480723]]    Loss_Validation:  [[ 11.17638964]]\n",
      "Loop  4711 :    Loss_Train:  [[ 12.72480644]]    Loss_Validation:  [[ 11.17639734]]\n",
      "Loop  4712 :    Loss_Train:  [[ 12.72480564]]    Loss_Validation:  [[ 11.17640503]]\n",
      "Loop  4713 :    Loss_Train:  [[ 12.72480484]]    Loss_Validation:  [[ 11.17641272]]\n",
      "Loop  4714 :    Loss_Train:  [[ 12.72480405]]    Loss_Validation:  [[ 11.1764204]]\n",
      "Loop  4715 :    Loss_Train:  [[ 12.72480326]]    Loss_Validation:  [[ 11.17642808]]\n",
      "Loop  4716 :    Loss_Train:  [[ 12.72480247]]    Loss_Validation:  [[ 11.17643575]]\n",
      "Loop  4717 :    Loss_Train:  [[ 12.72480168]]    Loss_Validation:  [[ 11.17644342]]\n",
      "Loop  4718 :    Loss_Train:  [[ 12.72480089]]    Loss_Validation:  [[ 11.17645108]]\n",
      "Loop  4719 :    Loss_Train:  [[ 12.7248001]]    Loss_Validation:  [[ 11.17645874]]\n",
      "Loop  4720 :    Loss_Train:  [[ 12.72479931]]    Loss_Validation:  [[ 11.17646639]]\n",
      "Loop  4721 :    Loss_Train:  [[ 12.72479853]]    Loss_Validation:  [[ 11.17647404]]\n",
      "Loop  4722 :    Loss_Train:  [[ 12.72479775]]    Loss_Validation:  [[ 11.17648168]]\n",
      "Loop  4723 :    Loss_Train:  [[ 12.72479696]]    Loss_Validation:  [[ 11.17648932]]\n",
      "Loop  4724 :    Loss_Train:  [[ 12.72479618]]    Loss_Validation:  [[ 11.17649695]]\n",
      "Loop  4725 :    Loss_Train:  [[ 12.7247954]]    Loss_Validation:  [[ 11.17650458]]\n",
      "Loop  4726 :    Loss_Train:  [[ 12.72479462]]    Loss_Validation:  [[ 11.1765122]]\n",
      "Loop  4727 :    Loss_Train:  [[ 12.72479385]]    Loss_Validation:  [[ 11.17651982]]\n",
      "Loop  4728 :    Loss_Train:  [[ 12.72479307]]    Loss_Validation:  [[ 11.17652743]]\n",
      "Loop  4729 :    Loss_Train:  [[ 12.7247923]]    Loss_Validation:  [[ 11.17653504]]\n",
      "Loop  4730 :    Loss_Train:  [[ 12.72479152]]    Loss_Validation:  [[ 11.17654264]]\n",
      "Loop  4731 :    Loss_Train:  [[ 12.72479075]]    Loss_Validation:  [[ 11.17655023]]\n",
      "Loop  4732 :    Loss_Train:  [[ 12.72478998]]    Loss_Validation:  [[ 11.17655783]]\n",
      "Loop  4733 :    Loss_Train:  [[ 12.72478921]]    Loss_Validation:  [[ 11.17656541]]\n",
      "Loop  4734 :    Loss_Train:  [[ 12.72478844]]    Loss_Validation:  [[ 11.17657299]]\n",
      "Loop  4735 :    Loss_Train:  [[ 12.72478768]]    Loss_Validation:  [[ 11.17658057]]\n",
      "Loop  4736 :    Loss_Train:  [[ 12.72478691]]    Loss_Validation:  [[ 11.17658814]]\n",
      "Loop  4737 :    Loss_Train:  [[ 12.72478615]]    Loss_Validation:  [[ 11.1765957]]\n",
      "Loop  4738 :    Loss_Train:  [[ 12.72478538]]    Loss_Validation:  [[ 11.17660327]]\n",
      "Loop  4739 :    Loss_Train:  [[ 12.72478462]]    Loss_Validation:  [[ 11.17661082]]\n",
      "Loop  4740 :    Loss_Train:  [[ 12.72478386]]    Loss_Validation:  [[ 11.17661837]]\n",
      "Loop  4741 :    Loss_Train:  [[ 12.7247831]]    Loss_Validation:  [[ 11.17662592]]\n",
      "Loop  4742 :    Loss_Train:  [[ 12.72478234]]    Loss_Validation:  [[ 11.17663346]]\n",
      "Loop  4743 :    Loss_Train:  [[ 12.72478158]]    Loss_Validation:  [[ 11.17664099]]\n",
      "Loop  4744 :    Loss_Train:  [[ 12.72478083]]    Loss_Validation:  [[ 11.17664852]]\n",
      "Loop  4745 :    Loss_Train:  [[ 12.72478007]]    Loss_Validation:  [[ 11.17665605]]\n",
      "Loop  4746 :    Loss_Train:  [[ 12.72477932]]    Loss_Validation:  [[ 11.17666357]]\n",
      "Loop  4747 :    Loss_Train:  [[ 12.72477857]]    Loss_Validation:  [[ 11.17667108]]\n",
      "Loop  4748 :    Loss_Train:  [[ 12.72477782]]    Loss_Validation:  [[ 11.17667859]]\n",
      "Loop  4749 :    Loss_Train:  [[ 12.72477707]]    Loss_Validation:  [[ 11.1766861]]\n",
      "Loop  4750 :    Loss_Train:  [[ 12.72477632]]    Loss_Validation:  [[ 11.1766936]]\n",
      "Loop  4751 :    Loss_Train:  [[ 12.72477557]]    Loss_Validation:  [[ 11.17670109]]\n",
      "Loop  4752 :    Loss_Train:  [[ 12.72477482]]    Loss_Validation:  [[ 11.17670858]]\n",
      "Loop  4753 :    Loss_Train:  [[ 12.72477408]]    Loss_Validation:  [[ 11.17671607]]\n",
      "Loop  4754 :    Loss_Train:  [[ 12.72477334]]    Loss_Validation:  [[ 11.17672355]]\n",
      "Loop  4755 :    Loss_Train:  [[ 12.72477259]]    Loss_Validation:  [[ 11.17673102]]\n",
      "Loop  4756 :    Loss_Train:  [[ 12.72477185]]    Loss_Validation:  [[ 11.17673849]]\n",
      "Loop  4757 :    Loss_Train:  [[ 12.72477111]]    Loss_Validation:  [[ 11.17674596]]\n",
      "Loop  4758 :    Loss_Train:  [[ 12.72477037]]    Loss_Validation:  [[ 11.17675342]]\n",
      "Loop  4759 :    Loss_Train:  [[ 12.72476963]]    Loss_Validation:  [[ 11.17676087]]\n",
      "Loop  4760 :    Loss_Train:  [[ 12.7247689]]    Loss_Validation:  [[ 11.17676832]]\n",
      "Loop  4761 :    Loss_Train:  [[ 12.72476816]]    Loss_Validation:  [[ 11.17677577]]\n",
      "Loop  4762 :    Loss_Train:  [[ 12.72476743]]    Loss_Validation:  [[ 11.17678321]]\n",
      "Loop  4763 :    Loss_Train:  [[ 12.7247667]]    Loss_Validation:  [[ 11.17679064]]\n",
      "Loop  4764 :    Loss_Train:  [[ 12.72476596]]    Loss_Validation:  [[ 11.17679807]]\n",
      "Loop  4765 :    Loss_Train:  [[ 12.72476523]]    Loss_Validation:  [[ 11.17680549]]\n",
      "Loop  4766 :    Loss_Train:  [[ 12.7247645]]    Loss_Validation:  [[ 11.17681291]]\n",
      "Loop  4767 :    Loss_Train:  [[ 12.72476378]]    Loss_Validation:  [[ 11.17682033]]\n",
      "Loop  4768 :    Loss_Train:  [[ 12.72476305]]    Loss_Validation:  [[ 11.17682774]]\n",
      "Loop  4769 :    Loss_Train:  [[ 12.72476232]]    Loss_Validation:  [[ 11.17683514]]\n",
      "Loop  4770 :    Loss_Train:  [[ 12.7247616]]    Loss_Validation:  [[ 11.17684254]]\n",
      "Loop  4771 :    Loss_Train:  [[ 12.72476088]]    Loss_Validation:  [[ 11.17684994]]\n",
      "Loop  4772 :    Loss_Train:  [[ 12.72476015]]    Loss_Validation:  [[ 11.17685733]]\n",
      "Loop  4773 :    Loss_Train:  [[ 12.72475943]]    Loss_Validation:  [[ 11.17686471]]\n",
      "Loop  4774 :    Loss_Train:  [[ 12.72475871]]    Loss_Validation:  [[ 11.17687209]]\n",
      "Loop  4775 :    Loss_Train:  [[ 12.72475799]]    Loss_Validation:  [[ 11.17687947]]\n",
      "Loop  4776 :    Loss_Train:  [[ 12.72475728]]    Loss_Validation:  [[ 11.17688684]]\n",
      "Loop  4777 :    Loss_Train:  [[ 12.72475656]]    Loss_Validation:  [[ 11.1768942]]\n",
      "Loop  4778 :    Loss_Train:  [[ 12.72475585]]    Loss_Validation:  [[ 11.17690156]]\n",
      "Loop  4779 :    Loss_Train:  [[ 12.72475513]]    Loss_Validation:  [[ 11.17690892]]\n",
      "Loop  4780 :    Loss_Train:  [[ 12.72475442]]    Loss_Validation:  [[ 11.17691627]]\n",
      "Loop  4781 :    Loss_Train:  [[ 12.72475371]]    Loss_Validation:  [[ 11.17692361]]\n",
      "Loop  4782 :    Loss_Train:  [[ 12.724753]]    Loss_Validation:  [[ 11.17693095]]\n",
      "Loop  4783 :    Loss_Train:  [[ 12.72475229]]    Loss_Validation:  [[ 11.17693829]]\n",
      "Loop  4784 :    Loss_Train:  [[ 12.72475158]]    Loss_Validation:  [[ 11.17694562]]\n",
      "Loop  4785 :    Loss_Train:  [[ 12.72475087]]    Loss_Validation:  [[ 11.17695294]]\n",
      "Loop  4786 :    Loss_Train:  [[ 12.72475017]]    Loss_Validation:  [[ 11.17696026]]\n",
      "Loop  4787 :    Loss_Train:  [[ 12.72474946]]    Loss_Validation:  [[ 11.17696758]]\n",
      "Loop  4788 :    Loss_Train:  [[ 12.72474876]]    Loss_Validation:  [[ 11.17697489]]\n",
      "Loop  4789 :    Loss_Train:  [[ 12.72474805]]    Loss_Validation:  [[ 11.1769822]]\n",
      "Loop  4790 :    Loss_Train:  [[ 12.72474735]]    Loss_Validation:  [[ 11.1769895]]\n",
      "Loop  4791 :    Loss_Train:  [[ 12.72474665]]    Loss_Validation:  [[ 11.17699679]]\n",
      "Loop  4792 :    Loss_Train:  [[ 12.72474595]]    Loss_Validation:  [[ 11.17700408]]\n",
      "Loop  4793 :    Loss_Train:  [[ 12.72474526]]    Loss_Validation:  [[ 11.17701137]]\n",
      "Loop  4794 :    Loss_Train:  [[ 12.72474456]]    Loss_Validation:  [[ 11.17701865]]\n",
      "Loop  4795 :    Loss_Train:  [[ 12.72474386]]    Loss_Validation:  [[ 11.17702593]]\n",
      "Loop  4796 :    Loss_Train:  [[ 12.72474317]]    Loss_Validation:  [[ 11.1770332]]\n",
      "Loop  4797 :    Loss_Train:  [[ 12.72474248]]    Loss_Validation:  [[ 11.17704047]]\n",
      "Loop  4798 :    Loss_Train:  [[ 12.72474178]]    Loss_Validation:  [[ 11.17704773]]\n",
      "Loop  4799 :    Loss_Train:  [[ 12.72474109]]    Loss_Validation:  [[ 11.17705498]]\n",
      "Loop  4800 :    Loss_Train:  [[ 12.7247404]]    Loss_Validation:  [[ 11.17706224]]\n",
      "Loop  4801 :    Loss_Train:  [[ 12.72473971]]    Loss_Validation:  [[ 11.17706948]]\n",
      "Loop  4802 :    Loss_Train:  [[ 12.72473903]]    Loss_Validation:  [[ 11.17707673]]\n",
      "Loop  4803 :    Loss_Train:  [[ 12.72473834]]    Loss_Validation:  [[ 11.17708396]]\n",
      "Loop  4804 :    Loss_Train:  [[ 12.72473765]]    Loss_Validation:  [[ 11.1770912]]\n",
      "Loop  4805 :    Loss_Train:  [[ 12.72473697]]    Loss_Validation:  [[ 11.17709842]]\n",
      "Loop  4806 :    Loss_Train:  [[ 12.72473629]]    Loss_Validation:  [[ 11.17710565]]\n",
      "Loop  4807 :    Loss_Train:  [[ 12.7247356]]    Loss_Validation:  [[ 11.17711286]]\n",
      "Loop  4808 :    Loss_Train:  [[ 12.72473492]]    Loss_Validation:  [[ 11.17712008]]\n",
      "Loop  4809 :    Loss_Train:  [[ 12.72473424]]    Loss_Validation:  [[ 11.17712728]]\n",
      "Loop  4810 :    Loss_Train:  [[ 12.72473356]]    Loss_Validation:  [[ 11.17713449]]\n",
      "Loop  4811 :    Loss_Train:  [[ 12.72473289]]    Loss_Validation:  [[ 11.17714169]]\n",
      "Loop  4812 :    Loss_Train:  [[ 12.72473221]]    Loss_Validation:  [[ 11.17714888]]\n",
      "Loop  4813 :    Loss_Train:  [[ 12.72473153]]    Loss_Validation:  [[ 11.17715607]]\n",
      "Loop  4814 :    Loss_Train:  [[ 12.72473086]]    Loss_Validation:  [[ 11.17716325]]\n",
      "Loop  4815 :    Loss_Train:  [[ 12.72473019]]    Loss_Validation:  [[ 11.17717043]]\n",
      "Loop  4816 :    Loss_Train:  [[ 12.72472951]]    Loss_Validation:  [[ 11.17717761]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  4817 :    Loss_Train:  [[ 12.72472884]]    Loss_Validation:  [[ 11.17718477]]\n",
      "Loop  4818 :    Loss_Train:  [[ 12.72472817]]    Loss_Validation:  [[ 11.17719194]]\n",
      "Loop  4819 :    Loss_Train:  [[ 12.7247275]]    Loss_Validation:  [[ 11.1771991]]\n",
      "Loop  4820 :    Loss_Train:  [[ 12.72472684]]    Loss_Validation:  [[ 11.17720625]]\n",
      "Loop  4821 :    Loss_Train:  [[ 12.72472617]]    Loss_Validation:  [[ 11.1772134]]\n",
      "Loop  4822 :    Loss_Train:  [[ 12.7247255]]    Loss_Validation:  [[ 11.17722055]]\n",
      "Loop  4823 :    Loss_Train:  [[ 12.72472484]]    Loss_Validation:  [[ 11.17722769]]\n",
      "Loop  4824 :    Loss_Train:  [[ 12.72472418]]    Loss_Validation:  [[ 11.17723482]]\n",
      "Loop  4825 :    Loss_Train:  [[ 12.72472351]]    Loss_Validation:  [[ 11.17724196]]\n",
      "Loop  4826 :    Loss_Train:  [[ 12.72472285]]    Loss_Validation:  [[ 11.17724908]]\n",
      "Loop  4827 :    Loss_Train:  [[ 12.72472219]]    Loss_Validation:  [[ 11.1772562]]\n",
      "Loop  4828 :    Loss_Train:  [[ 12.72472153]]    Loss_Validation:  [[ 11.17726332]]\n",
      "Loop  4829 :    Loss_Train:  [[ 12.72472087]]    Loss_Validation:  [[ 11.17727043]]\n",
      "Loop  4830 :    Loss_Train:  [[ 12.72472022]]    Loss_Validation:  [[ 11.17727754]]\n",
      "Loop  4831 :    Loss_Train:  [[ 12.72471956]]    Loss_Validation:  [[ 11.17728464]]\n",
      "Loop  4832 :    Loss_Train:  [[ 12.72471891]]    Loss_Validation:  [[ 11.17729174]]\n",
      "Loop  4833 :    Loss_Train:  [[ 12.72471825]]    Loss_Validation:  [[ 11.17729883]]\n",
      "Loop  4834 :    Loss_Train:  [[ 12.7247176]]    Loss_Validation:  [[ 11.17730592]]\n",
      "Loop  4835 :    Loss_Train:  [[ 12.72471695]]    Loss_Validation:  [[ 11.177313]]\n",
      "Loop  4836 :    Loss_Train:  [[ 12.7247163]]    Loss_Validation:  [[ 11.17732008]]\n",
      "Loop  4837 :    Loss_Train:  [[ 12.72471565]]    Loss_Validation:  [[ 11.17732715]]\n",
      "Loop  4838 :    Loss_Train:  [[ 12.724715]]    Loss_Validation:  [[ 11.17733422]]\n",
      "Loop  4839 :    Loss_Train:  [[ 12.72471435]]    Loss_Validation:  [[ 11.17734128]]\n",
      "Loop  4840 :    Loss_Train:  [[ 12.7247137]]    Loss_Validation:  [[ 11.17734834]]\n",
      "Loop  4841 :    Loss_Train:  [[ 12.72471306]]    Loss_Validation:  [[ 11.1773554]]\n",
      "Loop  4842 :    Loss_Train:  [[ 12.72471241]]    Loss_Validation:  [[ 11.17736245]]\n",
      "Loop  4843 :    Loss_Train:  [[ 12.72471177]]    Loss_Validation:  [[ 11.17736949]]\n",
      "Loop  4844 :    Loss_Train:  [[ 12.72471113]]    Loss_Validation:  [[ 11.17737653]]\n",
      "Loop  4845 :    Loss_Train:  [[ 12.72471049]]    Loss_Validation:  [[ 11.17738357]]\n",
      "Loop  4846 :    Loss_Train:  [[ 12.72470985]]    Loss_Validation:  [[ 11.1773906]]\n",
      "Loop  4847 :    Loss_Train:  [[ 12.72470921]]    Loss_Validation:  [[ 11.17739762]]\n",
      "Loop  4848 :    Loss_Train:  [[ 12.72470857]]    Loss_Validation:  [[ 11.17740464]]\n",
      "Loop  4849 :    Loss_Train:  [[ 12.72470793]]    Loss_Validation:  [[ 11.17741166]]\n",
      "Loop  4850 :    Loss_Train:  [[ 12.7247073]]    Loss_Validation:  [[ 11.17741867]]\n",
      "Loop  4851 :    Loss_Train:  [[ 12.72470666]]    Loss_Validation:  [[ 11.17742568]]\n",
      "Loop  4852 :    Loss_Train:  [[ 12.72470603]]    Loss_Validation:  [[ 11.17743268]]\n",
      "Loop  4853 :    Loss_Train:  [[ 12.7247054]]    Loss_Validation:  [[ 11.17743968]]\n",
      "Loop  4854 :    Loss_Train:  [[ 12.72470476]]    Loss_Validation:  [[ 11.17744667]]\n",
      "Loop  4855 :    Loss_Train:  [[ 12.72470413]]    Loss_Validation:  [[ 11.17745366]]\n",
      "Loop  4856 :    Loss_Train:  [[ 12.7247035]]    Loss_Validation:  [[ 11.17746064]]\n",
      "Loop  4857 :    Loss_Train:  [[ 12.72470287]]    Loss_Validation:  [[ 11.17746762]]\n",
      "Loop  4858 :    Loss_Train:  [[ 12.72470225]]    Loss_Validation:  [[ 11.17747459]]\n",
      "Loop  4859 :    Loss_Train:  [[ 12.72470162]]    Loss_Validation:  [[ 11.17748156]]\n",
      "Loop  4860 :    Loss_Train:  [[ 12.72470099]]    Loss_Validation:  [[ 11.17748853]]\n",
      "Loop  4861 :    Loss_Train:  [[ 12.72470037]]    Loss_Validation:  [[ 11.17749549]]\n",
      "Loop  4862 :    Loss_Train:  [[ 12.72469974]]    Loss_Validation:  [[ 11.17750244]]\n",
      "Loop  4863 :    Loss_Train:  [[ 12.72469912]]    Loss_Validation:  [[ 11.17750939]]\n",
      "Loop  4864 :    Loss_Train:  [[ 12.7246985]]    Loss_Validation:  [[ 11.17751634]]\n",
      "Loop  4865 :    Loss_Train:  [[ 12.72469788]]    Loss_Validation:  [[ 11.17752328]]\n",
      "Loop  4866 :    Loss_Train:  [[ 12.72469726]]    Loss_Validation:  [[ 11.17753022]]\n",
      "Loop  4867 :    Loss_Train:  [[ 12.72469664]]    Loss_Validation:  [[ 11.17753715]]\n",
      "Loop  4868 :    Loss_Train:  [[ 12.72469602]]    Loss_Validation:  [[ 11.17754407]]\n",
      "Loop  4869 :    Loss_Train:  [[ 12.72469541]]    Loss_Validation:  [[ 11.177551]]\n",
      "Loop  4870 :    Loss_Train:  [[ 12.72469479]]    Loss_Validation:  [[ 11.17755791]]\n",
      "Loop  4871 :    Loss_Train:  [[ 12.72469418]]    Loss_Validation:  [[ 11.17756483]]\n",
      "Loop  4872 :    Loss_Train:  [[ 12.72469356]]    Loss_Validation:  [[ 11.17757173]]\n",
      "Loop  4873 :    Loss_Train:  [[ 12.72469295]]    Loss_Validation:  [[ 11.17757864]]\n",
      "Loop  4874 :    Loss_Train:  [[ 12.72469234]]    Loss_Validation:  [[ 11.17758554]]\n",
      "Loop  4875 :    Loss_Train:  [[ 12.72469173]]    Loss_Validation:  [[ 11.17759243]]\n",
      "Loop  4876 :    Loss_Train:  [[ 12.72469112]]    Loss_Validation:  [[ 11.17759932]]\n",
      "Loop  4877 :    Loss_Train:  [[ 12.72469051]]    Loss_Validation:  [[ 11.17760621]]\n",
      "Loop  4878 :    Loss_Train:  [[ 12.7246899]]    Loss_Validation:  [[ 11.17761309]]\n",
      "Loop  4879 :    Loss_Train:  [[ 12.72468929]]    Loss_Validation:  [[ 11.17761996]]\n",
      "Loop  4880 :    Loss_Train:  [[ 12.72468869]]    Loss_Validation:  [[ 11.17762683]]\n",
      "Loop  4881 :    Loss_Train:  [[ 12.72468808]]    Loss_Validation:  [[ 11.1776337]]\n",
      "Loop  4882 :    Loss_Train:  [[ 12.72468748]]    Loss_Validation:  [[ 11.17764056]]\n",
      "Loop  4883 :    Loss_Train:  [[ 12.72468688]]    Loss_Validation:  [[ 11.17764742]]\n",
      "Loop  4884 :    Loss_Train:  [[ 12.72468628]]    Loss_Validation:  [[ 11.17765427]]\n",
      "Loop  4885 :    Loss_Train:  [[ 12.72468568]]    Loss_Validation:  [[ 11.17766112]]\n",
      "Loop  4886 :    Loss_Train:  [[ 12.72468508]]    Loss_Validation:  [[ 11.17766796]]\n",
      "Loop  4887 :    Loss_Train:  [[ 12.72468448]]    Loss_Validation:  [[ 11.1776748]]\n",
      "Loop  4888 :    Loss_Train:  [[ 12.72468388]]    Loss_Validation:  [[ 11.17768163]]\n",
      "Loop  4889 :    Loss_Train:  [[ 12.72468328]]    Loss_Validation:  [[ 11.17768846]]\n",
      "Loop  4890 :    Loss_Train:  [[ 12.72468269]]    Loss_Validation:  [[ 11.17769529]]\n",
      "Loop  4891 :    Loss_Train:  [[ 12.72468209]]    Loss_Validation:  [[ 11.17770211]]\n",
      "Loop  4892 :    Loss_Train:  [[ 12.7246815]]    Loss_Validation:  [[ 11.17770892]]\n",
      "Loop  4893 :    Loss_Train:  [[ 12.7246809]]    Loss_Validation:  [[ 11.17771574]]\n",
      "Loop  4894 :    Loss_Train:  [[ 12.72468031]]    Loss_Validation:  [[ 11.17772254]]\n",
      "Loop  4895 :    Loss_Train:  [[ 12.72467972]]    Loss_Validation:  [[ 11.17772934]]\n",
      "Loop  4896 :    Loss_Train:  [[ 12.72467913]]    Loss_Validation:  [[ 11.17773614]]\n",
      "Loop  4897 :    Loss_Train:  [[ 12.72467854]]    Loss_Validation:  [[ 11.17774293]]\n",
      "Loop  4898 :    Loss_Train:  [[ 12.72467795]]    Loss_Validation:  [[ 11.17774972]]\n",
      "Loop  4899 :    Loss_Train:  [[ 12.72467737]]    Loss_Validation:  [[ 11.1777565]]\n",
      "Loop  4900 :    Loss_Train:  [[ 12.72467678]]    Loss_Validation:  [[ 11.17776328]]\n",
      "Loop  4901 :    Loss_Train:  [[ 12.72467619]]    Loss_Validation:  [[ 11.17777006]]\n",
      "Loop  4902 :    Loss_Train:  [[ 12.72467561]]    Loss_Validation:  [[ 11.17777683]]\n",
      "Loop  4903 :    Loss_Train:  [[ 12.72467503]]    Loss_Validation:  [[ 11.17778359]]\n",
      "Loop  4904 :    Loss_Train:  [[ 12.72467444]]    Loss_Validation:  [[ 11.17779035]]\n",
      "Loop  4905 :    Loss_Train:  [[ 12.72467386]]    Loss_Validation:  [[ 11.17779711]]\n",
      "Loop  4906 :    Loss_Train:  [[ 12.72467328]]    Loss_Validation:  [[ 11.17780386]]\n",
      "Loop  4907 :    Loss_Train:  [[ 12.7246727]]    Loss_Validation:  [[ 11.17781061]]\n",
      "Loop  4908 :    Loss_Train:  [[ 12.72467212]]    Loss_Validation:  [[ 11.17781735]]\n",
      "Loop  4909 :    Loss_Train:  [[ 12.72467154]]    Loss_Validation:  [[ 11.17782409]]\n",
      "Loop  4910 :    Loss_Train:  [[ 12.72467097]]    Loss_Validation:  [[ 11.17783082]]\n",
      "Loop  4911 :    Loss_Train:  [[ 12.72467039]]    Loss_Validation:  [[ 11.17783755]]\n",
      "Loop  4912 :    Loss_Train:  [[ 12.72466982]]    Loss_Validation:  [[ 11.17784427]]\n",
      "Loop  4913 :    Loss_Train:  [[ 12.72466924]]    Loss_Validation:  [[ 11.17785099]]\n",
      "Loop  4914 :    Loss_Train:  [[ 12.72466867]]    Loss_Validation:  [[ 11.17785771]]\n",
      "Loop  4915 :    Loss_Train:  [[ 12.7246681]]    Loss_Validation:  [[ 11.17786442]]\n",
      "Loop  4916 :    Loss_Train:  [[ 12.72466753]]    Loss_Validation:  [[ 11.17787112]]\n",
      "Loop  4917 :    Loss_Train:  [[ 12.72466696]]    Loss_Validation:  [[ 11.17787782]]\n",
      "Loop  4918 :    Loss_Train:  [[ 12.72466639]]    Loss_Validation:  [[ 11.17788452]]\n",
      "Loop  4919 :    Loss_Train:  [[ 12.72466582]]    Loss_Validation:  [[ 11.17789121]]\n",
      "Loop  4920 :    Loss_Train:  [[ 12.72466525]]    Loss_Validation:  [[ 11.1778979]]\n",
      "Loop  4921 :    Loss_Train:  [[ 12.72466468]]    Loss_Validation:  [[ 11.17790458]]\n",
      "Loop  4922 :    Loss_Train:  [[ 12.72466412]]    Loss_Validation:  [[ 11.17791126]]\n",
      "Loop  4923 :    Loss_Train:  [[ 12.72466355]]    Loss_Validation:  [[ 11.17791794]]\n",
      "Loop  4924 :    Loss_Train:  [[ 12.72466299]]    Loss_Validation:  [[ 11.17792461]]\n",
      "Loop  4925 :    Loss_Train:  [[ 12.72466243]]    Loss_Validation:  [[ 11.17793127]]\n",
      "Loop  4926 :    Loss_Train:  [[ 12.72466186]]    Loss_Validation:  [[ 11.17793793]]\n",
      "Loop  4927 :    Loss_Train:  [[ 12.7246613]]    Loss_Validation:  [[ 11.17794459]]\n",
      "Loop  4928 :    Loss_Train:  [[ 12.72466074]]    Loss_Validation:  [[ 11.17795124]]\n",
      "Loop  4929 :    Loss_Train:  [[ 12.72466018]]    Loss_Validation:  [[ 11.17795789]]\n",
      "Loop  4930 :    Loss_Train:  [[ 12.72465962]]    Loss_Validation:  [[ 11.17796453]]\n",
      "Loop  4931 :    Loss_Train:  [[ 12.72465907]]    Loss_Validation:  [[ 11.17797117]]\n",
      "Loop  4932 :    Loss_Train:  [[ 12.72465851]]    Loss_Validation:  [[ 11.1779778]]\n",
      "Loop  4933 :    Loss_Train:  [[ 12.72465795]]    Loss_Validation:  [[ 11.17798443]]\n",
      "Loop  4934 :    Loss_Train:  [[ 12.7246574]]    Loss_Validation:  [[ 11.17799106]]\n",
      "Loop  4935 :    Loss_Train:  [[ 12.72465685]]    Loss_Validation:  [[ 11.17799768]]\n",
      "Loop  4936 :    Loss_Train:  [[ 12.72465629]]    Loss_Validation:  [[ 11.17800429]]\n",
      "Loop  4937 :    Loss_Train:  [[ 12.72465574]]    Loss_Validation:  [[ 11.1780109]]\n",
      "Loop  4938 :    Loss_Train:  [[ 12.72465519]]    Loss_Validation:  [[ 11.17801751]]\n",
      "Loop  4939 :    Loss_Train:  [[ 12.72465464]]    Loss_Validation:  [[ 11.17802411]]\n",
      "Loop  4940 :    Loss_Train:  [[ 12.72465409]]    Loss_Validation:  [[ 11.17803071]]\n",
      "Loop  4941 :    Loss_Train:  [[ 12.72465354]]    Loss_Validation:  [[ 11.1780373]]\n",
      "Loop  4942 :    Loss_Train:  [[ 12.72465299]]    Loss_Validation:  [[ 11.17804389]]\n",
      "Loop  4943 :    Loss_Train:  [[ 12.72465245]]    Loss_Validation:  [[ 11.17805048]]\n",
      "Loop  4944 :    Loss_Train:  [[ 12.7246519]]    Loss_Validation:  [[ 11.17805706]]\n",
      "Loop  4945 :    Loss_Train:  [[ 12.72465136]]    Loss_Validation:  [[ 11.17806363]]\n",
      "Loop  4946 :    Loss_Train:  [[ 12.72465081]]    Loss_Validation:  [[ 11.1780702]]\n",
      "Loop  4947 :    Loss_Train:  [[ 12.72465027]]    Loss_Validation:  [[ 11.17807677]]\n",
      "Loop  4948 :    Loss_Train:  [[ 12.72464973]]    Loss_Validation:  [[ 11.17808333]]\n",
      "Loop  4949 :    Loss_Train:  [[ 12.72464918]]    Loss_Validation:  [[ 11.17808989]]\n",
      "Loop  4950 :    Loss_Train:  [[ 12.72464864]]    Loss_Validation:  [[ 11.17809644]]\n",
      "Loop  4951 :    Loss_Train:  [[ 12.7246481]]    Loss_Validation:  [[ 11.17810299]]\n",
      "Loop  4952 :    Loss_Train:  [[ 12.72464757]]    Loss_Validation:  [[ 11.17810954]]\n",
      "Loop  4953 :    Loss_Train:  [[ 12.72464703]]    Loss_Validation:  [[ 11.17811608]]\n",
      "Loop  4954 :    Loss_Train:  [[ 12.72464649]]    Loss_Validation:  [[ 11.17812261]]\n",
      "Loop  4955 :    Loss_Train:  [[ 12.72464595]]    Loss_Validation:  [[ 11.17812915]]\n",
      "Loop  4956 :    Loss_Train:  [[ 12.72464542]]    Loss_Validation:  [[ 11.17813567]]\n",
      "Loop  4957 :    Loss_Train:  [[ 12.72464488]]    Loss_Validation:  [[ 11.17814219]]\n",
      "Loop  4958 :    Loss_Train:  [[ 12.72464435]]    Loss_Validation:  [[ 11.17814871]]\n",
      "Loop  4959 :    Loss_Train:  [[ 12.72464382]]    Loss_Validation:  [[ 11.17815523]]\n",
      "Loop  4960 :    Loss_Train:  [[ 12.72464329]]    Loss_Validation:  [[ 11.17816174]]\n",
      "Loop  4961 :    Loss_Train:  [[ 12.72464276]]    Loss_Validation:  [[ 11.17816824]]\n",
      "Loop  4962 :    Loss_Train:  [[ 12.72464223]]    Loss_Validation:  [[ 11.17817474]]\n",
      "Loop  4963 :    Loss_Train:  [[ 12.7246417]]    Loss_Validation:  [[ 11.17818124]]\n",
      "Loop  4964 :    Loss_Train:  [[ 12.72464117]]    Loss_Validation:  [[ 11.17818773]]\n",
      "Loop  4965 :    Loss_Train:  [[ 12.72464064]]    Loss_Validation:  [[ 11.17819421]]\n",
      "Loop  4966 :    Loss_Train:  [[ 12.72464011]]    Loss_Validation:  [[ 11.1782007]]\n",
      "Loop  4967 :    Loss_Train:  [[ 12.72463959]]    Loss_Validation:  [[ 11.17820718]]\n",
      "Loop  4968 :    Loss_Train:  [[ 12.72463906]]    Loss_Validation:  [[ 11.17821365]]\n",
      "Loop  4969 :    Loss_Train:  [[ 12.72463854]]    Loss_Validation:  [[ 11.17822012]]\n",
      "Loop  4970 :    Loss_Train:  [[ 12.72463802]]    Loss_Validation:  [[ 11.17822658]]\n",
      "Loop  4971 :    Loss_Train:  [[ 12.72463749]]    Loss_Validation:  [[ 11.17823305]]\n",
      "Loop  4972 :    Loss_Train:  [[ 12.72463697]]    Loss_Validation:  [[ 11.1782395]]\n",
      "Loop  4973 :    Loss_Train:  [[ 12.72463645]]    Loss_Validation:  [[ 11.17824595]]\n",
      "Loop  4974 :    Loss_Train:  [[ 12.72463593]]    Loss_Validation:  [[ 11.1782524]]\n",
      "Loop  4975 :    Loss_Train:  [[ 12.72463541]]    Loss_Validation:  [[ 11.17825885]]\n",
      "Loop  4976 :    Loss_Train:  [[ 12.72463489]]    Loss_Validation:  [[ 11.17826528]]\n",
      "Loop  4977 :    Loss_Train:  [[ 12.72463438]]    Loss_Validation:  [[ 11.17827172]]\n",
      "Loop  4978 :    Loss_Train:  [[ 12.72463386]]    Loss_Validation:  [[ 11.17827815]]\n",
      "Loop  4979 :    Loss_Train:  [[ 12.72463334]]    Loss_Validation:  [[ 11.17828457]]\n",
      "Loop  4980 :    Loss_Train:  [[ 12.72463283]]    Loss_Validation:  [[ 11.178291]]\n",
      "Loop  4981 :    Loss_Train:  [[ 12.72463231]]    Loss_Validation:  [[ 11.17829741]]\n",
      "Loop  4982 :    Loss_Train:  [[ 12.7246318]]    Loss_Validation:  [[ 11.17830383]]\n",
      "Loop  4983 :    Loss_Train:  [[ 12.72463129]]    Loss_Validation:  [[ 11.17831023]]\n",
      "Loop  4984 :    Loss_Train:  [[ 12.72463078]]    Loss_Validation:  [[ 11.17831664]]\n",
      "Loop  4985 :    Loss_Train:  [[ 12.72463027]]    Loss_Validation:  [[ 11.17832304]]\n",
      "Loop  4986 :    Loss_Train:  [[ 12.72462976]]    Loss_Validation:  [[ 11.17832943]]\n",
      "Loop  4987 :    Loss_Train:  [[ 12.72462925]]    Loss_Validation:  [[ 11.17833583]]\n",
      "Loop  4988 :    Loss_Train:  [[ 12.72462874]]    Loss_Validation:  [[ 11.17834221]]\n",
      "Loop  4989 :    Loss_Train:  [[ 12.72462823]]    Loss_Validation:  [[ 11.17834859]]\n",
      "Loop  4990 :    Loss_Train:  [[ 12.72462773]]    Loss_Validation:  [[ 11.17835497]]\n",
      "Loop  4991 :    Loss_Train:  [[ 12.72462722]]    Loss_Validation:  [[ 11.17836135]]\n",
      "Loop  4992 :    Loss_Train:  [[ 12.72462671]]    Loss_Validation:  [[ 11.17836772]]\n",
      "Loop  4993 :    Loss_Train:  [[ 12.72462621]]    Loss_Validation:  [[ 11.17837408]]\n",
      "Loop  4994 :    Loss_Train:  [[ 12.72462571]]    Loss_Validation:  [[ 11.17838044]]\n",
      "Loop  4995 :    Loss_Train:  [[ 12.7246252]]    Loss_Validation:  [[ 11.1783868]]\n",
      "Loop  4996 :    Loss_Train:  [[ 12.7246247]]    Loss_Validation:  [[ 11.17839315]]\n",
      "Loop  4997 :    Loss_Train:  [[ 12.7246242]]    Loss_Validation:  [[ 11.1783995]]\n",
      "Loop  4998 :    Loss_Train:  [[ 12.7246237]]    Loss_Validation:  [[ 11.17840584]]\n",
      "Loop  4999 :    Loss_Train:  [[ 12.7246232]]    Loss_Validation:  [[ 11.17841218]]\n",
      "Loop  5000 :    Loss_Train:  [[ 12.7246227]]    Loss_Validation:  [[ 11.17841852]]\n",
      "Loop  5001 :    Loss_Train:  [[ 12.72462221]]    Loss_Validation:  [[ 11.17842485]]\n",
      "Loop  5002 :    Loss_Train:  [[ 12.72462171]]    Loss_Validation:  [[ 11.17843117]]\n",
      "Loop  5003 :    Loss_Train:  [[ 12.72462121]]    Loss_Validation:  [[ 11.1784375]]\n",
      "Loop  5004 :    Loss_Train:  [[ 12.72462072]]    Loss_Validation:  [[ 11.17844381]]\n",
      "Loop  5005 :    Loss_Train:  [[ 12.72462022]]    Loss_Validation:  [[ 11.17845013]]\n",
      "Loop  5006 :    Loss_Train:  [[ 12.72461973]]    Loss_Validation:  [[ 11.17845644]]\n",
      "Loop  5007 :    Loss_Train:  [[ 12.72461924]]    Loss_Validation:  [[ 11.17846274]]\n",
      "Loop  5008 :    Loss_Train:  [[ 12.72461875]]    Loss_Validation:  [[ 11.17846904]]\n",
      "Loop  5009 :    Loss_Train:  [[ 12.72461826]]    Loss_Validation:  [[ 11.17847534]]\n",
      "Loop  5010 :    Loss_Train:  [[ 12.72461776]]    Loss_Validation:  [[ 11.17848163]]\n",
      "Loop  5011 :    Loss_Train:  [[ 12.72461728]]    Loss_Validation:  [[ 11.17848792]]\n",
      "Loop  5012 :    Loss_Train:  [[ 12.72461679]]    Loss_Validation:  [[ 11.1784942]]\n",
      "Loop  5013 :    Loss_Train:  [[ 12.7246163]]    Loss_Validation:  [[ 11.17850048]]\n",
      "Loop  5014 :    Loss_Train:  [[ 12.72461581]]    Loss_Validation:  [[ 11.17850676]]\n",
      "Loop  5015 :    Loss_Train:  [[ 12.72461532]]    Loss_Validation:  [[ 11.17851303]]\n",
      "Loop  5016 :    Loss_Train:  [[ 12.72461484]]    Loss_Validation:  [[ 11.1785193]]\n",
      "Loop  5017 :    Loss_Train:  [[ 12.72461435]]    Loss_Validation:  [[ 11.17852556]]\n",
      "Loop  5018 :    Loss_Train:  [[ 12.72461387]]    Loss_Validation:  [[ 11.17853182]]\n",
      "Loop  5019 :    Loss_Train:  [[ 12.72461339]]    Loss_Validation:  [[ 11.17853807]]\n",
      "Loop  5020 :    Loss_Train:  [[ 12.7246129]]    Loss_Validation:  [[ 11.17854432]]\n",
      "Loop  5021 :    Loss_Train:  [[ 12.72461242]]    Loss_Validation:  [[ 11.17855057]]\n",
      "Loop  5022 :    Loss_Train:  [[ 12.72461194]]    Loss_Validation:  [[ 11.17855681]]\n",
      "Loop  5023 :    Loss_Train:  [[ 12.72461146]]    Loss_Validation:  [[ 11.17856304]]\n",
      "Loop  5024 :    Loss_Train:  [[ 12.72461098]]    Loss_Validation:  [[ 11.17856928]]\n",
      "Loop  5025 :    Loss_Train:  [[ 12.7246105]]    Loss_Validation:  [[ 11.1785755]]\n",
      "Loop  5026 :    Loss_Train:  [[ 12.72461003]]    Loss_Validation:  [[ 11.17858173]]\n",
      "Loop  5027 :    Loss_Train:  [[ 12.72460955]]    Loss_Validation:  [[ 11.17858795]]\n",
      "Loop  5028 :    Loss_Train:  [[ 12.72460907]]    Loss_Validation:  [[ 11.17859416]]\n",
      "Loop  5029 :    Loss_Train:  [[ 12.7246086]]    Loss_Validation:  [[ 11.17860038]]\n",
      "Loop  5030 :    Loss_Train:  [[ 12.72460812]]    Loss_Validation:  [[ 11.17860658]]\n",
      "Loop  5031 :    Loss_Train:  [[ 12.72460765]]    Loss_Validation:  [[ 11.17861279]]\n",
      "Loop  5032 :    Loss_Train:  [[ 12.72460717]]    Loss_Validation:  [[ 11.17861899]]\n",
      "Loop  5033 :    Loss_Train:  [[ 12.7246067]]    Loss_Validation:  [[ 11.17862518]]\n",
      "Loop  5034 :    Loss_Train:  [[ 12.72460623]]    Loss_Validation:  [[ 11.17863137]]\n",
      "Loop  5035 :    Loss_Train:  [[ 12.72460576]]    Loss_Validation:  [[ 11.17863756]]\n",
      "Loop  5036 :    Loss_Train:  [[ 12.72460529]]    Loss_Validation:  [[ 11.17864374]]\n",
      "Loop  5037 :    Loss_Train:  [[ 12.72460482]]    Loss_Validation:  [[ 11.17864992]]\n",
      "Loop  5038 :    Loss_Train:  [[ 12.72460435]]    Loss_Validation:  [[ 11.17865609]]\n",
      "Loop  5039 :    Loss_Train:  [[ 12.72460388]]    Loss_Validation:  [[ 11.17866226]]\n",
      "Loop  5040 :    Loss_Train:  [[ 12.72460342]]    Loss_Validation:  [[ 11.17866843]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  5041 :    Loss_Train:  [[ 12.72460295]]    Loss_Validation:  [[ 11.17867459]]\n",
      "Loop  5042 :    Loss_Train:  [[ 12.72460248]]    Loss_Validation:  [[ 11.17868075]]\n",
      "Loop  5043 :    Loss_Train:  [[ 12.72460202]]    Loss_Validation:  [[ 11.1786869]]\n",
      "Loop  5044 :    Loss_Train:  [[ 12.72460156]]    Loss_Validation:  [[ 11.17869305]]\n",
      "Loop  5045 :    Loss_Train:  [[ 12.72460109]]    Loss_Validation:  [[ 11.17869919]]\n",
      "Loop  5046 :    Loss_Train:  [[ 12.72460063]]    Loss_Validation:  [[ 11.17870533]]\n",
      "Loop  5047 :    Loss_Train:  [[ 12.72460017]]    Loss_Validation:  [[ 11.17871147]]\n",
      "Loop  5048 :    Loss_Train:  [[ 12.72459971]]    Loss_Validation:  [[ 11.1787176]]\n",
      "Loop  5049 :    Loss_Train:  [[ 12.72459925]]    Loss_Validation:  [[ 11.17872373]]\n",
      "Loop  5050 :    Loss_Train:  [[ 12.72459879]]    Loss_Validation:  [[ 11.17872985]]\n",
      "Loop  5051 :    Loss_Train:  [[ 12.72459833]]    Loss_Validation:  [[ 11.17873597]]\n",
      "Loop  5052 :    Loss_Train:  [[ 12.72459787]]    Loss_Validation:  [[ 11.17874209]]\n",
      "Loop  5053 :    Loss_Train:  [[ 12.72459741]]    Loss_Validation:  [[ 11.1787482]]\n",
      "Loop  5054 :    Loss_Train:  [[ 12.72459696]]    Loss_Validation:  [[ 11.1787543]]\n",
      "Loop  5055 :    Loss_Train:  [[ 12.7245965]]    Loss_Validation:  [[ 11.17876041]]\n",
      "Loop  5056 :    Loss_Train:  [[ 12.72459605]]    Loss_Validation:  [[ 11.17876651]]\n",
      "Loop  5057 :    Loss_Train:  [[ 12.72459559]]    Loss_Validation:  [[ 11.1787726]]\n",
      "Loop  5058 :    Loss_Train:  [[ 12.72459514]]    Loss_Validation:  [[ 11.17877869]]\n",
      "Loop  5059 :    Loss_Train:  [[ 12.72459468]]    Loss_Validation:  [[ 11.17878478]]\n",
      "Loop  5060 :    Loss_Train:  [[ 12.72459423]]    Loss_Validation:  [[ 11.17879086]]\n",
      "Loop  5061 :    Loss_Train:  [[ 12.72459378]]    Loss_Validation:  [[ 11.17879694]]\n",
      "Loop  5062 :    Loss_Train:  [[ 12.72459333]]    Loss_Validation:  [[ 11.17880301]]\n",
      "Loop  5063 :    Loss_Train:  [[ 12.72459288]]    Loss_Validation:  [[ 11.17880908]]\n",
      "Loop  5064 :    Loss_Train:  [[ 12.72459243]]    Loss_Validation:  [[ 11.17881515]]\n",
      "Loop  5065 :    Loss_Train:  [[ 12.72459198]]    Loss_Validation:  [[ 11.17882121]]\n",
      "Loop  5066 :    Loss_Train:  [[ 12.72459153]]    Loss_Validation:  [[ 11.17882727]]\n",
      "Loop  5067 :    Loss_Train:  [[ 12.72459109]]    Loss_Validation:  [[ 11.17883332]]\n",
      "Loop  5068 :    Loss_Train:  [[ 12.72459064]]    Loss_Validation:  [[ 11.17883937]]\n",
      "Loop  5069 :    Loss_Train:  [[ 12.7245902]]    Loss_Validation:  [[ 11.17884541]]\n",
      "Loop  5070 :    Loss_Train:  [[ 12.72458975]]    Loss_Validation:  [[ 11.17885145]]\n",
      "Loop  5071 :    Loss_Train:  [[ 12.72458931]]    Loss_Validation:  [[ 11.17885749]]\n",
      "Loop  5072 :    Loss_Train:  [[ 12.72458886]]    Loss_Validation:  [[ 11.17886352]]\n",
      "Loop  5073 :    Loss_Train:  [[ 12.72458842]]    Loss_Validation:  [[ 11.17886955]]\n",
      "Loop  5074 :    Loss_Train:  [[ 12.72458798]]    Loss_Validation:  [[ 11.17887558]]\n",
      "Loop  5075 :    Loss_Train:  [[ 12.72458754]]    Loss_Validation:  [[ 11.1788816]]\n",
      "Loop  5076 :    Loss_Train:  [[ 12.7245871]]    Loss_Validation:  [[ 11.17888761]]\n",
      "Loop  5077 :    Loss_Train:  [[ 12.72458666]]    Loss_Validation:  [[ 11.17889363]]\n",
      "Loop  5078 :    Loss_Train:  [[ 12.72458622]]    Loss_Validation:  [[ 11.17889964]]\n",
      "Loop  5079 :    Loss_Train:  [[ 12.72458578]]    Loss_Validation:  [[ 11.17890564]]\n",
      "Loop  5080 :    Loss_Train:  [[ 12.72458534]]    Loss_Validation:  [[ 11.17891164]]\n",
      "Loop  5081 :    Loss_Train:  [[ 12.7245849]]    Loss_Validation:  [[ 11.17891764]]\n",
      "Loop  5082 :    Loss_Train:  [[ 12.72458447]]    Loss_Validation:  [[ 11.17892363]]\n",
      "Loop  5083 :    Loss_Train:  [[ 12.72458403]]    Loss_Validation:  [[ 11.17892962]]\n",
      "Loop  5084 :    Loss_Train:  [[ 12.7245836]]    Loss_Validation:  [[ 11.1789356]]\n",
      "Loop  5085 :    Loss_Train:  [[ 12.72458316]]    Loss_Validation:  [[ 11.17894158]]\n",
      "Loop  5086 :    Loss_Train:  [[ 12.72458273]]    Loss_Validation:  [[ 11.17894755]]\n",
      "Loop  5087 :    Loss_Train:  [[ 12.7245823]]    Loss_Validation:  [[ 11.17895353]]\n",
      "Loop  5088 :    Loss_Train:  [[ 12.72458186]]    Loss_Validation:  [[ 11.17895949]]\n",
      "Loop  5089 :    Loss_Train:  [[ 12.72458143]]    Loss_Validation:  [[ 11.17896546]]\n",
      "Loop  5090 :    Loss_Train:  [[ 12.724581]]    Loss_Validation:  [[ 11.17897142]]\n",
      "Loop  5091 :    Loss_Train:  [[ 12.72458057]]    Loss_Validation:  [[ 11.17897737]]\n",
      "Loop  5092 :    Loss_Train:  [[ 12.72458014]]    Loss_Validation:  [[ 11.17898332]]\n",
      "Loop  5093 :    Loss_Train:  [[ 12.72457971]]    Loss_Validation:  [[ 11.17898927]]\n",
      "Loop  5094 :    Loss_Train:  [[ 12.72457929]]    Loss_Validation:  [[ 11.17899521]]\n",
      "Loop  5095 :    Loss_Train:  [[ 12.72457886]]    Loss_Validation:  [[ 11.17900115]]\n",
      "Loop  5096 :    Loss_Train:  [[ 12.72457843]]    Loss_Validation:  [[ 11.17900709]]\n",
      "Loop  5097 :    Loss_Train:  [[ 12.72457801]]    Loss_Validation:  [[ 11.17901302]]\n",
      "Loop  5098 :    Loss_Train:  [[ 12.72457758]]    Loss_Validation:  [[ 11.17901895]]\n",
      "Loop  5099 :    Loss_Train:  [[ 12.72457716]]    Loss_Validation:  [[ 11.17902487]]\n",
      "Loop  5100 :    Loss_Train:  [[ 12.72457673]]    Loss_Validation:  [[ 11.17903079]]\n",
      "Loop  5101 :    Loss_Train:  [[ 12.72457631]]    Loss_Validation:  [[ 11.1790367]]\n",
      "Loop  5102 :    Loss_Train:  [[ 12.72457589]]    Loss_Validation:  [[ 11.17904261]]\n",
      "Loop  5103 :    Loss_Train:  [[ 12.72457547]]    Loss_Validation:  [[ 11.17904852]]\n",
      "Loop  5104 :    Loss_Train:  [[ 12.72457504]]    Loss_Validation:  [[ 11.17905442]]\n",
      "Loop  5105 :    Loss_Train:  [[ 12.72457462]]    Loss_Validation:  [[ 11.17906032]]\n",
      "Loop  5106 :    Loss_Train:  [[ 12.7245742]]    Loss_Validation:  [[ 11.17906622]]\n",
      "Loop  5107 :    Loss_Train:  [[ 12.72457379]]    Loss_Validation:  [[ 11.17907211]]\n",
      "Loop  5108 :    Loss_Train:  [[ 12.72457337]]    Loss_Validation:  [[ 11.179078]]\n",
      "Loop  5109 :    Loss_Train:  [[ 12.72457295]]    Loss_Validation:  [[ 11.17908388]]\n",
      "Loop  5110 :    Loss_Train:  [[ 12.72457253]]    Loss_Validation:  [[ 11.17908976]]\n",
      "Loop  5111 :    Loss_Train:  [[ 12.72457212]]    Loss_Validation:  [[ 11.17909563]]\n",
      "Loop  5112 :    Loss_Train:  [[ 12.7245717]]    Loss_Validation:  [[ 11.1791015]]\n",
      "Loop  5113 :    Loss_Train:  [[ 12.72457129]]    Loss_Validation:  [[ 11.17910737]]\n",
      "Loop  5114 :    Loss_Train:  [[ 12.72457087]]    Loss_Validation:  [[ 11.17911323]]\n",
      "Loop  5115 :    Loss_Train:  [[ 12.72457046]]    Loss_Validation:  [[ 11.17911909]]\n",
      "Loop  5116 :    Loss_Train:  [[ 12.72457004]]    Loss_Validation:  [[ 11.17912495]]\n",
      "Loop  5117 :    Loss_Train:  [[ 12.72456963]]    Loss_Validation:  [[ 11.1791308]]\n",
      "Loop  5118 :    Loss_Train:  [[ 12.72456922]]    Loss_Validation:  [[ 11.17913664]]\n",
      "Loop  5119 :    Loss_Train:  [[ 12.72456881]]    Loss_Validation:  [[ 11.17914249]]\n",
      "Loop  5120 :    Loss_Train:  [[ 12.7245684]]    Loss_Validation:  [[ 11.17914833]]\n",
      "Loop  5121 :    Loss_Train:  [[ 12.72456799]]    Loss_Validation:  [[ 11.17915416]]\n",
      "Loop  5122 :    Loss_Train:  [[ 12.72456758]]    Loss_Validation:  [[ 11.17915999]]\n",
      "Loop  5123 :    Loss_Train:  [[ 12.72456717]]    Loss_Validation:  [[ 11.17916582]]\n",
      "Loop  5124 :    Loss_Train:  [[ 12.72456676]]    Loss_Validation:  [[ 11.17917164]]\n",
      "Loop  5125 :    Loss_Train:  [[ 12.72456636]]    Loss_Validation:  [[ 11.17917746]]\n",
      "Loop  5126 :    Loss_Train:  [[ 12.72456595]]    Loss_Validation:  [[ 11.17918328]]\n",
      "Loop  5127 :    Loss_Train:  [[ 12.72456555]]    Loss_Validation:  [[ 11.17918909]]\n",
      "Loop  5128 :    Loss_Train:  [[ 12.72456514]]    Loss_Validation:  [[ 11.17919489]]\n",
      "Loop  5129 :    Loss_Train:  [[ 12.72456474]]    Loss_Validation:  [[ 11.1792007]]\n",
      "Loop  5130 :    Loss_Train:  [[ 12.72456433]]    Loss_Validation:  [[ 11.1792065]]\n",
      "Loop  5131 :    Loss_Train:  [[ 12.72456393]]    Loss_Validation:  [[ 11.17921229]]\n",
      "Loop  5132 :    Loss_Train:  [[ 12.72456353]]    Loss_Validation:  [[ 11.17921808]]\n",
      "Loop  5133 :    Loss_Train:  [[ 12.72456313]]    Loss_Validation:  [[ 11.17922387]]\n",
      "Loop  5134 :    Loss_Train:  [[ 12.72456272]]    Loss_Validation:  [[ 11.17922966]]\n",
      "Loop  5135 :    Loss_Train:  [[ 12.72456232]]    Loss_Validation:  [[ 11.17923544]]\n",
      "Loop  5136 :    Loss_Train:  [[ 12.72456192]]    Loss_Validation:  [[ 11.17924121]]\n",
      "Loop  5137 :    Loss_Train:  [[ 12.72456153]]    Loss_Validation:  [[ 11.17924698]]\n",
      "Loop  5138 :    Loss_Train:  [[ 12.72456113]]    Loss_Validation:  [[ 11.17925275]]\n",
      "Loop  5139 :    Loss_Train:  [[ 12.72456073]]    Loss_Validation:  [[ 11.17925851]]\n",
      "Loop  5140 :    Loss_Train:  [[ 12.72456033]]    Loss_Validation:  [[ 11.17926427]]\n",
      "Loop  5141 :    Loss_Train:  [[ 12.72455994]]    Loss_Validation:  [[ 11.17927003]]\n",
      "Loop  5142 :    Loss_Train:  [[ 12.72455954]]    Loss_Validation:  [[ 11.17927578]]\n",
      "Loop  5143 :    Loss_Train:  [[ 12.72455914]]    Loss_Validation:  [[ 11.17928153]]\n",
      "Loop  5144 :    Loss_Train:  [[ 12.72455875]]    Loss_Validation:  [[ 11.17928728]]\n",
      "Loop  5145 :    Loss_Train:  [[ 12.72455836]]    Loss_Validation:  [[ 11.17929302]]\n",
      "Loop  5146 :    Loss_Train:  [[ 12.72455796]]    Loss_Validation:  [[ 11.17929875]]\n",
      "Loop  5147 :    Loss_Train:  [[ 12.72455757]]    Loss_Validation:  [[ 11.17930448]]\n",
      "Loop  5148 :    Loss_Train:  [[ 12.72455718]]    Loss_Validation:  [[ 11.17931021]]\n",
      "Loop  5149 :    Loss_Train:  [[ 12.72455679]]    Loss_Validation:  [[ 11.17931594]]\n",
      "Loop  5150 :    Loss_Train:  [[ 12.72455639]]    Loss_Validation:  [[ 11.17932166]]\n",
      "Loop  5151 :    Loss_Train:  [[ 12.724556]]    Loss_Validation:  [[ 11.17932738]]\n",
      "Loop  5152 :    Loss_Train:  [[ 12.72455562]]    Loss_Validation:  [[ 11.17933309]]\n",
      "Loop  5153 :    Loss_Train:  [[ 12.72455523]]    Loss_Validation:  [[ 11.1793388]]\n",
      "Loop  5154 :    Loss_Train:  [[ 12.72455484]]    Loss_Validation:  [[ 11.1793445]]\n",
      "Loop  5155 :    Loss_Train:  [[ 12.72455445]]    Loss_Validation:  [[ 11.1793502]]\n",
      "Loop  5156 :    Loss_Train:  [[ 12.72455406]]    Loss_Validation:  [[ 11.1793559]]\n",
      "Loop  5157 :    Loss_Train:  [[ 12.72455368]]    Loss_Validation:  [[ 11.1793616]]\n",
      "Loop  5158 :    Loss_Train:  [[ 12.72455329]]    Loss_Validation:  [[ 11.17936729]]\n",
      "Loop  5159 :    Loss_Train:  [[ 12.72455291]]    Loss_Validation:  [[ 11.17937297]]\n",
      "Loop  5160 :    Loss_Train:  [[ 12.72455252]]    Loss_Validation:  [[ 11.17937865]]\n",
      "Loop  5161 :    Loss_Train:  [[ 12.72455214]]    Loss_Validation:  [[ 11.17938433]]\n",
      "Loop  5162 :    Loss_Train:  [[ 12.72455175]]    Loss_Validation:  [[ 11.17939001]]\n",
      "Loop  5163 :    Loss_Train:  [[ 12.72455137]]    Loss_Validation:  [[ 11.17939568]]\n",
      "Loop  5164 :    Loss_Train:  [[ 12.72455099]]    Loss_Validation:  [[ 11.17940134]]\n",
      "Loop  5165 :    Loss_Train:  [[ 12.72455061]]    Loss_Validation:  [[ 11.17940701]]\n",
      "Loop  5166 :    Loss_Train:  [[ 12.72455023]]    Loss_Validation:  [[ 11.17941266]]\n",
      "Loop  5167 :    Loss_Train:  [[ 12.72454985]]    Loss_Validation:  [[ 11.17941832]]\n",
      "Loop  5168 :    Loss_Train:  [[ 12.72454947]]    Loss_Validation:  [[ 11.17942397]]\n",
      "Loop  5169 :    Loss_Train:  [[ 12.72454909]]    Loss_Validation:  [[ 11.17942962]]\n",
      "Loop  5170 :    Loss_Train:  [[ 12.72454871]]    Loss_Validation:  [[ 11.17943526]]\n",
      "Loop  5171 :    Loss_Train:  [[ 12.72454833]]    Loss_Validation:  [[ 11.1794409]]\n",
      "Loop  5172 :    Loss_Train:  [[ 12.72454795]]    Loss_Validation:  [[ 11.17944654]]\n",
      "Loop  5173 :    Loss_Train:  [[ 12.72454758]]    Loss_Validation:  [[ 11.17945217]]\n",
      "Loop  5174 :    Loss_Train:  [[ 12.7245472]]    Loss_Validation:  [[ 11.1794578]]\n",
      "Loop  5175 :    Loss_Train:  [[ 12.72454683]]    Loss_Validation:  [[ 11.17946342]]\n",
      "Loop  5176 :    Loss_Train:  [[ 12.72454645]]    Loss_Validation:  [[ 11.17946904]]\n",
      "Loop  5177 :    Loss_Train:  [[ 12.72454608]]    Loss_Validation:  [[ 11.17947466]]\n",
      "Loop  5178 :    Loss_Train:  [[ 12.7245457]]    Loss_Validation:  [[ 11.17948027]]\n",
      "Loop  5179 :    Loss_Train:  [[ 12.72454533]]    Loss_Validation:  [[ 11.17948588]]\n",
      "Loop  5180 :    Loss_Train:  [[ 12.72454496]]    Loss_Validation:  [[ 11.17949148]]\n",
      "Loop  5181 :    Loss_Train:  [[ 12.72454459]]    Loss_Validation:  [[ 11.17949709]]\n",
      "Loop  5182 :    Loss_Train:  [[ 12.72454422]]    Loss_Validation:  [[ 11.17950268]]\n",
      "Loop  5183 :    Loss_Train:  [[ 12.72454385]]    Loss_Validation:  [[ 11.17950828]]\n",
      "Loop  5184 :    Loss_Train:  [[ 12.72454348]]    Loss_Validation:  [[ 11.17951387]]\n",
      "Loop  5185 :    Loss_Train:  [[ 12.72454311]]    Loss_Validation:  [[ 11.17951945]]\n",
      "Loop  5186 :    Loss_Train:  [[ 12.72454274]]    Loss_Validation:  [[ 11.17952503]]\n",
      "Loop  5187 :    Loss_Train:  [[ 12.72454237]]    Loss_Validation:  [[ 11.17953061]]\n",
      "Loop  5188 :    Loss_Train:  [[ 12.724542]]    Loss_Validation:  [[ 11.17953619]]\n",
      "Loop  5189 :    Loss_Train:  [[ 12.72454164]]    Loss_Validation:  [[ 11.17954176]]\n",
      "Loop  5190 :    Loss_Train:  [[ 12.72454127]]    Loss_Validation:  [[ 11.17954733]]\n",
      "Loop  5191 :    Loss_Train:  [[ 12.7245409]]    Loss_Validation:  [[ 11.17955289]]\n",
      "Loop  5192 :    Loss_Train:  [[ 12.72454054]]    Loss_Validation:  [[ 11.17955845]]\n",
      "Loop  5193 :    Loss_Train:  [[ 12.72454017]]    Loss_Validation:  [[ 11.179564]]\n",
      "Loop  5194 :    Loss_Train:  [[ 12.72453981]]    Loss_Validation:  [[ 11.17956956]]\n",
      "Loop  5195 :    Loss_Train:  [[ 12.72453945]]    Loss_Validation:  [[ 11.1795751]]\n",
      "Loop  5196 :    Loss_Train:  [[ 12.72453908]]    Loss_Validation:  [[ 11.17958065]]\n",
      "Loop  5197 :    Loss_Train:  [[ 12.72453872]]    Loss_Validation:  [[ 11.17958619]]\n",
      "Loop  5198 :    Loss_Train:  [[ 12.72453836]]    Loss_Validation:  [[ 11.17959172]]\n",
      "Loop  5199 :    Loss_Train:  [[ 12.724538]]    Loss_Validation:  [[ 11.17959726]]\n",
      "Loop  5200 :    Loss_Train:  [[ 12.72453764]]    Loss_Validation:  [[ 11.17960279]]\n",
      "Loop  5201 :    Loss_Train:  [[ 12.72453728]]    Loss_Validation:  [[ 11.17960831]]\n",
      "Loop  5202 :    Loss_Train:  [[ 12.72453692]]    Loss_Validation:  [[ 11.17961383]]\n",
      "Loop  5203 :    Loss_Train:  [[ 12.72453656]]    Loss_Validation:  [[ 11.17961935]]\n",
      "Loop  5204 :    Loss_Train:  [[ 12.7245362]]    Loss_Validation:  [[ 11.17962487]]\n",
      "Loop  5205 :    Loss_Train:  [[ 12.72453585]]    Loss_Validation:  [[ 11.17963038]]\n",
      "Loop  5206 :    Loss_Train:  [[ 12.72453549]]    Loss_Validation:  [[ 11.17963588]]\n",
      "Loop  5207 :    Loss_Train:  [[ 12.72453513]]    Loss_Validation:  [[ 11.17964138]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop  5208 :    Loss_Train:  [[ 12.72453478]]    Loss_Validation:  [[ 11.17964688]]\n",
      "Loop  5209 :    Loss_Train:  [[ 12.72453442]]    Loss_Validation:  [[ 11.17965238]]\n",
      "Loop  5210 :    Loss_Train:  [[ 12.72453407]]    Loss_Validation:  [[ 11.17965787]]\n",
      "Loop  5211 :    Loss_Train:  [[ 12.72453371]]    Loss_Validation:  [[ 11.17966336]]\n",
      "Loop  5212 :    Loss_Train:  [[ 12.72453336]]    Loss_Validation:  [[ 11.17966884]]\n",
      "Loop  5213 :    Loss_Train:  [[ 12.72453301]]    Loss_Validation:  [[ 11.17967432]]\n",
      "Loop  5214 :    Loss_Train:  [[ 12.72453265]]    Loss_Validation:  [[ 11.1796798]]\n",
      "Loop  5215 :    Loss_Train:  [[ 12.7245323]]    Loss_Validation:  [[ 11.17968527]]\n",
      "Loop  5216 :    Loss_Train:  [[ 12.72453195]]    Loss_Validation:  [[ 11.17969074]]\n",
      "Loop  5217 :    Loss_Train:  [[ 12.7245316]]    Loss_Validation:  [[ 11.17969621]]\n",
      "Loop  5218 :    Loss_Train:  [[ 12.72453125]]    Loss_Validation:  [[ 11.17970167]]\n",
      "Loop  5219 :    Loss_Train:  [[ 12.7245309]]    Loss_Validation:  [[ 11.17970713]]\n",
      "Loop  5220 :    Loss_Train:  [[ 12.72453055]]    Loss_Validation:  [[ 11.17971258]]\n",
      "Loop  5221 :    Loss_Train:  [[ 12.7245302]]    Loss_Validation:  [[ 11.17971803]]\n",
      "Loop  5222 :    Loss_Train:  [[ 12.72452985]]    Loss_Validation:  [[ 11.17972348]]\n",
      "Loop  5223 :    Loss_Train:  [[ 12.72452951]]    Loss_Validation:  [[ 11.17972892]]\n",
      "Loop  5224 :    Loss_Train:  [[ 12.72452916]]    Loss_Validation:  [[ 11.17973436]]\n",
      "Loop  5225 :    Loss_Train:  [[ 12.72452881]]    Loss_Validation:  [[ 11.17973979]]\n",
      "Loop  5226 :    Loss_Train:  [[ 12.72452847]]    Loss_Validation:  [[ 11.17974523]]\n",
      "Loop  5227 :    Loss_Train:  [[ 12.72452812]]    Loss_Validation:  [[ 11.17975065]]\n",
      "Loop  5228 :    Loss_Train:  [[ 12.72452778]]    Loss_Validation:  [[ 11.17975608]]\n",
      "Loop  5229 :    Loss_Train:  [[ 12.72452744]]    Loss_Validation:  [[ 11.1797615]]\n",
      "Loop  5230 :    Loss_Train:  [[ 12.72452709]]    Loss_Validation:  [[ 11.17976692]]\n",
      "Loop  5231 :    Loss_Train:  [[ 12.72452675]]    Loss_Validation:  [[ 11.17977233]]\n",
      "Loop  5232 :    Loss_Train:  [[ 12.72452641]]    Loss_Validation:  [[ 11.17977774]]\n",
      "Loop  5233 :    Loss_Train:  [[ 12.72452607]]    Loss_Validation:  [[ 11.17978315]]\n",
      "Loop  5234 :    Loss_Train:  [[ 12.72452572]]    Loss_Validation:  [[ 11.17978855]]\n",
      "Loop  5235 :    Loss_Train:  [[ 12.72452538]]    Loss_Validation:  [[ 11.17979395]]\n",
      "Loop  5236 :    Loss_Train:  [[ 12.72452504]]    Loss_Validation:  [[ 11.17979934]]\n",
      "Loop  5237 :    Loss_Train:  [[ 12.7245247]]    Loss_Validation:  [[ 11.17980473]]\n",
      "Loop  5238 :    Loss_Train:  [[ 12.72452436]]    Loss_Validation:  [[ 11.17981012]]\n",
      "Loop  5239 :    Loss_Train:  [[ 12.72452403]]    Loss_Validation:  [[ 11.1798155]]\n",
      "Loop  5240 :    Loss_Train:  [[ 12.72452369]]    Loss_Validation:  [[ 11.17982088]]\n",
      "Loop  5241 :    Loss_Train:  [[ 12.72452335]]    Loss_Validation:  [[ 11.17982626]]\n",
      "Loop  5242 :    Loss_Train:  [[ 12.72452301]]    Loss_Validation:  [[ 11.17983163]]\n",
      "Loop  5243 :    Loss_Train:  [[ 12.72452268]]    Loss_Validation:  [[ 11.179837]]\n",
      "Loop  5244 :    Loss_Train:  [[ 12.72452234]]    Loss_Validation:  [[ 11.17984237]]\n",
      "Loop  5245 :    Loss_Train:  [[ 12.72452201]]    Loss_Validation:  [[ 11.17984773]]\n",
      "Loop  5246 :    Loss_Train:  [[ 12.72452167]]    Loss_Validation:  [[ 11.17985309]]\n",
      "Loop  5247 :    Loss_Train:  [[ 12.72452134]]    Loss_Validation:  [[ 11.17985844]]\n",
      "Loop  5248 :    Loss_Train:  [[ 12.72452101]]    Loss_Validation:  [[ 11.17986379]]\n",
      "Loop  5249 :    Loss_Train:  [[ 12.72452067]]    Loss_Validation:  [[ 11.17986914]]\n",
      "Loop  5250 :    Loss_Train:  [[ 12.72452034]]    Loss_Validation:  [[ 11.17987448]]\n",
      "Loop  5251 :    Loss_Train:  [[ 12.72452001]]    Loss_Validation:  [[ 11.17987982]]\n",
      "Loop  5252 :    Loss_Train:  [[ 12.72451968]]    Loss_Validation:  [[ 11.17988516]]\n",
      "Loop  5253 :    Loss_Train:  [[ 12.72451935]]    Loss_Validation:  [[ 11.17989049]]\n",
      "Loop  5254 :    Loss_Train:  [[ 12.72451902]]    Loss_Validation:  [[ 11.17989582]]\n",
      "Loop  5255 :    Loss_Train:  [[ 12.72451869]]    Loss_Validation:  [[ 11.17990115]]\n",
      "Loop  5256 :    Loss_Train:  [[ 12.72451836]]    Loss_Validation:  [[ 11.17990647]]\n",
      "Loop  5257 :    Loss_Train:  [[ 12.72451803]]    Loss_Validation:  [[ 11.17991179]]\n",
      "Loop  5258 :    Loss_Train:  [[ 12.7245177]]    Loss_Validation:  [[ 11.1799171]]\n",
      "Loop  5259 :    Loss_Train:  [[ 12.72451737]]    Loss_Validation:  [[ 11.17992241]]\n",
      "Loop  5260 :    Loss_Train:  [[ 12.72451704]]    Loss_Validation:  [[ 11.17992772]]\n",
      "Loop  5261 :    Loss_Train:  [[ 12.72451672]]    Loss_Validation:  [[ 11.17993302]]\n",
      "Loop  5262 :    Loss_Train:  [[ 12.72451639]]    Loss_Validation:  [[ 11.17993832]]\n",
      "Loop  5263 :    Loss_Train:  [[ 12.72451607]]    Loss_Validation:  [[ 11.17994362]]\n",
      "Loop  5264 :    Loss_Train:  [[ 12.72451574]]    Loss_Validation:  [[ 11.17994891]]\n",
      "Loop  5265 :    Loss_Train:  [[ 12.72451542]]    Loss_Validation:  [[ 11.1799542]]\n",
      "Loop  5266 :    Loss_Train:  [[ 12.72451509]]    Loss_Validation:  [[ 11.17995948]]\n",
      "Loop  5267 :    Loss_Train:  [[ 12.72451477]]    Loss_Validation:  [[ 11.17996477]]\n",
      "Loop  5268 :    Loss_Train:  [[ 12.72451445]]    Loss_Validation:  [[ 11.17997004]]\n",
      "Loop  5269 :    Loss_Train:  [[ 12.72451412]]    Loss_Validation:  [[ 11.17997532]]\n",
      "Loop  5270 :    Loss_Train:  [[ 12.7245138]]    Loss_Validation:  [[ 11.17998059]]\n",
      "Loop  5271 :    Loss_Train:  [[ 12.72451348]]    Loss_Validation:  [[ 11.17998586]]\n",
      "Loop  5272 :    Loss_Train:  [[ 12.72451316]]    Loss_Validation:  [[ 11.17999112]]\n",
      "Loop  5273 :    Loss_Train:  [[ 12.72451284]]    Loss_Validation:  [[ 11.17999638]]\n",
      "Loop  5274 :    Loss_Train:  [[ 12.72451252]]    Loss_Validation:  [[ 11.18000164]]\n",
      "Loop  5275 :    Loss_Train:  [[ 12.7245122]]    Loss_Validation:  [[ 11.18000689]]\n",
      "Loop  5276 :    Loss_Train:  [[ 12.72451188]]    Loss_Validation:  [[ 11.18001214]]\n",
      "Loop  5277 :    Loss_Train:  [[ 12.72451156]]    Loss_Validation:  [[ 11.18001738]]\n",
      "Loop  5278 :    Loss_Train:  [[ 12.72451125]]    Loss_Validation:  [[ 11.18002263]]\n",
      "Loop  5279 :    Loss_Train:  [[ 12.72451093]]    Loss_Validation:  [[ 11.18002787]]\n",
      "Loop  5280 :    Loss_Train:  [[ 12.72451061]]    Loss_Validation:  [[ 11.1800331]]\n",
      "Loop  5281 :    Loss_Train:  [[ 12.7245103]]    Loss_Validation:  [[ 11.18003833]]\n",
      "Loop  5282 :    Loss_Train:  [[ 12.72450998]]    Loss_Validation:  [[ 11.18004356]]\n",
      "Loop  5283 :    Loss_Train:  [[ 12.72450966]]    Loss_Validation:  [[ 11.18004878]]\n",
      "Loop  5284 :    Loss_Train:  [[ 12.72450935]]    Loss_Validation:  [[ 11.180054]]\n",
      "Loop  5285 :    Loss_Train:  [[ 12.72450904]]    Loss_Validation:  [[ 11.18005922]]\n",
      "Loop  5286 :    Loss_Train:  [[ 12.72450872]]    Loss_Validation:  [[ 11.18006444]]\n",
      "Loop  5287 :    Loss_Train:  [[ 12.72450841]]    Loss_Validation:  [[ 11.18006965]]\n",
      "Loop  5288 :    Loss_Train:  [[ 12.7245081]]    Loss_Validation:  [[ 11.18007485]]\n",
      "Loop  5289 :    Loss_Train:  [[ 12.72450778]]    Loss_Validation:  [[ 11.18008005]]\n",
      "Loop  5290 :    Loss_Train:  [[ 12.72450747]]    Loss_Validation:  [[ 11.18008525]]\n",
      "Loop  5291 :    Loss_Train:  [[ 12.72450716]]    Loss_Validation:  [[ 11.18009045]]\n",
      "Loop  5292 :    Loss_Train:  [[ 12.72450685]]    Loss_Validation:  [[ 11.18009564]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1545b1efc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# read data\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"D://test.txt\")\n",
    "    return data\n",
    "\n",
    "# Draw\n",
    "def Draw(loops, Loss_train, Loss_Validation):\n",
    "    plt.plot(np.arange(0,100,1), Loss_train[0:100], label='Loss Train ')\n",
    "    plt.plot(np.arange(0,100,1), Loss_Validation[0:100], label='Loss Validation')\n",
    "    plt.xlabel('loops')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# grad\n",
    "def Grad(Lambda, theta, X_train):\n",
    "    grad = Lambda * theta + (np.dot(X_train.transpose(), np.dot(X_train, theta)))\n",
    "    return grad\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X = get_data()[0]\n",
    "    y = get_data()[1]\n",
    "\n",
    "    X = X.toarray()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "    train_row = X_train.shape[0]\n",
    "    test_row = X_test.shape[0]\n",
    "    col = X_train.shape[1]\n",
    "\n",
    "    \n",
    "    y_train = y_train.reshape(train_row, 1)\n",
    "    y_test = y_test.reshape(test_row, 1) \n",
    "    \n",
    "    theta = np.random.random(size = (col, 1))\n",
    "    Lambda = 0.01\n",
    "    Eta = 0.000085\n",
    "    \n",
    "    # batch grandient descent \n",
    "    loops = 100000 # 循环总次数\n",
    "    epsilon = 0.0001 # 收敛精度\n",
    "    count = 0 # loop的次数\n",
    "    error = np.zeros((col, 1)) # 上次theta的值，初始为0向量\n",
    "    finish = 0 # 完成标志位\n",
    "    \n",
    "    lossTrain = []\n",
    "    lossValidation = []\n",
    "\n",
    "    Tensor = np.dot(-X_train.transpose(), y_train)\n",
    "    \n",
    "    while count < loops:\n",
    "        count += 1\n",
    "        # 所有训练数据的期望更新一次theta\n",
    "        theta = theta - Eta * (Grad(Lambda, theta, X_train) + Tensor)\n",
    "        if (np.linalg.norm(theta - error) < epsilon):\n",
    "            finish = 1\n",
    "            break\n",
    "        else:\n",
    "            error = theta # 没有则将当前theta向量赋值给error，作为下次判断收敛的参数之一\n",
    "            Loss_Train = (1.0/2) * Lambda * theta.transpose().dot(theta) + (1.0/2) * (y_train - X_train.dot(theta)).transpose().dot((y_train - X_train.dot(theta)))\n",
    "            Loss_Validation = (1.0/2) * Lambda * theta.transpose().dot(theta) + (1.0/2) * (y_test-X_test.dot(theta)).transpose().dot((y_test - X_test.dot(theta)))\n",
    "            lossTrain.append(Loss_Train[0] / train_row)\n",
    "            lossValidation.append(Loss_Validation[0] / test_row)\n",
    "            print('Loop '.format(count), count, ':',  '   Loss_Train: ',Loss_Train / train_row, '   Loss_Validation: ',Loss_Validation / test_row)\n",
    "\n",
    "    Draw(count, lossTrain, lossValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
